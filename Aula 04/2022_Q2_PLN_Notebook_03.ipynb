{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022-Q2 PLN Notebook 03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyODi/Rk1Mqbsx+n4HHaIwQB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalves-ufabc/2022.Q2-PLN/blob/main/2022_Q2_PLN_Notebook_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX66rCyzFT9A"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2022.Q2]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C70pRuctFXyX"
      },
      "source": [
        "# **Arquivos PDF**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoXGy9DwFhO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6619c39-137e-41be-b6f2-f9569815bd4b"
      },
      "source": [
        "# https://github.com/pdfminer/pdfminer.six\n",
        "\n",
        "# Pdfminer.six is a community maintained fork of the original PDFMiner. \n",
        "# It is a tool for extracting information from PDF documents. \n",
        "# It focuses on getting and analyzing text data. \n",
        "\n",
        "# Pdfminer.six extracts the text from a page directly from the sourcecode of the PDF. \n",
        "# It can also be used to get the exact location, font or color of the text.\n",
        "\n",
        "\n",
        "# install and import all the necessary libraries\n",
        "!pip3 install pdfminer.six"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20220524-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 5.4 MB/s \n",
            "\u001b[?25hCollecting cryptography>=36.0.0\n",
            "  Downloading cryptography-37.0.3-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 44.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (2.0.12)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Installing collected packages: cryptography, pdfminer.six\n",
            "Successfully installed cryptography-37.0.3 pdfminer.six-20220524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyv6mXz59GZW"
      },
      "source": [
        "### **Arquivos com 2 colunas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9p0su4uIu2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe2448b-b7b9-4917-b6bc-10a7546a1027"
      },
      "source": [
        "import pdfminer\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "print(pdfminer.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20220524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E09k1WV6KRCg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2e1f15f-6d87-42d4-be0e-3c8dbe28aa95"
      },
      "source": [
        "text = extract_text('/content/example01.pdf')\n",
        "\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cats and Dogs\n",
            "\n",
            "Omkar M Parkhi1,2 Andrea Vedaldi1 Andrew Zisserman1 C. V. Jawahar2\n",
            "1Department of Engineering Science,\n",
            "University of Oxford,\n",
            "United Kingdom\n",
            "{omkar,vedaldi,az}@robots.ox.ac.uk\n",
            "\n",
            "2Center for Visual Information Technology,\n",
            "International Institute of Information Technology,\n",
            "Hyderabad, India\n",
            "jawahar@iiit.ac.in\n",
            "\n",
            "Abstract\n",
            "\n",
            "We investigate the ﬁne grained object categorization\n",
            "problem of determining the breed of animal from an image.\n",
            "To this end we introduce a new annotated dataset of pets,\n",
            "the Oxford-IIIT-Pet dataset, covering 37 different breeds of\n",
            "cats and dogs. The visual problem is very challenging as\n",
            "these animals, particularly cats, are very deformable and\n",
            "there can be quite subtle differences between the breeds.\n",
            "\n",
            "We make a number of contributions: ﬁrst, we introduce a\n",
            "model to classify a pet breed automatically from an image.\n",
            "The model combines shape, captured by a deformable part\n",
            "model detecting the pet face, and appearance, captured by\n",
            "a bag-of-words model that describes the pet fur. Fitting the\n",
            "model involves automatically segmenting the animal in the\n",
            "image. Second, we compare two classiﬁcation approaches:\n",
            "a hierarchical one, in which a pet is ﬁrst assigned to the cat\n",
            "or dog family and then to a breed, and a ﬂat one, in which\n",
            "the breed is obtained directly. We also investigate a number\n",
            "of animal and image orientated spatial layouts.\n",
            "\n",
            "These models are very good: they beat all previously\n",
            "published results on the challenging ASIRRA test (cat vs\n",
            "dog discrimination). When applied to the task of discrimi-\n",
            "nating the 37 different breeds of pets, the models obtain an\n",
            "average accuracy of about 59%, a very encouraging result\n",
            "considering the difﬁculty of the problem.\n",
            "\n",
            "1. Introduction\n",
            "\n",
            "Research on object category recognition has largely fo-\n",
            "cused on the discrimination of well distinguished object cat-\n",
            "egories (e.g, airplane vs cat). Most popular international\n",
            "benchmarks (e.g, Caltech-101 [22], Caltech-256 [26], PAS-\n",
            "CAL VOC [20]) contain a few dozen object classes that,\n",
            "for the most part, are visually dissimilar. Even in the much\n",
            "larger ImageNet database [18], categories are deﬁned based\n",
            "on a high-level ontology and, as such, any visual similar-\n",
            "ity between them is more accidental than systematic. This\n",
            "work concentrates instead on the problem of discriminat-\n",
            "\n",
            "ing different breeds of cats and dogs, a challenging exam-\n",
            "ple of ﬁne grained object categorization in line with that of\n",
            "previous work on ﬂower [15, 32, 33, 39] and animal and\n",
            "bird species [14, 27, 28, 43] categorization. The difﬁculty\n",
            "is in the fact that breeds may differ only by a few subtle\n",
            "phenotypic details that, due to the highly deformable na-\n",
            "ture of the bodies of such animals, can be difﬁcult to mea-\n",
            "sure automatically. Indeed, authors have often focused on\n",
            "cats and dogs as examples of highly deformable objects\n",
            "for which recognition and detection is particularly challeng-\n",
            "ing [24, 29, 34, 45].\n",
            "\n",
            "Beyond the technical interest of ﬁne grained categoriza-\n",
            "tion, extracting information from images of pets has a prac-\n",
            "tical side too. People devote a lot of attention to their do-\n",
            "mestic animals, as suggested by the large number of so-\n",
            "cial networks dedicated to the sharing of images of cats\n",
            "and dogs: Pet Finder [11], Catster [4], Dogster [5], My\n",
            "Cat Space [9], My Dog Space [10], The International Cat\n",
            "Association [8] and several others [1, 2, 3, 12].\n",
            "In fact,\n",
            "the bulk of the data used in this paper has been extracted\n",
            "from annotated images that users of these social sites post\n",
            "daily (Sect. 2). It is not unusual for owners to believe (and\n",
            "post) the incorrect breed for their pet, so having a method\n",
            "of automated classiﬁcation could provide a gentle way of\n",
            "alerting them to such errors.\n",
            "\n",
            "The ﬁrst contribution of this paper is the introduction of a\n",
            "large annotated collection of images of 37 different breeds\n",
            "of cats and dogs (Sect. 2).\n",
            "It includes 12 cat breeds and\n",
            "25 dog breeds. This data constitutes the benchmark for pet\n",
            "breed classiﬁcation, and, due to its focus on ﬁne grained cat-\n",
            "egorization, is complementary to the standard object recog-\n",
            "nition benchmarks. The data, which is publicly available,\n",
            "comes with rich annotations: in addition to a breed label,\n",
            "each pet has a pixel level segmentation and a rectangle lo-\n",
            "calising its head. A simple evaluation protocol, inspired by\n",
            "the PASCAL VOC challenge, is also proposed to enable\n",
            "the comparison of future methods on a common grounds\n",
            "(Sect. 2). This dataset is also complementary to the subset\n",
            "of ImageNet used in [27] for dogs, as it contains additional\n",
            "annotations, though for fewer breeds.\n",
            "\n",
            "1\n",
            "\n",
            "\fVOC data. The dataset contains about 200 images for each\n",
            "breed (which have been split randomly into 50 for training,\n",
            "50 for validation, and 100 for testing). A detailed list of\n",
            "breeds is given in Tab. 1, and example images are given in\n",
            "Fig. 2. The dataset is available at [35].\n",
            "\n",
            "Dataset collection. The pet images were downloaded\n",
            "from Catster [4] and Dogster [5], two social web sites ded-\n",
            "icated to the collection and discussion of images of pets,\n",
            "from Flickr [6] groups, and from Google images [7]. Peo-\n",
            "ple uploading images to Catster and Dogster provide the\n",
            "breed information as well, and the Flickr groups are spe-\n",
            "ciﬁc to each breed, which simpliﬁes tagging. For each of\n",
            "the 37 breeds, about 2,000 – 2,500 images were down-\n",
            "loaded from these data sources to form a pool of candidates\n",
            "for inclusion in the dataset. From this candidate list, im-\n",
            "ages were dropped if any of the following conditions ap-\n",
            "plied, as judged by the annotators: (i) the image was gray\n",
            "scale, (ii) another image portraying the same animal existed\n",
            "(which happens frequently in Flickr), (iii) the illumination\n",
            "was poor, (iv) the pet was not centered in the image, or (v)\n",
            "the pet was wearing clothes. The most common problem\n",
            "in all the data sources, however, was found to be errors in\n",
            "the breed labels. Thus labels were reviewed by the human\n",
            "annotators and ﬁxed whenever possible. When ﬁxing was\n",
            "not possible, for instance because the pet was a cross breed,\n",
            "the image was dropped. Overall, up to 200 images for each\n",
            "of the 37 breeds were obtained.\n",
            "\n",
            "Annotations. Each image is annotated with a breed label,\n",
            "a pixel level segmentation marking the body, and a tight\n",
            "bounding box about the head. The segmentation is a trimap\n",
            "with regions corresponding to: foreground (the pet body),\n",
            "background, and ambiguous (the pet body boundary and\n",
            "any accessory such as collars). Fig. 1 shows examples of\n",
            "these annotations.\n",
            "\n",
            "Evaluation protocol. Three tasks are deﬁned: pet family\n",
            "classiﬁcation (Cat vs Dog, a two class problem), breed clas-\n",
            "siﬁcation given the family (a 12 class problem for cats and\n",
            "a 25 class problem for dogs), and breed and family classi-\n",
            "ﬁcation (a 37 class problem). In all cases, the performance\n",
            "is measured as the average per-class classiﬁcation accuracy.\n",
            "This is the proportion of correctly classiﬁed images for each\n",
            "of the classes and can be computed as the average of the\n",
            "diagonal of the (row normalized) confusion matrix. This\n",
            "means that, for example, a random classiﬁer has average ac-\n",
            "curacy of 1/2 = 50% for the family classiﬁcation task, and\n",
            "of 1/37 ≈ 3% for the breed and family classiﬁcation task.\n",
            "Algorithms are trained on the training and validation sub-\n",
            "sets and tested on the test subset. The split between training\n",
            "and validation is provided only for convenience, but can be\n",
            "disregarded.\n",
            "\n",
            "Figure 1. Annotations in the Oxford-IIIT Pet data. From left\n",
            "to right: pet image, head bounding box, and trimap segmentation\n",
            "(blue: background region; red: ambiguous region; yellow: fore-\n",
            "ground region).\n",
            "\n",
            "The second contribution of the paper is a model for pet\n",
            "breed discrimination (Sect. 3). The model captures both\n",
            "shape (by a deformable part model [23, 42] of the pet face)\n",
            "and texture (by a bag-of-visual-words model [16, 30, 38, 44]\n",
            "of the pet fur). Unfortunately, current deformable part mod-\n",
            "els are not sufﬁciently advanced to represent satisfactorily\n",
            "the highly deformable bodies of cats and dogs; nevertheless,\n",
            "they can be used to reliably extract stable and distinctive\n",
            "components of the body, such as the pet face. The method\n",
            "used in [34] followed from this observation: a cat’s face\n",
            "was detected as the ﬁrst stage in detecting the entire animal.\n",
            "Here we go further in using the detected head shape as a part\n",
            "of the feature descriptor. Two natural ways of combining\n",
            "the shape and appearance features are then considered and\n",
            "compared: a ﬂat approach, in which both features are used\n",
            "to regress the pet’s family and the breed simultaneously, and\n",
            "a hierarchical one, in which the family is determined ﬁrst\n",
            "based on the shape features alone, and then appearance is\n",
            "used to predict the breed conditioned on the family. Infer-\n",
            "ring the model in an image involves segmenting the animal\n",
            "from the background. To this end, we improved on our pre-\n",
            "vious method on of segmentation in [34] basing it on the\n",
            "extraction of superpixels.\n",
            "\n",
            "The model is validated experimentally on the task of dis-\n",
            "criminating the 37 pet breeds (Sect. 4), obtaining very en-\n",
            "couraging results, especially considering the toughness of\n",
            "the problem. Furthermore, we also use the model to break\n",
            "the ASIRRA test that uses the ability of discriminating be-\n",
            "tween cats and dogs to tell humans from machines.\n",
            "\n",
            "2. Datasets and evaluation measures\n",
            "\n",
            "2.1. The Oxford-IIIT Pet dataset\n",
            "\n",
            "The Oxford-IIIT Pet dataset is a collection of 7,349 im-\n",
            "ages of cats and dogs of 37 different breeds, of which 25\n",
            "are dogs and 12 are cats. Images are divided into training,\n",
            "validation, and test sets, in a similar manner to the PASCAL\n",
            "\n",
            "\fBreed\n",
            "\n",
            "Abyssinian\n",
            "Bengal\n",
            "Birman\n",
            "Bombay\n",
            "British Shorthair\n",
            "Egyptian Mau\n",
            "Maine Coon\n",
            "Persian\n",
            "Ragdoll\n",
            "Russian Blue\n",
            "Siamese\n",
            "Sphynx\n",
            "American Bulldog\n",
            "American Pit Bull Terrier\n",
            "Basset Hound\n",
            "Beagle\n",
            "Boxer\n",
            "Chihuahua\n",
            "English Cocker Spaniel\n",
            "\n",
            "Breed\n",
            "\n",
            "Japanese Chin\n",
            "\n",
            "Training Validation Test Total\n",
            "50\n",
            "50\n",
            "50\n",
            "47\n",
            "50\n",
            "46\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "49\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "46\n",
            "\n",
            "98\n",
            "100\n",
            "100\n",
            "88\n",
            "100\n",
            "97\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "99\n",
            "100\n",
            "100\n",
            "\n",
            "198 English Setter\n",
            "200 German Shorthaired\n",
            "200 Great Pyrenees\n",
            "184 Havanese\n",
            "200\n",
            "190 Keeshond\n",
            "200 Leonberger\n",
            "200 Miniature Pinscher\n",
            "200 Newfoundland\n",
            "200\n",
            "199\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "199 Wheaten Terrier\n",
            "200 Yorkshire Terrier\n",
            "196\n",
            "\n",
            "Pomeranian\n",
            "Pug\n",
            "Saint Bernard\n",
            "Samoyed\n",
            "Scottish Terrier\n",
            "Shiba Inu\n",
            "Staffordshire Bull Terrier\n",
            "\n",
            "50\n",
            "50\n",
            "50\n",
            "49\n",
            "50\n",
            "47\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "\n",
            "Total\n",
            "\n",
            "Training Validation Test Total\n",
            "200\n",
            "100\n",
            "200\n",
            "100\n",
            "200\n",
            "100\n",
            "200\n",
            "100\n",
            "200\n",
            "100\n",
            "199\n",
            "99\n",
            "200\n",
            "100\n",
            "200\n",
            "100\n",
            "196\n",
            "100\n",
            "200\n",
            "100\n",
            "200\n",
            "100\n",
            "200\n",
            "100\n",
            "200\n",
            "100\n",
            "199\n",
            "99\n",
            "200\n",
            "100\n",
            "189\n",
            "89\n",
            "200\n",
            "100\n",
            "100\n",
            "200\n",
            "3669 7349\n",
            "\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "1846\n",
            "\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "46\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "1834\n",
            "\n",
            "Table 1. Oxford-IIIT Pet data composition. The 12 cat breeds followed by the 25 dog breeds.\n",
            "\n",
            "Abyssinian\n",
            "\n",
            "Bengal\n",
            "\n",
            "Bombay\n",
            "\n",
            "Birman\n",
            "\n",
            "British Shorthair\n",
            "\n",
            "Maine Coon\n",
            "\n",
            "Persian\n",
            "\n",
            "Egyptian\n",
            "\n",
            "Ragdoll\n",
            "\n",
            "Russian Blue\n",
            "\n",
            "Siamese\n",
            "\n",
            "Sphynx\n",
            "\n",
            "Eng. Setter\n",
            "\n",
            "Boxer\n",
            "\n",
            "Keeshond\n",
            "\n",
            "Havanese\n",
            "\n",
            "Basset Hound\n",
            "\n",
            "Mini Pinscher\n",
            "\n",
            "Chihuahua\n",
            "\n",
            "Great Pyrenees\n",
            "\n",
            "German Shorthaired\n",
            "\n",
            "Beagle\n",
            "\n",
            "Staff. Bull Terrier\n",
            "\n",
            "Eng. Cocker\n",
            "\n",
            "New Found Land\n",
            "\n",
            "Pomeranian\n",
            "\n",
            "Leonberger\n",
            "\n",
            "Am. Pit Bull Terrier\n",
            "\n",
            "Wheaten Terrier\n",
            "\n",
            "Japanese Chin\n",
            "\n",
            "Samoyed\n",
            "\n",
            "Scottish Terrier\n",
            "\n",
            "Shiba Inu\n",
            "\n",
            "Pug\n",
            "\n",
            "Saint Bernard\n",
            "\n",
            "Am. Bull Dog\n",
            "\n",
            "Figure 2. Example images from the Oxford-IIIT Pet data. Two images per breed are shown side by side to illustrate the data variability.\n",
            "\n",
            "2.2. The ASIRRA dataset\n",
            "\n",
            "Microsoft Research (MSR) proposed the problem of dis-\n",
            "criminating cats from dogs as a test to tell humans from ma-\n",
            "\n",
            "chines, and created the ASIRRA test ([19], Fig. 3) on this ba-\n",
            "sis. The assumption is that, out of a batch of twelve images\n",
            "of pets, any machine would predict incorrectly the family\n",
            "\n",
            "\fseen by examining the performance of this detector on the\n",
            "cats and dogs in the recent PASCAL VOC 2011 challenge\n",
            "data [20]. The deformable parts detector [23] obtains an\n",
            "Average Precision (AP) of only 31.7% and 22.1% on cats\n",
            "and dogs respectively [20]; by comparison, an easier cat-\n",
            "egory such as bicycle has AP of 54% [20]. However, in\n",
            "the PASCAL VOC challenge the task is to detect the whole\n",
            "body of the animal. As in the method of [34], we use the\n",
            "deformable part model to detect certain stable and distinc-\n",
            "tive components of the body. In particular, the head annota-\n",
            "tions included in the Oxford-IIIT Pet data are used to learn\n",
            "a deformable part model of the cat faces, and one of the\n",
            "dog faces ([24, 29, 45] also focus on modelling the faces of\n",
            "pets). Sect. 4.1 shows that these shape models are in fact\n",
            "very good.\n",
            "\n",
            "To represent texture, we use a bag-of-words [16] model.\n",
            "Visual words [38] are computed densely on the image by ex-\n",
            "tracting SIFT descriptors [31] with a stride of 6 pixels and\n",
            "at 4 scales, deﬁned by setting the width of the SIFT spatial\n",
            "bins to 4, 6, 8, and 10 pixels respectively. The SIFT features\n",
            "have constant orientation (i.e, they are not adapted to the lo-\n",
            "cal image appearance). The SIFT descriptors are then quan-\n",
            "tized based on a vocabulary of 4,000 visual words. The vo-\n",
            "cabulary is learned by using k-means on features randomly\n",
            "sampled from the training data. In order to obtain a descrip-\n",
            "tor for the image, the quantized SIFT features are pooled\n",
            "into a spatial histogram [30], which has dimension equal to\n",
            "4,000 times the number of spatial bins. Histograms are then\n",
            "l1 normalized and used in a support vector machine (SVM)\n",
            "based on the exponential-χ2 kernel [44] for classiﬁcation.\n",
            "\n",
            "Different variants of the spatial histograms can be ob-\n",
            "tained by placing the spatial bins in correspondence of par-\n",
            "ticular geometric features of the pet. These layouts are de-\n",
            "scribed next and in Fig. 4:\n",
            "\n",
            "Image layout. This layout consists of ﬁve spatial bins or-\n",
            "ganized as a 1 × 1 and a 2 × 2 grids (Fig. 4a) covering the\n",
            "entire image area, as in [30]. This results in a 20,000 di-\n",
            "mensional feature vector.\n",
            "\n",
            "Image+head layout. This layout adds to the image layout\n",
            "just described a spatial bin in correspondence of the head\n",
            "bounding box (as detected by the deformable part model\n",
            "of the pet face) as well as one for the complement of this\n",
            "box. These two regions do not contain further spatial subdi-\n",
            "visions (Fig. 4b). Concatenating the histograms for all the\n",
            "spatial bins in this layout results in a 28,000 dimensional\n",
            "feature vector.\n",
            "\n",
            "Figure 3. Example images from the MSR ASIRRA dataset.\n",
            "\n",
            "3.2. Appearance model\n",
            "\n",
            "of at least one of them, while humans would make no mis-\n",
            "takes. The ASIRRA test is currently used to protect a num-\n",
            "ber of web sites from the unwanted access by Internet bots.\n",
            "However, the reliability of this test depends on the clas-\n",
            "siﬁcation accuracy α of the classiﬁer implemented by the\n",
            "bot. For instance, if the classiﬁer has accuracy α = 95%,\n",
            "then the bot fools the ASIRRA test roughly half of the times\n",
            "(α12 ≈ 54%).\n",
            "\n",
            "The complete MSR ASIRRA system is based on a\n",
            "database of several millions images of pets, equally divided\n",
            "between cats and dogs. Our classiﬁers are tested on the\n",
            "24,990 images that have been made available to the public\n",
            "for research and evaluation purposes.\n",
            "\n",
            "3. A model for breed discrimination\n",
            "\n",
            "The breed of a pet affects its size, shape, fur type and\n",
            "color. Since it is not possible to measure the pet size from\n",
            "an image without an absolute reference, our model focuses\n",
            "on capturing the pet shape (Sect. 3.1) and the appearance of\n",
            "its fur (Sect. 3.2). The model also involves automatically\n",
            "segmenting the pet from the image background (Sect. 3.3).\n",
            "\n",
            "3.1. Shape model\n",
            "\n",
            "To represent shape, we use the deformable part model\n",
            "of [23]. In this model, an object is given by a root part con-\n",
            "nected with springs to eight smaller parts at a ﬁner scale.\n",
            "The appearance of each part is represented by a HOG ﬁl-\n",
            "ter [17], capturing the local distribution of the image edges;\n",
            "inference (detection) uses dynamic programming to ﬁnd the\n",
            "best trade-off between matching well each part to the image\n",
            "and not deforming the springs too much.\n",
            "\n",
            "While powerful, this model is insufﬁcient to represent\n",
            "the ﬂexibility and variability of a pet body. This can be\n",
            "\n",
            "Image+head+body layout. This layout combines the\n",
            "spatial tiles in the image layout with an additional spatial bin\n",
            "\n",
            "\f(a) Image\n",
            "\n",
            "(b) Image+Head\n",
            "\n",
            "(c) Image+Head+Body\n",
            "\n",
            "Figure 4. Spatial histogram layouts. The three different spatial\n",
            "layouts used for computing the image descriptors. The image de-\n",
            "scriptor in each case is formed by concatenating the histograms\n",
            "computed on the individual spatial components of the layout. The\n",
            "spatial bins are denoted by yellow-black lines.\n",
            "\n",
            "in correspondence of the pet head (as for the image+head\n",
            "layout) as well as other spatial bins computed on the fore-\n",
            "ground object region and its complement, as described next\n",
            "and in Fig. 4c. The foreground region is obtained either\n",
            "from the automatic segmentation of the pet body or from\n",
            "the ground-truth segmentation to obtain a best-case base-\n",
            "line. The foreground region is subdivided into ﬁve spatial\n",
            "bins, similar to the image layout. An additional bin obtained\n",
            "from the foreground region with the head region removed\n",
            "and no further spatial subdivisions is also used. Concate-\n",
            "nating the histograms for all the spatial bins in this layout\n",
            "results in a 48,000 dimensional feature vector.\n",
            "\n",
            "3.3. Automatic segmentation\n",
            "\n",
            "The foreground (pet) and background regions needed for\n",
            "computing the appearance descriptors are obtained auto-\n",
            "matically using the grab-cut segmentation technique [36].\n",
            "Initialization of grab-cut segmentations was done using\n",
            "cues from the over-segmentation of an image (i.e, super-\n",
            "pixels) similar to the method of [15].\n",
            "In this method, a\n",
            "SVM classiﬁer is used to assign superpixels a conﬁdence\n",
            "score. This conﬁdence score is then used to assign super-\n",
            "pixels to a foreground or background region to initialize\n",
            "the grab-cut iteration. We used Berkeley’s ultrametric color\n",
            "map (UCM) [13] for obtaining the superpixels. Each super-\n",
            "pixel was described by a feature vector comprising the color\n",
            "histogram and Sift-BoW histogram computed on it. Super-\n",
            "pixels were assigned a score using a linear-SVM [21] which\n",
            "was trained on the features computed on the training data.\n",
            "After this initialization, grab-cut was used as in [34]. The\n",
            "improved initialization achieves segmentation accuracy of\n",
            "65% this improving over our previous method [34] by 4%\n",
            "and is about 20% better than simply choosing all pixels as\n",
            "foreground (i.e, assuming the pet foreground entirely occu-\n",
            "pies the image). (Tab. 2). Example segmentations produced\n",
            "by our method on the Oxford-IIIT Pet data are shown in\n",
            "Fig. 5.\n",
            "\n",
            "Method\n",
            "All foreground\n",
            "Parkhi et al. [34]\n",
            "This paper\n",
            "\n",
            "Mean Segmentation Accuracy\n",
            "45%\n",
            "61%\n",
            "65%\n",
            "\n",
            "Table 2. Performance of segmentation schemes. Segmentation\n",
            "accuracy computed as intersection over union of segmentation\n",
            "with ground truth.\n",
            "\n",
            "Dataset\n",
            "Oxford-IIIT Pet Dataset\n",
            "UCSD-Caltech Birds\n",
            "Oxford-Flowers102\n",
            "\n",
            "Mean Classiﬁcation Accuracy\n",
            "38.45%\n",
            "6.91%\n",
            "53.71%\n",
            "\n",
            "Table 3. Fine grained classiﬁcation baseline. Mean classiﬁcation\n",
            "accuracies obtained on three different datasets using the VLFeat-\n",
            "BoW classiﬁcation code.\n",
            "\n",
            "4. Experiments\n",
            "\n",
            "The models are evaluated ﬁrst on the task of discrim-\n",
            "inating the family of the pet (Sect. 4.1), then on the one\n",
            "of discriminating their breed given the family (Sect. 4.2),\n",
            "and ﬁnally discriminating both the family and the breed\n",
            "(Sect. 4.3). For the third task, both hierarchical classiﬁca-\n",
            "tion (i.e, determining ﬁrst the family and then the breed)\n",
            "and ﬂat classiﬁcation (i.e, determining the family and the\n",
            "breed simultaneously) are evaluated. Training uses the\n",
            "Oxford-IIIT Pet train and validation data and testing uses\n",
            "the Oxford-IIIT Pet test data. All these results are summa-\n",
            "rized in Tab. 4 and further results for pet family discrimina-\n",
            "tion on the ASIRRA data are reported in Sect. 4.1. Failure\n",
            "cases are reported in Fig. 7.\n",
            "\n",
            "Baseline.\n",
            "In order to compare the difﬁculty of the Oxford-\n",
            "IIIT Pet dataset\n",
            "to other Fine Grained Visual Catego-\n",
            "rization datasets, and also to provide a baseline for our\n",
            "breed classiﬁcation task, we have run the publicly available\n",
            "VLFeat [40] BoW classiﬁcation code over three datasets:\n",
            "Oxford Flowers 102 [33], UCSD-Caltech Birds [14], and\n",
            "Oxford-IIIT Pet dataset (note that this code is a faster suc-\n",
            "cessor to the VGG-MKL package [41] used on the UCSD-\n",
            "Caltech Birds dataset in [14]). The code employs a spatial\n",
            "pyramid [30], but does not use segmentation or salient parts.\n",
            "The results are given in Table 3.\n",
            "\n",
            "4.1. Pet family discrimination\n",
            "\n",
            "This section evaluates the different models on the task\n",
            "of discriminating the family of a pet (cat Vs dog classiﬁca-\n",
            "tion).\n",
            "\n",
            "Shape only. The maximum response of the cat face detec-\n",
            "tor (Sect. 3.1) on an image is used as an image-level score\n",
            "\n",
            "\f.\n",
            "\n",
            "Shape\n",
            "\n",
            "Appearance\n",
            "\n",
            "Classiﬁcation Accuracy (%)\n",
            "\n",
            "layout type\n",
            "\n",
            "using ground truth\n",
            "\n",
            "both (S. 4.3)\n",
            "\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "\n",
            "(cid:88)\n",
            "–\n",
            "–\n",
            "–\n",
            "–\n",
            "(cid:88)\n",
            "(cid:88)\n",
            "(cid:88)\n",
            "(cid:88)\n",
            "\n",
            "–\n",
            "Image\n",
            "Image+Head\n",
            "Image+Head+Body\n",
            "Image+Head+Body\n",
            "Image\n",
            "Image+Head\n",
            "Image+Head+Body\n",
            "Image+Head+Body\n",
            "\n",
            "–\n",
            "–\n",
            "–\n",
            "–\n",
            "(cid:88)\n",
            "–\n",
            "–\n",
            "–\n",
            "(cid:88)\n",
            "\n",
            "family\n",
            "(S. 4.1)\n",
            "94.21\n",
            "82.56\n",
            "85.06\n",
            "87.78\n",
            "88.68\n",
            "94.88\n",
            "95.07\n",
            "94.89\n",
            "95.37\n",
            "\n",
            "breed (S. 4.2)\n",
            "dog\n",
            "cat\n",
            "NA\n",
            "NA\n",
            "40.59\n",
            "52.01\n",
            "52.10\n",
            "60.37\n",
            "54.31\n",
            "64.27\n",
            "57.29\n",
            "66.12\n",
            "42.94\n",
            "50.27\n",
            "54.56\n",
            "59.11\n",
            "55.68\n",
            "63.48\n",
            "59.18\n",
            "66.07\n",
            "\n",
            "hierarchical\n",
            "NA\n",
            "NA\n",
            "NA\n",
            "NA\n",
            "NA\n",
            "42.29\n",
            "52.78\n",
            "55.26\n",
            "57.77\n",
            "\n",
            "ﬂat\n",
            "NA\n",
            "39.64\n",
            "51.23\n",
            "54.05\n",
            "56.60\n",
            "43.30\n",
            "54.03\n",
            "56.68\n",
            "59.21\n",
            "\n",
            "Table 4. Comparison between different models. The table compares different models on the three tasks of discriminating the family, the\n",
            "breed given the family, and the breed and family of the pets in the Oxford-IIIT Pet dataset (Sect. 2). Different combinations of the shape\n",
            "features (deformable part model of the pet faces) and of the various appearance features are tested (Sect. 3.2, Fig. 4).\n",
            "\n",
            "for the cat class. The same is done to obtain a score for\n",
            "the dog class. Then a linear SVM is learned to discriminate\n",
            "between cats and dogs based on these two scores. The clas-\n",
            "siﬁcation accuracy of this model on the Oxford-IIIT Pet test\n",
            "data is 94.21%.\n",
            "\n",
            "Appearance only. Spatial histograms of visual words are\n",
            "used in a non-linear SVM to discriminate between cats and\n",
            "dogs, as detailed in Sect. 3.2. The accuracy depends on\n",
            "the type of spatial histograms considered, which in turn\n",
            "depends on the layout of the spatial bins. On the Oxford-\n",
            "IIIT Pet test data, the image layout obtains an accuracy of\n",
            "82.56%; adding head information using image+head layout\n",
            "yields an accuracy of 85.06%. Using image+head+body\n",
            "layout improves accuracy by a further 2.7% to 87.78%. An\n",
            "improvement of 1% was observed when the ground-truth\n",
            "segmentations were used in place of the segmentations es-\n",
            "timated by grab-cut (Sect. 3.2). This progression indicates\n",
            "that the more accurate the localization of the pet body, the\n",
            "better is the classiﬁcation accuracy.\n",
            "\n",
            "Shape and appearance. The appearance and shape infor-\n",
            "mation are combined by summing the exp-χ2 kernel for the\n",
            "appearance part (Sect. 3.2) with a linear kernel on the cat\n",
            "scores and a linear kernel on the dog scores. The combina-\n",
            "tion boosts the performance by an additional 7% over that\n",
            "of using appearance alone, yielding approximately 95.37%\n",
            "accuracy (Table 4, rows 5 and 9), with all the variants of the\n",
            "appearance model performing similarly.\n",
            "\n",
            "Method\n",
            "Golle et al. [25]\n",
            "This paper (Shape only)\n",
            "\n",
            "Mean Class. Accuracy\n",
            "82.7%\n",
            "92.9%\n",
            "\n",
            "Table 5. Performance on ASIRRA Data. Table shows perfor-\n",
            "mance achieved on task of pet family classiﬁcation posed by the\n",
            "ASIRRA challenge. Best results obtained by Golle [25] were ob-\n",
            "tained using 10000 images from the data. 8000 for training and\n",
            "2000 for testing. Our test results are shown on 24990 images in\n",
            "the ASIRRA dataset.\n",
            "\n",
            "92.9%, which corresponds to a 42% probability of breaking\n",
            "the test in a single try. For comparison, the best accuracy re-\n",
            "ported in the literature on the ASIRRA data is 82.7% [25],\n",
            "which corresponds to just a 9.2% chance of breaking the\n",
            "test. Due to lack of sufﬁcient training data to train appear-\n",
            "ance models for ASIRRA data, we did not evaluate these\n",
            "models on ASIRRA dataset.\n",
            "\n",
            "4.2. Breed discrimination\n",
            "\n",
            "This section evaluates the models on the task of discrimi-\n",
            "nating the different breeds of cats and dogs given their fam-\n",
            "ily. This is done by learning a multi-class SVM by using\n",
            "the 1-Vs-rest decomposition [37] (this means learning 12\n",
            "binary classiﬁers for cats and 25 for dogs). The relative per-\n",
            "formance of the different models is similar to that observed\n",
            "for pet family classiﬁcation in Sect. 4.1. The best breed\n",
            "classiﬁcation accuracies for cats and dogs are 63.48% and\n",
            "55.68% respectively, which improve to 66.07% and 59.18%\n",
            "when the ground truth segmentations are used.\n",
            "\n",
            "4.3. Family and breed discrimination\n",
            "\n",
            "The ASIRRA data. The ASIRRA data does not specify a\n",
            "training set, so we used models trained on the Oxford-IIIT\n",
            "Pet data and the ASIRRA data was used only for testing.\n",
            "The accuracy of the shape model on the ASIRRA data is\n",
            "\n",
            "This section investigates classifying both the family and\n",
            "the breed. Two approaches are explored: hierarchical clas-\n",
            "siﬁcation, in which the family is decided ﬁrst as in Sect. 4.1,\n",
            "and then the breed is decided as in Sect. 4.2, and ﬂat classi-\n",
            "\n",
            "\fFigure 6. Confusion matrix for breed discrimination. The ver-\n",
            "tical axis reports the ground truth labels, and the horizontal axis to\n",
            "the predicted ones (the upper-left block are the cats). The matrix is\n",
            "normalized by row and the values along the diagonal are reported\n",
            "on the right. The matrix corresponds to the breed classiﬁer using\n",
            "shape features, appearance features with the image, head, body,\n",
            "body-head layouts with automatic segmentations, and a 37-class\n",
            "SVM. This is the best result for breed classiﬁcation, and corre-\n",
            "sponds to the last entry of row number 8 in Tab. 4.\n",
            "\n",
            "Figure 5. Example segmentation results on Oxford-IIIT Pet\n",
            "dataset. The segmentation of the pet from the background was\n",
            "obtained automatically as described in Sect. 3.3.\n",
            "\n",
            "e\n",
            "\n",
            "f\n",
            "\n",
            "g\n",
            "\n",
            "a\n",
            "\n",
            "b\n",
            "\n",
            "c\n",
            "\n",
            "d\n",
            "\n",
            "h\n",
            "\n",
            "ﬁcation, in which a 37-class SVM is learned directly, using\n",
            "the same method discussed in Sect. 4.2. The relative per-\n",
            "formance of the different models is similar to that observed\n",
            "in Sect. 4.1 and 4.2. Flat classiﬁcation is better than hier-\n",
            "archical, but the latter requires less work at test time, due\n",
            "to the fact that fewer SVM classiﬁers need to be evaluated.\n",
            "For example, using the appearance model with the image,\n",
            "head, image-head layouts for 37 class classiﬁcation yields\n",
            "an accuracy of 51.23%, adding the shape information hi-\n",
            "erarchically improves this accuracy to 52.78%, and using\n",
            "shape and appearance together in a ﬂat classiﬁcation ap-\n",
            "proach achieves an accuracy 54.03%. The confusion matrix\n",
            "for the best result for breed classiﬁcation, corresponding to\n",
            "the last entry of the eight row of Table 4 is shown in Fig. 4.\n",
            "\n",
            "Figure 7. Failure cases for the model using appearance only (im-\n",
            "age layout) in Sect. 4.2. First row: Cat images that were incor-\n",
            "rectly classiﬁed as dogs and vice versa. Second row: Bengal cats\n",
            "(b–d) classiﬁed as Egyptian Mau (a). Third row: English Setter\n",
            "(f–h) classiﬁed as English Cocker Spaniel (e).\n",
            "\n",
            "5. Summary\n",
            "\n",
            "This paper has introduced the Oxford-IIIT Pet dataset\n",
            "for the ﬁne-grained categorisation problem of identifying\n",
            "the family and breed of pets (cats and dogs). Three differ-\n",
            "ent tasks and corresponding baseline algorithms have been\n",
            "proposed and investigated obtaining very encouraging clas-\n",
            "siﬁcation results on the dataset. Furthermore, the baseline\n",
            "models were shown to achieve state-of-the-art performance\n",
            "on the ASIRRA challenge data, breaking the test with 42%\n",
            "\n",
            "35.7%39.0%77.0%81.8%69.0%71.1%60.0%64.0%51.0%46.0%70.0%82.0%52.0%4.0%62.0%33.0%38.4%20.0%29.0%43.0%80.0%70.0%51.0%82.0%75.8%53.0%39.0%82.0%28.0%85.0%59.0%91.0%66.7%57.0%37.1%53.0%50.0%12345678910111213141516171819202122232425262728293031323334353637Abyssinian  1Bengal  2Birman  3Bombay  4British Shorthair  5Egyptian Mau  6Maine Coon  7Persian  8Ragdoll  9Russian Blue 10Siamese 11Sphynx 12Am. Bulldog 13Am. Pit Bull Terrier 14Basset Hound 15Beagle 16Boxer 17Chihuahua 18Eng. Cocker Spaniel 19Eng. Setter 20German Shorthaired 21Great Pyrenees 22Havanese 23Japanese Chin 24Keeshond 25Leonberger 26Miniature Pinscher 27Newfoundland 28Pomeranian 29Pug 30Saint Bernard 31Samoyed 32Scottish Terrier 33Shiba Inu 34Staff. Bull Terrier 35Wheaten Terrier 36Yorkshire Terrier 37\fprobability, a remarkable achievement considering that this\n",
            "dataset was designed to be challenging for machines.\n",
            "\n",
            "Acknowledgements. We are grateful for ﬁnancial sup-\n",
            "port from the UKIERI, EU Project AXES ICT-269980 and\n",
            "ERC grant VisRec no. 228180.\n",
            "\n",
            "References\n",
            "\n",
            "[1] American kennel club. http://www.akc.org/.\n",
            "[2] The cat fanciers association inc.\n",
            "org/Client/home.aspx.\n",
            "\n",
            "http://www.cfa.\n",
            "\n",
            "[3] Cats in sinks. http://catsinsinks.com/.\n",
            "[4] Catster. http://www.catster.com/.\n",
            "[5] Dogster. http://www.dogster.com/.\n",
            "[6] Flickr! http://www.flickr.com/.\n",
            "[7] Google images. http://images.google.com/.\n",
            "[8] The international cat association. http://www.tica.\n",
            "\n",
            "org/.\n",
            "\n",
            "[9] My cat space. http://www.mycatspace.com/.\n",
            "[10] My dog space. http://www.mydogspace.com/.\n",
            "[11] Petﬁnder.\n",
            "html.\n",
            "\n",
            "http://www.petfinder.com/index.\n",
            "\n",
            "[12] World canine organisation. http://www.fci.be/.\n",
            "[13] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. From con-\n",
            "tours to regions: An empirical evaluation. In Proc. CVPR,\n",
            "2009.\n",
            "\n",
            "[14] S. Branson, C. Wah, F. Schroff, B. Babenko, P. Welinder,\n",
            "P. Perona, and S. Belongie. Visual recognition with humans\n",
            "in the loop. In Proc. ECCV, 2010.\n",
            "\n",
            "[15] Y. Chai, V. Lempitsky, and A. Zisserman. Bicos: A bi-level\n",
            "In Proc.\n",
            "\n",
            "co-segmentation method for image classiﬁcation.\n",
            "ICCV, 2011.\n",
            "\n",
            "[16] G. Csurka, C. R. Dance, L. Dan, J. Willamowski, and\n",
            "C. Bray. Visual categorization with bags of keypoints.\n",
            "In\n",
            "Proc. ECCV Workshop on Stat. Learn. in Comp. Vision,\n",
            "2004.\n",
            "\n",
            "[17] N. Dalal and B. Triggs. Histograms of oriented gradients for\n",
            "\n",
            "human detection. In Proc. CVPR, 2005.\n",
            "\n",
            "[18] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\n",
            "ImageNet: A Large-Scale Hierarchical Image Database. In\n",
            "Proc. CVPR, 2009.\n",
            "\n",
            "[19] J. Elson, J. Douceur, J. Howell, and J. J. Saul. Asirra: A\n",
            "CAPTCHA that exploits interest-aligned manual image cat-\n",
            "egorization. In Conf. on Computer and Communications Se-\n",
            "curity (CCS), 2007.\n",
            "\n",
            "[20] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\n",
            "and A. Zisserman. The PASCAL Visual Object Classes\n",
            "Challenge 2011 (VOC2011) Results.\n",
            "http://www.pascal-\n",
            "network.org/challenges/VOC/voc2011/workshop/index.html.\n",
            "[21] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J.\n",
            "Lin. LIBLINEAR: A library for large linear classiﬁcation.\n",
            "Journal of Machine Learning Research, 9, 2008.\n",
            "\n",
            "[22] L. Fei-Fei, R. Fergus, and P. Perona. A Bayesian approach to\n",
            "unsupervised one-shot learning of object categories. In Proc.\n",
            "ICCV, 2003.\n",
            "\n",
            "[23] P. F. Felzenszwalb, R. B. Grishick, D. McAllester, and D. Ra-\n",
            "manan. Object detection with discriminatively trained part\n",
            "based models. PAMI, 2009.\n",
            "\n",
            "[24] F. Fleuret and D. Geman. Stationary features and cat detec-\n",
            "tion. Journal of Machine Learning Research, 9, 2008.\n",
            "[25] P. Golle. Machine learning attacks against the asirra captcha.\n",
            "In 15th ACM Conference on Computer and Communications\n",
            "Security (CCS), 2008.\n",
            "\n",
            "[26] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object cat-\n",
            "egory dataset. Technical report, California Institute of Tech-\n",
            "nology, 2007.\n",
            "\n",
            "[27] A. Khosla, N. Jayadevaprakash, B. Yao, and F. F. Li. Novel\n",
            "dataset for ﬁne-grained image categorization. In First Work-\n",
            "shop on Fine-Grained Visual Categorization, CVPR, 2011.\n",
            "\n",
            "[28] C. Lampert, H. Nickisch, and S. Harmeling. Learning to de-\n",
            "tect unseen object classes by between-class attribute transfer.\n",
            "In Proc. CVPR, 2009.\n",
            "\n",
            "[29] I. Laptev. Improvements of object detection using boosted\n",
            "\n",
            "histograms. In Proc. BMVC, 2006.\n",
            "\n",
            "[30] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bag of\n",
            "features: Spatial pyramid matching for recognizing natural\n",
            "scene categories. In Proc. CVPR, 2006.\n",
            "\n",
            "[31] D. G. Lowe. Object recognition from local scale-invariant\n",
            "\n",
            "features. In Proc. ICCV, 1999.\n",
            "\n",
            "[32] M.-E. Nilsback and A. Zisserman. A visual vocabulary for\n",
            "\n",
            "ﬂower classiﬁcation. In Proc. CVPR, 2006.\n",
            "\n",
            "[33] M.-E. Nilsback and A. Zisserman. Automated ﬂower clas-\n",
            "siﬁcation over a large number of classes. In Proc. ICVGIP,\n",
            "2008.\n",
            "\n",
            "[34] O. Parkhi, A. Vedaldi, C. V. Jawahar, and A. Zisserman. The\n",
            "\n",
            "truth about cats and dogs. In Proc. ICCV, 2011.\n",
            "\n",
            "[35] O. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar. The\n",
            "Oxford-IIIT PET Dataset. http://www.robots.ox.\n",
            "ac.uk/˜vgg/data/pets/index.html, 2012.\n",
            "[36] C. Rother, V. Kolmogorov, and A. Blake. “grabcut” — in-\n",
            "teractive foreground extraction using iterated graph cuts. In\n",
            "ACM Trans. on Graphics, 2004.\n",
            "\n",
            "[37] B. Sch¨olkopf and A. J. Smola. Learning with Kernels. MIT\n",
            "\n",
            "Press, 2002.\n",
            "\n",
            "[38] J. Sivic and A. Zisserman. Video Google: A text retrieval\n",
            "approach to object matching in videos. In Proc. ICCV, 2003.\n",
            "[39] M. Varma and D. Ray. Learning the discriminative power-\n",
            "\n",
            "invariance trade-off. In Proc. ICCV, 2007.\n",
            "\n",
            "[40] A. Vedaldi and B. Fulkerson. VLFeat library. http://\n",
            "\n",
            "www.vlfeat.org/, 2008.\n",
            "\n",
            "[41] A. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman. Mul-\n",
            "tiple kernels for object detection. In Proc. ICCV, 2009.\n",
            "[42] A. Vedaldi and A. Zisserman. Structured output regression\n",
            "\n",
            "for detection with partial occulsion. In Proc. NIPS, 2009.\n",
            "\n",
            "[43] P. Welinder, S. Branson, T. Mita, C. Wah, and F. Schroff.\n",
            "Caltech-ucsd birds 200. Technical report, Caltech-UCSD,\n",
            "2010.\n",
            "\n",
            "[44] J. Zhang, M. Marszalek, S. Lazebnik, and C. Schmid. Local\n",
            "features and kernels for classiﬁcation of texture and object\n",
            "categories: A comprehensive study. IJCV, 2007.\n",
            "\n",
            "[45] W. Zhang, J. Sun, and X. Tang. Cat head detection - how\n",
            "In Proc.\n",
            "\n",
            "to effectively exploit shape and texture features.\n",
            "ECCV, 2008.\n",
            "\n",
            "\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U79jRurkPfEG"
      },
      "source": [
        "https://pdfminersix.readthedocs.io/_/downloads/en/latest/pdf/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TgP__CpOb35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f694d11-470a-41e0-d8b4-9fd9203d3768"
      },
      "source": [
        "from pdfminer.high_level import extract_pages\n",
        "\n",
        "for page_layout in extract_pages(\"/content/example01.pdf\"):\n",
        "  for element in page_layout:\n",
        "    print(element)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<LTTextBoxHorizontal(0) 253.376,672.069,341.849,686.415 'Cats and Dogs\\n'>\n",
            "<LTTextBoxHorizontal(1) 89.949,584.770,481.027,647.972 'Omkar M Parkhi1,2 Andrea Vedaldi1 Andrew Zisserman1 C. V. Jawahar2\\n1Department of Engineering Science,\\nUniversity of Oxford,\\nUnited Kingdom\\n{omkar,vedaldi,az}@robots.ox.ac.uk\\n'>\n",
            "<LTTextBoxHorizontal(2) 325.420,584.774,503.762,627.596 '2Center for Visual Information Technology,\\nInternational Institute of Information Technology,\\nHyderabad, India\\njawahar@iiit.ac.in\\n'>\n",
            "<LTTextBoxHorizontal(3) 145.995,544.907,190.480,556.863 'Abstract\\n'>\n",
            "<LTTextBoxHorizontal(4) 50.112,449.108,286.366,530.802 'We investigate the ﬁne grained object categorization\\nproblem of determining the breed of animal from an image.\\nTo this end we introduce a new annotated dataset of pets,\\nthe Oxford-IIIT-Pet dataset, covering 37 different breeds of\\ncats and dogs. The visual problem is very challenging as\\nthese animals, particularly cats, are very deformable and\\nthere can be quite subtle differences between the breeds.\\n'>\n",
            "<LTTextBoxHorizontal(5) 50.112,316.867,286.365,446.382 'We make a number of contributions: ﬁrst, we introduce a\\nmodel to classify a pet breed automatically from an image.\\nThe model combines shape, captured by a deformable part\\nmodel detecting the pet face, and appearance, captured by\\na bag-of-words model that describes the pet fur. Fitting the\\nmodel involves automatically segmenting the animal in the\\nimage. Second, we compare two classiﬁcation approaches:\\na hierarchical one, in which a pet is ﬁrst assigned to the cat\\nor dog family and then to a breed, and a ﬂat one, in which\\nthe breed is obtained directly. We also investigate a number\\nof animal and image orientated spatial layouts.\\n'>\n",
            "<LTTextBoxHorizontal(6) 50.112,244.403,286.365,314.141 'These models are very good: they beat all previously\\npublished results on the challenging ASIRRA test (cat vs\\ndog discrimination). When applied to the task of discrimi-\\nnating the 37 different breeds of pets, the models obtain an\\naverage accuracy of about 59%, a very encouraging result\\nconsidering the difﬁculty of the problem.\\n'>\n",
            "<LTTextBoxHorizontal(7) 50.112,205.760,126.948,217.716 '1. Introduction\\n'>\n",
            "<LTTextBoxHorizontal(8) 50.112,78.848,286.365,196.407 'Research on object category recognition has largely fo-\\ncused on the discrimination of well distinguished object cat-\\negories (e.g, airplane vs cat). Most popular international\\nbenchmarks (e.g, Caltech-101 [22], Caltech-256 [26], PAS-\\nCAL VOC [20]) contain a few dozen object classes that,\\nfor the most part, are visually dissimilar. Even in the much\\nlarger ImageNet database [18], categories are deﬁned based\\non a high-level ontology and, as such, any visual similar-\\nity between them is more accidental than systematic. This\\nwork concentrates instead on the problem of discriminat-\\n'>\n",
            "<LTTextBoxHorizontal(9) 308.862,425.702,545.117,555.446 'ing different breeds of cats and dogs, a challenging exam-\\nple of ﬁne grained object categorization in line with that of\\nprevious work on ﬂower [15, 32, 33, 39] and animal and\\nbird species [14, 27, 28, 43] categorization. The difﬁculty\\nis in the fact that breeds may differ only by a few subtle\\nphenotypic details that, due to the highly deformable na-\\nture of the bodies of such animals, can be difﬁcult to mea-\\nsure automatically. Indeed, authors have often focused on\\ncats and dogs as examples of highly deformable objects\\nfor which recognition and detection is particularly challeng-\\ning [24, 29, 34, 45].\\n'>\n",
            "<LTTextBoxHorizontal(10) 308.862,258.252,545.115,423.633 'Beyond the technical interest of ﬁne grained categoriza-\\ntion, extracting information from images of pets has a prac-\\ntical side too. People devote a lot of attention to their do-\\nmestic animals, as suggested by the large number of so-\\ncial networks dedicated to the sharing of images of cats\\nand dogs: Pet Finder [11], Catster [4], Dogster [5], My\\nCat Space [9], My Dog Space [10], The International Cat\\nAssociation [8] and several others [1, 2, 3, 12].\\nIn fact,\\nthe bulk of the data used in this paper has been extracted\\nfrom annotated images that users of these social sites post\\ndaily (Sect. 2). It is not unusual for owners to believe (and\\npost) the incorrect breed for their pet, so having a method\\nof automated classiﬁcation could provide a gentle way of\\nalerting them to such errors.\\n'>\n",
            "<LTTextBoxHorizontal(11) 308.862,78.848,545.115,256.183 'The ﬁrst contribution of this paper is the introduction of a\\nlarge annotated collection of images of 37 different breeds\\nof cats and dogs (Sect. 2).\\nIt includes 12 cat breeds and\\n25 dog breeds. This data constitutes the benchmark for pet\\nbreed classiﬁcation, and, due to its focus on ﬁne grained cat-\\negorization, is complementary to the standard object recog-\\nnition benchmarks. The data, which is publicly available,\\ncomes with rich annotations: in addition to a breed label,\\neach pet has a pixel level segmentation and a rectangle lo-\\ncalising its head. A simple evaluation protocol, inspired by\\nthe PASCAL VOC challenge, is also proposed to enable\\nthe comparison of future methods on a common grounds\\n(Sect. 2). This dataset is also complementary to the subset\\nof ImageNet used in [27] for dogs, as it contains additional\\nannotations, though for fewer breeds.\\n'>\n",
            "<LTTextBoxHorizontal(12) 295.121,48.960,300.102,58.923 '1\\n'>\n",
            "<LTTextBoxHorizontal(0) 308.862,660.065,545.115,717.848 'VOC data. The dataset contains about 200 images for each\\nbreed (which have been split randomly into 50 for training,\\n50 for validation, and 100 for testing). A detailed list of\\nbreeds is given in Tab. 1, and example images are given in\\nFig. 2. The dataset is available at [35].\\n'>\n",
            "<LTTextBoxHorizontal(1) 308.862,382.639,545.115,643.730 'Dataset collection. The pet images were downloaded\\nfrom Catster [4] and Dogster [5], two social web sites ded-\\nicated to the collection and discussion of images of pets,\\nfrom Flickr [6] groups, and from Google images [7]. Peo-\\nple uploading images to Catster and Dogster provide the\\nbreed information as well, and the Flickr groups are spe-\\nciﬁc to each breed, which simpliﬁes tagging. For each of\\nthe 37 breeds, about 2,000 – 2,500 images were down-\\nloaded from these data sources to form a pool of candidates\\nfor inclusion in the dataset. From this candidate list, im-\\nages were dropped if any of the following conditions ap-\\nplied, as judged by the annotators: (i) the image was gray\\nscale, (ii) another image portraying the same animal existed\\n(which happens frequently in Flickr), (iii) the illumination\\nwas poor, (iv) the pet was not centered in the image, or (v)\\nthe pet was wearing clothes. The most common problem\\nin all the data sources, however, was found to be errors in\\nthe breed labels. Thus labels were reviewed by the human\\nannotators and ﬁxed whenever possible. When ﬁxing was\\nnot possible, for instance because the pet was a cross breed,\\nthe image was dropped. Overall, up to 200 images for each\\nof the 37 breeds were obtained.\\n'>\n",
            "<LTTextBoxHorizontal(2) 308.862,284.542,545.115,366.305 'Annotations. Each image is annotated with a breed label,\\na pixel level segmentation marking the body, and a tight\\nbounding box about the head. The segmentation is a trimap\\nwith regions corresponding to: foreground (the pet body),\\nbackground, and ambiguous (the pet body boundary and\\nany accessory such as collars). Fig. 1 shows examples of\\nthese annotations.\\n'>\n",
            "<LTTextBoxHorizontal(3) 308.862,78.848,545.115,268.207 'Evaluation protocol. Three tasks are deﬁned: pet family\\nclassiﬁcation (Cat vs Dog, a two class problem), breed clas-\\nsiﬁcation given the family (a 12 class problem for cats and\\na 25 class problem for dogs), and breed and family classi-\\nﬁcation (a 37 class problem). In all cases, the performance\\nis measured as the average per-class classiﬁcation accuracy.\\nThis is the proportion of correctly classiﬁed images for each\\nof the classes and can be computed as the average of the\\ndiagonal of the (row normalized) confusion matrix. This\\nmeans that, for example, a random classiﬁer has average ac-\\ncuracy of 1/2 = 50% for the family classiﬁcation task, and\\nof 1/37 ≈ 3% for the breed and family classiﬁcation task.\\nAlgorithms are trained on the training and validation sub-\\nsets and tested on the test subset. The split between training\\nand validation is provided only for convenience, but can be\\ndisregarded.\\n'>\n",
            "<LTTextBoxHorizontal(4) 50.112,547.048,286.364,588.954 'Figure 1. Annotations in the Oxford-IIIT Pet data. From left\\nto right: pet image, head bounding box, and trimap segmentation\\n(blue: background region; red: ambiguous region; yellow: fore-\\nground region).\\n'>\n",
            "<LTTextBoxHorizontal(5) 50.112,250.797,286.365,523.774 'The second contribution of the paper is a model for pet\\nbreed discrimination (Sect. 3). The model captures both\\nshape (by a deformable part model [23, 42] of the pet face)\\nand texture (by a bag-of-visual-words model [16, 30, 38, 44]\\nof the pet fur). Unfortunately, current deformable part mod-\\nels are not sufﬁciently advanced to represent satisfactorily\\nthe highly deformable bodies of cats and dogs; nevertheless,\\nthey can be used to reliably extract stable and distinctive\\ncomponents of the body, such as the pet face. The method\\nused in [34] followed from this observation: a cat’s face\\nwas detected as the ﬁrst stage in detecting the entire animal.\\nHere we go further in using the detected head shape as a part\\nof the feature descriptor. Two natural ways of combining\\nthe shape and appearance features are then considered and\\ncompared: a ﬂat approach, in which both features are used\\nto regress the pet’s family and the breed simultaneously, and\\na hierarchical one, in which the family is determined ﬁrst\\nbased on the shape features alone, and then appearance is\\nused to predict the breed conditioned on the family. Infer-\\nring the model in an image involves segmenting the animal\\nfrom the background. To this end, we improved on our pre-\\nvious method on of segmentation in [34] basing it on the\\nextraction of superpixels.\\n'>\n",
            "<LTTextBoxHorizontal(6) 50.112,178.469,286.365,248.208 'The model is validated experimentally on the task of dis-\\ncriminating the 37 pet breeds (Sect. 4), obtaining very en-\\ncouraging results, especially considering the toughness of\\nthe problem. Furthermore, we also use the model to break\\nthe ASIRRA test that uses the ability of discriminating be-\\ntween cats and dogs to tell humans from machines.\\n'>\n",
            "<LTTextBoxHorizontal(7) 50.112,152.422,233.899,164.378 '2. Datasets and evaluation measures\\n'>\n",
            "<LTTextBoxHorizontal(8) 50.112,133.105,201.816,144.063 '2.1. The Oxford-IIIT Pet dataset\\n'>\n",
            "<LTTextBoxHorizontal(9) 50.112,78.848,286.365,124.905 'The Oxford-IIIT Pet dataset is a collection of 7,349 im-\\nages of cats and dogs of 37 different breeds, of which 25\\nare dogs and 12 are cats. Images are divided into training,\\nvalidation, and test sets, in a similar manner to the PASCAL\\n'>\n",
            "<LTFigure(Im1) 50.112,663.307,125.704,720.001 matrix=[0.38,0.00,0.00,0.38, (50.11,663.31)]>\n",
            "<LTFigure(Im2) 129.203,663.307,204.788,719.996 matrix=[0.15,0.00,0.00,0.15, (129.20,663.31)]>\n",
            "<LTFigure(Im3) 208.287,663.307,283.872,719.996 matrix=[0.15,0.00,0.00,0.15, (208.29,663.31)]>\n",
            "<LTFigure(Im4) 50.112,602.783,125.768,659.475 matrix=[0.20,0.00,0.00,0.20, (50.11,602.78)]>\n",
            "<LTFigure(Im5) 130.481,602.783,206.066,659.472 matrix=[0.15,0.00,0.00,0.15, (130.48,602.78)]>\n",
            "<LTFigure(Im6) 210.778,602.783,286.363,659.472 matrix=[0.15,0.00,0.00,0.15, (210.78,602.78)]>\n",
            "<LTTextBoxHorizontal(0) 91.571,709.151,116.846,719.113 'Breed\\n'>\n",
            "<LTTextBoxHorizontal(1) 53.230,481.534,155.187,706.690 'Abyssinian\\nBengal\\nBirman\\nBombay\\nBritish Shorthair\\nEgyptian Mau\\nMaine Coon\\nPersian\\nRagdoll\\nRussian Blue\\nSiamese\\nSphynx\\nAmerican Bulldog\\nAmerican Pit Bull Terrier\\nBasset Hound\\nBeagle\\nBoxer\\nChihuahua\\nEnglish Cocker Spaniel\\n'>\n",
            "<LTTextBoxHorizontal(2) 346.117,709.151,371.392,719.113 'Breed\\n'>\n",
            "<LTTextBoxHorizontal(3) 308.319,648.907,365.594,658.870 'Japanese Chin\\n'>\n",
            "<LTTextBoxHorizontal(4) 161.423,481.534,300.090,719.113 'Training Validation Test Total\\n50\\n50\\n50\\n47\\n50\\n46\\n50\\n50\\n50\\n50\\n49\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n46\\n'>\n",
            "<LTTextBoxHorizontal(5) 255.931,481.534,270.875,706.690 '98\\n100\\n100\\n88\\n100\\n97\\n100\\n100\\n100\\n100\\n100\\n100\\n100\\n100\\n100\\n100\\n99\\n100\\n100\\n'>\n",
            "<LTTextBoxHorizontal(6) 281.729,481.534,389.385,706.690 '198 English Setter\\n200 German Shorthaired\\n200 Great Pyrenees\\n184 Havanese\\n200\\n190 Keeshond\\n200 Leonberger\\n200 Miniature Pinscher\\n200 Newfoundland\\n200\\n199\\n200\\n200\\n200\\n200\\n200\\n199 Wheaten Terrier\\n200 Yorkshire Terrier\\n196\\n'>\n",
            "<LTTextBoxHorizontal(7) 308.319,517.400,409.190,599.094 'Pomeranian\\nPug\\nSaint Bernard\\nSamoyed\\nScottish Terrier\\nShiba Inu\\nStaffordshire Bull Terrier\\n'>\n",
            "<LTTextBoxHorizontal(8) 174.897,481.534,184.860,706.690 '50\\n50\\n50\\n49\\n50\\n47\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n'>\n",
            "<LTTextBoxHorizontal(9) 347.865,481.604,369.643,491.566 'Total\\n'>\n",
            "<LTTextBoxHorizontal(10) 415.426,481.534,556.673,719.113 'Training Validation Test Total\\n200\\n100\\n200\\n100\\n200\\n100\\n200\\n100\\n200\\n100\\n199\\n99\\n200\\n100\\n200\\n100\\n196\\n100\\n200\\n100\\n200\\n100\\n200\\n100\\n200\\n100\\n199\\n99\\n200\\n100\\n189\\n89\\n200\\n100\\n100\\n200\\n3669 7349\\n'>\n",
            "<LTTextBoxHorizontal(11) 423.918,481.534,443.843,706.690 '50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n1846\\n'>\n",
            "<LTTextBoxHorizontal(12) 470.573,481.534,490.498,706.690 '50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n46\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n1834\\n'>\n",
            "<LTTextBoxHorizontal(13) 126.683,457.105,468.542,466.134 'Table 1. Oxford-IIIT Pet data composition. The 12 cat breeds followed by the 25 dog breeds.\\n'>\n",
            "<LTTextBoxHorizontal(14) 122.371,412.215,142.770,416.748 'Abyssinian\\n'>\n",
            "<LTTextBoxHorizontal(15) 220.351,412.215,233.170,416.748 'Bengal\\n'>\n",
            "<LTTextBoxHorizontal(16) 300.151,412.215,315.513,416.748 'Bombay\\n'>\n",
            "<LTTextBoxHorizontal(17) 376.035,412.215,389.634,416.748 'Birman\\n'>\n",
            "<LTTextBoxHorizontal(18) 436.145,412.215,466.493,416.748 'British Shorthair\\n'>\n",
            "<LTTextBoxHorizontal(19) 520.632,412.215,543.170,416.748 'Maine Coon\\n'>\n",
            "<LTTextBoxHorizontal(20) 112.477,363.677,125.822,368.210 'Persian\\n'>\n",
            "<LTTextBoxHorizontal(21) 198.308,363.677,214.676,368.210 'Egyptian\\n'>\n",
            "<LTTextBoxHorizontal(22) 301.224,363.677,315.580,368.210 'Ragdoll\\n'>\n",
            "<LTTextBoxHorizontal(23) 394.854,363.677,418.906,368.210 'Russian Blue\\n'>\n",
            "<LTTextBoxHorizontal(24) 476.028,363.677,491.136,368.210 'Siamese\\n'>\n",
            "<LTTextBoxHorizontal(25) 529.339,363.677,543.169,368.210 'Sphynx\\n'>\n",
            "<LTTextBoxHorizontal(26) 122.685,315.138,143.101,319.671 'Eng. Setter\\n'>\n",
            "<LTTextBoxHorizontal(27) 229.603,315.138,240.613,319.671 'Boxer\\n'>\n",
            "<LTTextBoxHorizontal(28) 296.474,315.138,314.488,319.671 'Keeshond\\n'>\n",
            "<LTTextBoxHorizontal(29) 399.370,315.138,416.786,319.671 'Havanese\\n'>\n",
            "<LTTextBoxHorizontal(30) 467.258,315.138,492.565,319.671 'Basset Hound\\n'>\n",
            "<LTTextBoxHorizontal(31) 517.608,315.138,543.170,319.671 'Mini Pinscher\\n'>\n",
            "<LTTextBoxHorizontal(32) 123.382,266.599,143.023,271.132 'Chihuahua\\n'>\n",
            "<LTTextBoxHorizontal(33) 204.758,266.599,232.323,271.132 'Great Pyrenees\\n'>\n",
            "<LTTextBoxHorizontal(34) 278.419,266.599,315.304,271.132 'German Shorthaired\\n'>\n",
            "<LTTextBoxHorizontal(35) 405.392,266.599,417.980,271.132 'Beagle\\n'>\n",
            "<LTTextBoxHorizontal(36) 455.748,266.599,488.195,271.132 'Staff. Bull Terrier\\n'>\n",
            "<LTTextBoxHorizontal(37) 520.283,266.599,543.170,271.132 'Eng. Cocker\\n'>\n",
            "<LTTextBoxHorizontal(38) 109.212,218.060,140.757,222.593 'New Found Land\\n'>\n",
            "<LTTextBoxHorizontal(39) 211.733,218.060,233.387,222.593 'Pomeranian\\n'>\n",
            "<LTTextBoxHorizontal(40) 302.453,218.060,323.264,222.593 'Leonberger\\n'>\n",
            "<LTTextBoxHorizontal(41) 357.434,218.060,394.155,222.593 'Am. Pit Bull Terrier\\n'>\n",
            "<LTTextBoxHorizontal(42) 462.709,218.060,492.219,222.593 'Wheaten Terrier\\n'>\n",
            "<LTTextBoxHorizontal(43) 517.110,218.060,543.170,222.593 'Japanese Chin\\n'>\n",
            "<LTTextBoxHorizontal(44) 99.005,169.521,115.831,174.054 'Samoyed\\n'>\n",
            "<LTTextBoxHorizontal(45) 170.345,169.521,198.354,174.054 'Scottish Terrier\\n'>\n",
            "<LTTextBoxHorizontal(46) 276.527,169.521,294.029,174.054 'Shiba Inu\\n'>\n",
            "<LTTextBoxHorizontal(47) 396.624,169.521,403.677,174.054 'Pug\\n'>\n",
            "<LTTextBoxHorizontal(48) 457.576,169.521,482.630,174.054 'Saint Bernard\\n'>\n",
            "<LTTextBoxHorizontal(49) 517.082,169.521,543.169,174.054 'Am. Bull Dog\\n'>\n",
            "<LTTextBoxHorizontal(50) 50.112,142.013,545.109,151.042 'Figure 2. Example images from the Oxford-IIIT Pet data. Two images per breed are shown side by side to illustrate the data variability.\\n'>\n",
            "<LTTextBoxHorizontal(51) 50.112,109.831,168.523,120.789 '2.2. The ASIRRA dataset\\n'>\n",
            "<LTTextBoxHorizontal(52) 50.112,78.848,286.365,100.766 'Microsoft Research (MSR) proposed the problem of dis-\\ncriminating cats from dogs as a test to tell humans from ma-\\n'>\n",
            "<LTTextBoxHorizontal(53) 308.862,86.059,545.115,120.161 'chines, and created the ASIRRA test ([19], Fig. 3) on this ba-\\nsis. The assumption is that, out of a batch of twelve images\\nof pets, any machine would predict incorrectly the family\\n'>\n",
            "<LTLine 50.112,719.801,559.792,719.801>\n",
            "<LTLine 50.112,707.646,50.112,719.601>\n",
            "<LTLine 158.304,707.646,158.304,719.601>\n",
            "<LTLine 201.452,707.646,201.452,719.601>\n",
            "<LTLine 251.613,707.646,251.613,719.601>\n",
            "<LTLine 275.194,707.646,275.194,719.601>\n",
            "<LTLine 303.208,707.646,303.208,719.601>\n",
            "<LTLine 305.201,707.646,305.201,719.601>\n",
            "<LTLine 305.201,707.646,305.201,719.601>\n",
            "<LTLine 412.307,707.646,412.307,719.601>\n",
            "<LTLine 455.455,707.646,455.455,719.601>\n",
            "<LTLine 505.616,707.646,505.616,719.601>\n",
            "<LTLine 531.777,707.646,531.777,719.601>\n",
            "<LTLine 559.792,707.646,559.792,719.601>\n",
            "<LTLine 50.112,707.447,559.792,707.447>\n",
            "<LTLine 50.112,695.293,50.112,707.248>\n",
            "<LTLine 158.304,695.293,158.304,707.248>\n",
            "<LTLine 201.452,695.293,201.452,707.248>\n",
            "<LTLine 251.613,695.293,251.613,707.248>\n",
            "<LTLine 275.194,695.293,275.194,707.248>\n",
            "<LTLine 303.208,695.293,303.208,707.248>\n",
            "<LTLine 305.201,695.293,305.201,707.248>\n",
            "<LTLine 412.307,695.293,412.307,707.248>\n",
            "<LTLine 455.455,695.293,455.455,707.248>\n",
            "<LTLine 505.616,695.293,505.616,707.248>\n",
            "<LTLine 531.777,695.293,531.777,707.248>\n",
            "<LTLine 559.792,695.293,559.792,707.248>\n",
            "<LTLine 50.112,683.337,50.112,695.292>\n",
            "<LTLine 158.304,683.337,158.304,695.292>\n",
            "<LTLine 201.452,683.337,201.452,695.292>\n",
            "<LTLine 251.613,683.337,251.613,695.292>\n",
            "<LTLine 275.194,683.337,275.194,695.292>\n",
            "<LTLine 303.208,683.337,303.208,695.292>\n",
            "<LTLine 305.201,683.337,305.201,695.292>\n",
            "<LTLine 412.307,683.337,412.307,695.292>\n",
            "<LTLine 455.455,683.337,455.455,695.292>\n",
            "<LTLine 505.616,683.337,505.616,695.292>\n",
            "<LTLine 531.777,683.337,531.777,695.292>\n",
            "<LTLine 559.792,683.337,559.792,695.292>\n",
            "<LTLine 50.112,671.382,50.112,683.337>\n",
            "<LTLine 158.304,671.382,158.304,683.337>\n",
            "<LTLine 201.452,671.382,201.452,683.337>\n",
            "<LTLine 251.613,671.382,251.613,683.337>\n",
            "<LTLine 275.194,671.382,275.194,683.337>\n",
            "<LTLine 303.208,671.382,303.208,683.337>\n",
            "<LTLine 305.201,671.382,305.201,683.337>\n",
            "<LTLine 412.307,671.382,412.307,683.337>\n",
            "<LTLine 455.455,671.382,455.455,683.337>\n",
            "<LTLine 505.616,671.382,505.616,683.337>\n",
            "<LTLine 531.777,671.382,531.777,683.337>\n",
            "<LTLine 559.792,671.382,559.792,683.337>\n",
            "<LTLine 50.112,659.427,50.112,671.382>\n",
            "<LTLine 158.304,659.427,158.304,671.382>\n",
            "<LTLine 201.452,659.427,201.452,671.382>\n",
            "<LTLine 251.613,659.427,251.613,671.382>\n",
            "<LTLine 275.194,659.427,275.194,671.382>\n",
            "<LTLine 303.208,659.427,303.208,671.382>\n",
            "<LTLine 305.201,659.427,305.201,671.382>\n",
            "<LTLine 412.307,659.427,412.307,671.382>\n",
            "<LTLine 455.455,659.427,455.455,671.382>\n",
            "<LTLine 505.616,659.427,505.616,671.382>\n",
            "<LTLine 531.777,659.427,531.777,671.382>\n",
            "<LTLine 559.792,659.427,559.792,671.382>\n",
            "<LTLine 50.112,647.472,50.112,659.427>\n",
            "<LTLine 158.304,647.472,158.304,659.427>\n",
            "<LTLine 201.452,647.472,201.452,659.427>\n",
            "<LTLine 251.613,647.472,251.613,659.427>\n",
            "<LTLine 275.194,647.472,275.194,659.427>\n",
            "<LTLine 303.208,647.472,303.208,659.427>\n",
            "<LTLine 305.201,647.472,305.201,659.427>\n",
            "<LTLine 412.307,647.472,412.307,659.427>\n",
            "<LTLine 455.455,647.472,455.455,659.427>\n",
            "<LTLine 505.616,647.472,505.616,659.427>\n",
            "<LTLine 531.777,647.472,531.777,659.427>\n",
            "<LTLine 559.792,647.472,559.792,659.427>\n",
            "<LTLine 50.112,635.517,50.112,647.472>\n",
            "<LTLine 158.304,635.517,158.304,647.472>\n",
            "<LTLine 201.452,635.517,201.452,647.472>\n",
            "<LTLine 251.613,635.517,251.613,647.472>\n",
            "<LTLine 275.194,635.517,275.194,647.472>\n",
            "<LTLine 303.208,635.517,303.208,647.472>\n",
            "<LTLine 305.201,635.517,305.201,647.472>\n",
            "<LTLine 412.307,635.517,412.307,647.472>\n",
            "<LTLine 455.455,635.517,455.455,647.472>\n",
            "<LTLine 505.616,635.517,505.616,647.472>\n",
            "<LTLine 531.777,635.517,531.777,647.472>\n",
            "<LTLine 559.792,635.517,559.792,647.472>\n",
            "<LTLine 50.112,623.562,50.112,635.517>\n",
            "<LTLine 158.304,623.562,158.304,635.517>\n",
            "<LTLine 201.452,623.562,201.452,635.517>\n",
            "<LTLine 251.613,623.562,251.613,635.517>\n",
            "<LTLine 275.194,623.562,275.194,635.517>\n",
            "<LTLine 303.208,623.562,303.208,635.517>\n",
            "<LTLine 305.201,623.562,305.201,635.517>\n",
            "<LTLine 412.307,623.562,412.307,635.517>\n",
            "<LTLine 455.455,623.562,455.455,635.517>\n",
            "<LTLine 505.616,623.562,505.616,635.517>\n",
            "<LTLine 531.777,623.562,531.777,635.517>\n",
            "<LTLine 559.792,623.562,559.792,635.517>\n",
            "<LTLine 50.112,611.606,50.112,623.561>\n",
            "<LTLine 158.304,611.606,158.304,623.561>\n",
            "<LTLine 201.452,611.606,201.452,623.561>\n",
            "<LTLine 251.613,611.606,251.613,623.561>\n",
            "<LTLine 275.194,611.606,275.194,623.561>\n",
            "<LTLine 303.208,611.606,303.208,623.561>\n",
            "<LTLine 305.201,611.606,305.201,623.561>\n",
            "<LTLine 412.307,611.606,412.307,623.561>\n",
            "<LTLine 455.455,611.606,455.455,623.561>\n",
            "<LTLine 505.616,611.606,505.616,623.561>\n",
            "<LTLine 531.777,611.606,531.777,623.561>\n",
            "<LTLine 559.792,611.606,559.792,623.561>\n",
            "<LTLine 50.112,599.651,50.112,611.606>\n",
            "<LTLine 158.304,599.651,158.304,611.606>\n",
            "<LTLine 201.452,599.651,201.452,611.606>\n",
            "<LTLine 251.613,599.651,251.613,611.606>\n",
            "<LTLine 275.194,599.651,275.194,611.606>\n",
            "<LTLine 303.208,599.651,303.208,611.606>\n",
            "<LTLine 305.201,599.651,305.201,611.606>\n",
            "<LTLine 412.307,599.651,412.307,611.606>\n",
            "<LTLine 455.455,599.651,455.455,611.606>\n",
            "<LTLine 505.616,599.651,505.616,611.606>\n",
            "<LTLine 531.777,599.651,531.777,611.606>\n",
            "<LTLine 559.792,599.651,559.792,611.606>\n",
            "<LTLine 50.112,587.696,50.112,599.651>\n",
            "<LTLine 158.304,587.696,158.304,599.651>\n",
            "<LTLine 201.452,587.696,201.452,599.651>\n",
            "<LTLine 251.613,587.696,251.613,599.651>\n",
            "<LTLine 275.194,587.696,275.194,599.651>\n",
            "<LTLine 303.208,587.696,303.208,599.651>\n",
            "<LTLine 305.201,587.696,305.201,599.651>\n",
            "<LTLine 412.307,587.696,412.307,599.651>\n",
            "<LTLine 455.455,587.696,455.455,599.651>\n",
            "<LTLine 505.616,587.696,505.616,599.651>\n",
            "<LTLine 531.777,587.696,531.777,599.651>\n",
            "<LTLine 559.792,587.696,559.792,599.651>\n",
            "<LTLine 50.112,575.741,50.112,587.696>\n",
            "<LTLine 158.304,575.741,158.304,587.696>\n",
            "<LTLine 201.452,575.741,201.452,587.696>\n",
            "<LTLine 251.613,575.741,251.613,587.696>\n",
            "<LTLine 275.194,575.741,275.194,587.696>\n",
            "<LTLine 303.208,575.741,303.208,587.696>\n",
            "<LTLine 305.201,575.741,305.201,587.696>\n",
            "<LTLine 412.307,575.741,412.307,587.696>\n",
            "<LTLine 455.455,575.741,455.455,587.696>\n",
            "<LTLine 505.616,575.741,505.616,587.696>\n",
            "<LTLine 531.777,575.741,531.777,587.696>\n",
            "<LTLine 559.792,575.741,559.792,587.696>\n",
            "<LTLine 50.112,563.786,50.112,575.741>\n",
            "<LTLine 158.304,563.786,158.304,575.741>\n",
            "<LTLine 201.452,563.786,201.452,575.741>\n",
            "<LTLine 251.613,563.786,251.613,575.741>\n",
            "<LTLine 275.194,563.786,275.194,575.741>\n",
            "<LTLine 303.208,563.786,303.208,575.741>\n",
            "<LTLine 305.201,563.786,305.201,575.741>\n",
            "<LTLine 412.307,563.786,412.307,575.741>\n",
            "<LTLine 455.455,563.786,455.455,575.741>\n",
            "<LTLine 505.616,563.786,505.616,575.741>\n",
            "<LTLine 531.777,563.786,531.777,575.741>\n",
            "<LTLine 559.792,563.786,559.792,575.741>\n",
            "<LTLine 50.112,563.587,305.201,563.587>\n",
            "<LTLine 50.112,551.831,50.112,563.786>\n",
            "<LTLine 158.304,551.831,158.304,563.786>\n",
            "<LTLine 201.452,551.831,201.452,563.786>\n",
            "<LTLine 251.613,551.831,251.613,563.786>\n",
            "<LTLine 275.194,551.831,275.194,563.786>\n",
            "<LTLine 303.208,551.831,303.208,563.786>\n",
            "<LTLine 305.201,551.831,305.201,563.786>\n",
            "<LTLine 412.307,551.831,412.307,563.786>\n",
            "<LTLine 455.455,551.831,455.455,563.786>\n",
            "<LTLine 505.616,551.831,505.616,563.786>\n",
            "<LTLine 531.777,551.831,531.777,563.786>\n",
            "<LTLine 559.792,551.831,559.792,563.786>\n",
            "<LTLine 50.112,539.875,50.112,551.830>\n",
            "<LTLine 158.304,539.875,158.304,551.830>\n",
            "<LTLine 201.452,539.875,201.452,551.830>\n",
            "<LTLine 251.613,539.875,251.613,551.830>\n",
            "<LTLine 275.194,539.875,275.194,551.830>\n",
            "<LTLine 303.208,539.875,303.208,551.830>\n",
            "<LTLine 305.201,539.875,305.201,551.830>\n",
            "<LTLine 412.307,539.875,412.307,551.830>\n",
            "<LTLine 455.455,539.875,455.455,551.830>\n",
            "<LTLine 505.616,539.875,505.616,551.830>\n",
            "<LTLine 531.777,539.875,531.777,551.830>\n",
            "<LTLine 559.792,539.875,559.792,551.830>\n",
            "<LTLine 50.112,527.920,50.112,539.875>\n",
            "<LTLine 158.304,527.920,158.304,539.875>\n",
            "<LTLine 201.452,527.920,201.452,539.875>\n",
            "<LTLine 251.613,527.920,251.613,539.875>\n",
            "<LTLine 275.194,527.920,275.194,539.875>\n",
            "<LTLine 303.208,527.920,303.208,539.875>\n",
            "<LTLine 305.201,527.920,305.201,539.875>\n",
            "<LTLine 412.307,527.920,412.307,539.875>\n",
            "<LTLine 455.455,527.920,455.455,539.875>\n",
            "<LTLine 505.616,527.920,505.616,539.875>\n",
            "<LTLine 531.777,527.920,531.777,539.875>\n",
            "<LTLine 559.792,527.920,559.792,539.875>\n",
            "<LTLine 50.112,515.965,50.112,527.920>\n",
            "<LTLine 158.304,515.965,158.304,527.920>\n",
            "<LTLine 201.452,515.965,201.452,527.920>\n",
            "<LTLine 251.613,515.965,251.613,527.920>\n",
            "<LTLine 275.194,515.965,275.194,527.920>\n",
            "<LTLine 303.208,515.965,303.208,527.920>\n",
            "<LTLine 305.201,515.965,305.201,527.920>\n",
            "<LTLine 412.307,515.965,412.307,527.920>\n",
            "<LTLine 455.455,515.965,455.455,527.920>\n",
            "<LTLine 505.616,515.965,505.616,527.920>\n",
            "<LTLine 531.777,515.965,531.777,527.920>\n",
            "<LTLine 559.792,515.965,559.792,527.920>\n",
            "<LTLine 50.112,504.010,50.112,515.965>\n",
            "<LTLine 158.304,504.010,158.304,515.965>\n",
            "<LTLine 201.452,504.010,201.452,515.965>\n",
            "<LTLine 251.613,504.010,251.613,515.965>\n",
            "<LTLine 275.194,504.010,275.194,515.965>\n",
            "<LTLine 303.208,504.010,303.208,515.965>\n",
            "<LTLine 305.201,504.010,305.201,515.965>\n",
            "<LTLine 412.307,504.010,412.307,515.965>\n",
            "<LTLine 455.455,504.010,455.455,515.965>\n",
            "<LTLine 505.616,504.010,505.616,515.965>\n",
            "<LTLine 531.777,504.010,531.777,515.965>\n",
            "<LTLine 559.792,504.010,559.792,515.965>\n",
            "<LTLine 50.112,492.055,50.112,504.010>\n",
            "<LTLine 158.304,492.055,158.304,504.010>\n",
            "<LTLine 201.452,492.055,201.452,504.010>\n",
            "<LTLine 251.613,492.055,251.613,504.010>\n",
            "<LTLine 275.194,492.055,275.194,504.010>\n",
            "<LTLine 303.208,492.055,303.208,504.010>\n",
            "<LTLine 305.201,492.055,305.201,504.010>\n",
            "<LTLine 412.307,492.055,412.307,504.010>\n",
            "<LTLine 455.455,492.055,455.455,504.010>\n",
            "<LTLine 505.616,492.055,505.616,504.010>\n",
            "<LTLine 531.777,492.055,531.777,504.010>\n",
            "<LTLine 559.792,492.055,559.792,504.010>\n",
            "<LTLine 305.201,491.856,559.792,491.856>\n",
            "<LTLine 50.112,480.100,50.112,492.055>\n",
            "<LTLine 158.304,480.100,158.304,492.055>\n",
            "<LTLine 201.452,480.100,201.452,492.055>\n",
            "<LTLine 251.613,480.100,251.613,492.055>\n",
            "<LTLine 275.194,480.100,275.194,492.055>\n",
            "<LTLine 303.208,480.100,303.208,492.055>\n",
            "<LTLine 305.201,480.100,305.201,492.055>\n",
            "<LTLine 305.201,480.100,305.201,492.055>\n",
            "<LTLine 412.307,480.100,412.307,492.055>\n",
            "<LTLine 455.455,480.100,455.455,492.055>\n",
            "<LTLine 505.616,480.100,505.616,492.055>\n",
            "<LTLine 531.777,480.100,531.777,492.055>\n",
            "<LTLine 559.792,480.100,559.792,492.055>\n",
            "<LTLine 50.112,479.900,559.792,479.900>\n",
            "<LTFigure(Im7) 50.112,412.547,93.568,445.139 matrix=[0.05,0.00,0.00,0.05, (50.11,412.55)]>\n",
            "<LTFigure(Im8) 93.569,412.547,142.219,445.143 matrix=[0.10,0.00,0.00,0.10, (93.57,412.55)]>\n",
            "<LTRect 120.429,410.271,144.713,418.229>\n",
            "<LTFigure(Im9) 145.693,412.547,189.158,445.146 matrix=[0.09,0.00,0.00,0.09, (145.69,412.55)]>\n",
            "<LTFigure(Im10) 189.158,412.547,232.623,445.146 matrix=[0.09,0.00,0.00,0.09, (189.16,412.55)]>\n",
            "<LTRect 218.408,410.271,235.112,418.229>\n",
            "<LTFigure(Im11) 236.094,412.547,285.039,445.144 matrix=[0.10,0.00,0.00,0.10, (236.09,412.55)]>\n",
            "<LTFigure(Im12) 285.036,412.547,314.964,445.142 matrix=[0.15,0.00,0.00,0.15, (285.04,412.55)]>\n",
            "<LTRect 298.208,410.271,317.456,418.229>\n",
            "<LTFigure(Im13) 318.437,412.547,367.382,445.144 matrix=[0.10,0.00,0.00,0.10, (318.44,412.55)]>\n",
            "<LTFigure(Im14) 367.379,412.547,389.087,445.142 matrix=[0.07,0.00,0.00,0.07, (367.38,412.55)]>\n",
            "<LTRect 374.093,410.604,391.577,418.229>\n",
            "<LTFigure(Im15) 392.558,412.547,441.503,445.144 matrix=[0.10,0.00,0.00,0.10, (392.56,412.55)]>\n",
            "<LTFigure(Im16) 441.500,412.547,465.946,445.142 matrix=[0.07,0.00,0.00,0.07, (441.50,412.55)]>\n",
            "<LTRect 434.202,410.604,468.436,418.229>\n",
            "<LTFigure(Im17) 471.907,412.547,520.852,445.144 matrix=[0.10,0.00,0.00,0.10, (471.91,412.55)]>\n",
            "<LTFigure(Im18) 520.849,412.547,542.622,445.142 matrix=[0.07,0.00,0.00,0.07, (520.85,412.55)]>\n",
            "<LTRect 518.689,410.604,545.112,418.229>\n",
            "<LTFigure(Im19) 50.112,364.008,76.188,396.603 matrix=[0.07,0.00,0.00,0.07, (50.11,364.01)]>\n",
            "<LTFigure(Im20) 76.186,364.008,125.276,396.604 matrix=[0.10,0.00,0.00,0.10, (76.19,364.01)]>\n",
            "<LTRect 110.534,362.065,127.764,369.690>\n",
            "<LTFigure(Im21) 128.505,364.008,164.360,396.604 matrix=[0.19,0.00,0.00,0.19, (128.50,364.01)]>\n",
            "<LTFigure(Im22) 164.360,364.008,214.130,396.607 matrix=[0.08,0.00,0.00,0.08, (164.36,364.01)]>\n",
            "<LTRect 196.365,361.732,216.619,369.690>\n",
            "<LTFigure(Im23) 217.359,364.008,266.379,396.606 matrix=[0.25,0.00,0.00,0.25, (217.36,364.01)]>\n",
            "<LTFigure(Im24) 266.379,364.008,315.029,396.604 matrix=[0.10,0.00,0.00,0.10, (266.38,364.01)]>\n",
            "<LTRect 299.281,361.732,317.523,369.690>\n",
            "<LTFigure(Im25) 320.754,364.008,369.415,396.606 matrix=[0.47,0.00,0.00,0.47, (320.75,364.01)]>\n",
            "<LTFigure(Im26) 369.415,364.008,418.360,396.605 matrix=[0.10,0.00,0.00,0.10, (369.41,364.01)]>\n",
            "<LTRect 392.911,362.065,420.848,369.690>\n",
            "<LTFigure(Im27) 421.589,364.008,465.054,396.607 matrix=[0.09,0.00,0.00,0.09, (421.59,364.01)]>\n",
            "<LTFigure(Im28) 465.053,364.008,490.588,396.606 matrix=[0.27,0.00,0.00,0.27, (465.05,364.01)]>\n",
            "<LTRect 474.085,362.065,493.079,369.690>\n",
            "<LTFigure(Im29) 496.310,364.008,520.756,396.603 matrix=[0.07,0.00,0.00,0.07, (496.31,364.01)]>\n",
            "<LTFigure(Im30) 520.755,364.008,542.622,396.605 matrix=[0.14,0.00,0.00,0.14, (520.75,364.01)]>\n",
            "<LTRect 527.396,361.732,545.112,369.690>\n",
            "<LTFigure(Im31) 50.112,315.469,99.057,348.066 matrix=[0.10,0.00,0.00,0.10, (50.11,315.47)]>\n",
            "<LTFigure(Im32) 99.054,315.469,142.554,348.065 matrix=[0.12,0.00,0.00,0.12, (99.05,315.47)]>\n",
            "<LTRect 120.742,313.193,145.044,321.151>\n",
            "<LTFigure(Im33) 147.658,315.469,191.123,348.068 matrix=[0.09,0.00,0.00,0.09, (147.66,315.47)]>\n",
            "<LTFigure(Im34) 191.123,315.469,240.068,348.066 matrix=[0.10,0.00,0.00,0.10, (191.12,315.47)]>\n",
            "<LTRect 227.660,313.527,242.556,321.088>\n",
            "<LTFigure(Im35) 247.661,315.469,291.126,348.068 matrix=[0.09,0.00,0.00,0.09, (247.66,315.47)]>\n",
            "<LTFigure(Im36) 291.125,315.469,313.942,348.064 matrix=[0.07,0.00,0.00,0.07, (291.12,315.47)]>\n",
            "<LTRect 294.532,313.527,316.431,321.151>\n",
            "<LTFigure(Im37) 319.045,315.469,367.339,348.067 matrix=[0.24,0.00,0.00,0.24, (319.04,315.47)]>\n",
            "<LTFigure(Im38) 367.339,315.469,416.237,348.068 matrix=[0.33,0.00,0.00,0.33, (367.34,315.47)]>\n",
            "<LTRect 397.427,313.527,418.728,321.088>\n",
            "<LTFigure(Im39) 421.342,315.469,470.287,348.066 matrix=[0.10,0.00,0.00,0.10, (421.34,315.47)]>\n",
            "<LTFigure(Im40) 470.285,315.469,492.017,348.067 matrix=[0.27,0.00,0.00,0.27, (470.28,315.47)]>\n",
            "<LTRect 465.315,313.527,494.508,321.151>\n",
            "<LTFigure(Im41) 497.122,315.469,520.851,348.064 matrix=[0.07,0.00,0.00,0.07, (497.12,315.47)]>\n",
            "<LTFigure(Im42) 520.849,315.469,542.622,348.064 matrix=[0.07,0.00,0.00,0.07, (520.85,315.47)]>\n",
            "<LTRect 515.666,313.527,545.113,321.151>\n",
            "<LTFigure(Im43) 50.112,266.931,99.010,299.530 matrix=[0.33,0.00,0.00,0.33, (50.11,266.93)]>\n",
            "<LTFigure(Im44) 99.010,266.931,142.475,299.530 matrix=[0.09,0.00,0.00,0.09, (99.01,266.93)]>\n",
            "<LTRect 121.439,264.988,144.966,272.613>\n",
            "<LTFigure(Im45) 151.520,266.931,191.034,299.530 matrix=[0.33,0.00,0.00,0.33, (151.52,266.93)]>\n",
            "<LTFigure(Im46) 191.034,266.931,231.774,299.523 matrix=[0.08,0.00,0.00,0.08, (191.03,266.93)]>\n",
            "<LTRect 202.815,264.654,234.265,272.612>\n",
            "<LTFigure(Im47) 243.310,266.931,286.775,299.530 matrix=[0.09,0.00,0.00,0.09, (243.31,266.93)]>\n",
            "<LTFigure(Im48) 286.775,266.931,314.755,299.529 matrix=[0.27,0.00,0.00,0.27, (286.77,266.93)]>\n",
            "<LTRect 276.476,264.988,317.246,272.613>\n",
            "<LTFigure(Im49) 323.800,266.931,375.819,299.530 matrix=[0.69,0.00,0.00,0.69, (323.80,266.93)]>\n",
            "<LTFigure(Im50) 375.819,266.931,417.433,299.528 matrix=[0.17,0.00,0.00,0.17, (375.82,266.93)]>\n",
            "<LTRect 403.449,264.654,419.923,272.612>\n",
            "<LTFigure(Im51) 426.477,266.931,468.092,299.529 matrix=[0.35,0.00,0.00,0.35, (426.48,266.93)]>\n",
            "<LTFigure(Im52) 468.091,266.931,487.648,299.526 matrix=[0.07,0.00,0.00,0.07, (468.09,266.93)]>\n",
            "<LTRect 453.805,264.988,490.137,272.613>\n",
            "<LTFigure(Im53) 499.182,266.931,520.890,299.526 matrix=[0.07,0.00,0.00,0.07, (499.18,266.93)]>\n",
            "<LTFigure(Im54) 520.889,266.931,542.621,299.529 matrix=[0.27,0.00,0.00,0.27, (520.89,266.93)]>\n",
            "<LTRect 518.340,264.654,545.112,272.612>\n",
            "<LTFigure(Im55) 50.112,218.392,96.744,250.989 matrix=[0.45,0.00,0.00,0.45, (50.11,218.39)]>\n",
            "<LTFigure(Im56) 96.744,218.392,140.209,250.991 matrix=[0.09,0.00,0.00,0.09, (96.74,218.39)]>\n",
            "<LTRect 107.269,216.449,142.699,224.074>\n",
            "<LTFigure(Im57) 145.910,218.392,189.375,250.991 matrix=[0.09,0.00,0.00,0.09, (145.91,218.39)]>\n",
            "<LTFigure(Im58) 189.374,218.392,232.839,250.991 matrix=[0.09,0.00,0.00,0.09, (189.37,218.39)]>\n",
            "<LTRect 209.790,216.449,235.329,224.074>\n",
            "<LTFigure(Im59) 238.539,218.392,273.819,250.991 matrix=[0.07,0.00,0.00,0.07, (238.54,218.39)]>\n",
            "<LTFigure(Im60) 273.817,218.392,322.716,250.991 matrix=[0.33,0.00,0.00,0.33, (273.82,218.39)]>\n",
            "<LTRect 300.510,216.116,325.206,224.074>\n",
            "<LTFigure(Im61) 328.416,218.392,369.162,250.989 matrix=[0.34,0.00,0.00,0.34, (328.42,218.39)]>\n",
            "<LTFigure(Im62) 369.163,218.392,393.609,250.987 matrix=[0.07,0.00,0.00,0.07, (369.16,218.39)]>\n",
            "<LTRect 355.491,216.449,396.098,224.074>\n",
            "<LTFigure(Im63) 399.308,218.392,448.207,250.991 matrix=[0.33,0.00,0.00,0.33, (399.31,218.39)]>\n",
            "<LTFigure(Im64) 448.206,218.392,491.671,250.991 matrix=[0.09,0.00,0.00,0.09, (448.21,218.39)]>\n",
            "<LTRect 460.767,216.449,494.162,224.074>\n",
            "<LTFigure(Im65) 499.143,218.392,520.851,250.987 matrix=[0.07,0.00,0.00,0.07, (499.14,218.39)]>\n",
            "<LTFigure(Im66) 520.849,218.392,542.622,250.987 matrix=[0.07,0.00,0.00,0.07, (520.85,218.39)]>\n",
            "<LTRect 515.167,216.116,545.112,224.074>\n",
            "<LTFigure(Im67) 50.112,169.853,93.577,202.452 matrix=[0.09,0.00,0.00,0.09, (50.11,169.85)]>\n",
            "<LTFigure(Im68) 93.576,169.853,115.284,202.448 matrix=[0.07,0.00,0.00,0.07, (93.58,169.85)]>\n",
            "<LTRect 97.062,167.577,117.774,175.535>\n",
            "<LTFigure(Im69) 127.091,169.853,176.036,202.450 matrix=[0.10,0.00,0.00,0.10, (127.09,169.85)]>\n",
            "<LTFigure(Im70) 176.034,169.853,197.807,202.448 matrix=[0.07,0.00,0.00,0.07, (176.03,169.85)]>\n",
            "<LTRect 168.402,167.910,200.296,175.535>\n",
            "<LTFigure(Im71) 209.614,169.853,267.409,202.449 matrix=[0.12,0.00,0.00,0.12, (209.61,169.85)]>\n",
            "<LTFigure(Im72) 267.406,169.853,293.482,202.448 matrix=[0.07,0.00,0.00,0.07, (267.41,169.85)]>\n",
            "<LTRect 274.584,167.910,295.971,175.535>\n",
            "<LTFigure(Im73) 305.288,169.853,354.187,202.452 matrix=[0.41,0.00,0.00,0.41, (305.29,169.85)]>\n",
            "<LTFigure(Im74) 354.186,169.853,403.131,202.450 matrix=[0.10,0.00,0.00,0.10, (354.19,169.85)]>\n",
            "<LTRect 394.681,167.577,405.620,175.472>\n",
            "<LTFigure(Im75) 417.428,169.853,460.893,202.452 matrix=[0.09,0.00,0.00,0.09, (417.43,169.85)]>\n",
            "<LTFigure(Im76) 460.892,169.853,482.082,202.453 matrix=[0.16,0.00,0.00,0.16, (460.89,169.85)]>\n",
            "<LTRect 455.633,167.910,484.572,175.535>\n",
            "<LTFigure(Im77) 496.380,169.853,518.112,202.451 matrix=[0.22,0.00,0.00,0.22, (496.38,169.85)]>\n",
            "<LTFigure(Im78) 518.112,169.853,542.623,202.448 matrix=[0.07,0.00,0.00,0.07, (518.11,169.85)]>\n",
            "<LTRect 515.140,167.577,545.113,175.535>\n",
            "<LTTextBoxHorizontal(0) 308.862,540.513,545.115,717.848 'seen by examining the performance of this detector on the\\ncats and dogs in the recent PASCAL VOC 2011 challenge\\ndata [20]. The deformable parts detector [23] obtains an\\nAverage Precision (AP) of only 31.7% and 22.1% on cats\\nand dogs respectively [20]; by comparison, an easier cat-\\negory such as bicycle has AP of 54% [20]. However, in\\nthe PASCAL VOC challenge the task is to detect the whole\\nbody of the animal. As in the method of [34], we use the\\ndeformable part model to detect certain stable and distinc-\\ntive components of the body. In particular, the head annota-\\ntions included in the Oxford-IIIT Pet data are used to learn\\na deformable part model of the cat faces, and one of the\\ndog faces ([24, 29, 45] also focus on modelling the faces of\\npets). Sect. 4.1 shows that these shape models are in fact\\nvery good.\\n'>\n",
            "<LTTextBoxHorizontal(1) 308.862,336.333,545.116,513.669 'To represent texture, we use a bag-of-words [16] model.\\nVisual words [38] are computed densely on the image by ex-\\ntracting SIFT descriptors [31] with a stride of 6 pixels and\\nat 4 scales, deﬁned by setting the width of the SIFT spatial\\nbins to 4, 6, 8, and 10 pixels respectively. The SIFT features\\nhave constant orientation (i.e, they are not adapted to the lo-\\ncal image appearance). The SIFT descriptors are then quan-\\ntized based on a vocabulary of 4,000 visual words. The vo-\\ncabulary is learned by using k-means on features randomly\\nsampled from the training data. In order to obtain a descrip-\\ntor for the image, the quantized SIFT features are pooled\\ninto a spatial histogram [30], which has dimension equal to\\n4,000 times the number of spatial bins. Histograms are then\\nl1 normalized and used in a support vector machine (SVM)\\nbased on the exponential-χ2 kernel [44] for classiﬁcation.\\n'>\n",
            "<LTTextBoxHorizontal(2) 308.862,288.512,545.115,334.341 'Different variants of the spatial histograms can be ob-\\ntained by placing the spatial bins in correspondence of par-\\nticular geometric features of the pet. These layouts are de-\\nscribed next and in Fig. 4:\\n'>\n",
            "<LTTextBoxHorizontal(3) 308.862,226.594,545.112,272.492 'Image layout. This layout consists of ﬁve spatial bins or-\\nganized as a 1 × 1 and a 2 × 2 grids (Fig. 4a) covering the\\nentire image area, as in [30]. This results in a 20,000 di-\\nmensional feature vector.\\n'>\n",
            "<LTTextBoxHorizontal(4) 308.862,116.855,545.117,210.734 'Image+head layout. This layout adds to the image layout\\njust described a spatial bin in correspondence of the head\\nbounding box (as detected by the deformable part model\\nof the pet face) as well as one for the complement of this\\nbox. These two regions do not contain further spatial subdi-\\nvisions (Fig. 4b). Concatenating the histograms for all the\\nspatial bins in this layout results in a 28,000 dimensional\\nfeature vector.\\n'>\n",
            "<LTTextBoxHorizontal(5) 50.112,516.412,273.088,525.441 'Figure 3. Example images from the MSR ASIRRA dataset.\\n'>\n",
            "<LTTextBoxHorizontal(6) 308.862,521.500,415.733,532.458 '3.2. Appearance model\\n'>\n",
            "<LTTextBoxHorizontal(7) 50.112,400.011,286.365,493.660 'of at least one of them, while humans would make no mis-\\ntakes. The ASIRRA test is currently used to protect a num-\\nber of web sites from the unwanted access by Internet bots.\\nHowever, the reliability of this test depends on the clas-\\nsiﬁcation accuracy α of the classiﬁer implemented by the\\nbot. For instance, if the classiﬁer has accuracy α = 95%,\\nthen the bot fools the ASIRRA test roughly half of the times\\n(α12 ≈ 54%).\\n'>\n",
            "<LTTextBoxHorizontal(8) 50.112,339.900,286.367,397.683 'The complete MSR ASIRRA system is based on a\\ndatabase of several millions images of pets, equally divided\\nbetween cats and dogs. Our classiﬁers are tested on the\\n24,990 images that have been made available to the public\\nfor research and evaluation purposes.\\n'>\n",
            "<LTTextBoxHorizontal(9) 50.112,304.673,232.238,316.629 '3. A model for breed discrimination\\n'>\n",
            "<LTTextBoxHorizontal(10) 50.112,225.980,286.365,295.719 'The breed of a pet affects its size, shape, fur type and\\ncolor. Since it is not possible to measure the pet size from\\nan image without an absolute reference, our model focuses\\non capturing the pet shape (Sect. 3.1) and the appearance of\\nits fur (Sect. 3.2). The model also involves automatically\\nsegmenting the pet from the image background (Sect. 3.3).\\n'>\n",
            "<LTTextBoxHorizontal(11) 50.112,204.910,129.268,215.868 '3.1. Shape model\\n'>\n",
            "<LTTextBoxHorizontal(12) 50.112,103.094,286.365,196.743 'To represent shape, we use the deformable part model\\nof [23]. In this model, an object is given by a root part con-\\nnected with springs to eight smaller parts at a ﬁner scale.\\nThe appearance of each part is represented by a HOG ﬁl-\\nter [17], capturing the local distribution of the image edges;\\ninference (detection) uses dynamic programming to ﬁnd the\\nbest trade-off between matching well each part to the image\\nand not deforming the springs too much.\\n'>\n",
            "<LTTextBoxHorizontal(13) 50.112,78.848,286.365,100.766 'While powerful, this model is insufﬁcient to represent\\nthe ﬂexibility and variability of a pet body. This can be\\n'>\n",
            "<LTTextBoxHorizontal(14) 308.862,78.848,545.111,100.835 'Image+head+body layout. This layout combines the\\nspatial tiles in the image layout with an additional spatial bin\\n'>\n",
            "<LTFigure(Im79) 50.112,678.898,106.562,720.000 matrix=[0.37,0.00,0.00,0.37, (50.11,678.90)]>\n",
            "<LTFigure(Im80) 114.462,678.898,158.343,720.000 matrix=[0.15,0.00,0.00,0.15, (114.46,678.90)]>\n",
            "<LTFigure(Im81) 166.244,678.898,215.759,719.995 matrix=[0.10,0.00,0.00,0.10, (166.24,678.90)]>\n",
            "<LTFigure(Im82) 223.659,678.898,278.459,719.998 matrix=[0.11,0.00,0.00,0.11, (223.66,678.90)]>\n",
            "<LTFigure(Im83) 50.112,633.964,82.413,675.065 matrix=[0.10,0.00,0.00,0.10, (50.11,633.96)]>\n",
            "<LTFigure(Im84) 92.650,633.964,142.887,675.067 matrix=[0.38,0.00,0.00,0.38, (92.65,633.96)]>\n",
            "<LTFigure(Im85) 153.123,633.964,207.174,675.065 matrix=[0.56,0.00,0.00,0.56, (153.12,633.96)]>\n",
            "<LTFigure(Im86) 217.410,633.964,276.125,675.065 matrix=[0.12,0.00,0.00,0.12, (217.41,633.96)]>\n",
            "<LTFigure(Im87) 50.112,589.031,88.942,630.132 matrix=[0.21,0.00,0.00,0.21, (50.11,589.03)]>\n",
            "<LTFigure(Im88) 100.697,589.031,155.498,630.131 matrix=[0.24,0.00,0.00,0.24, (100.70,589.03)]>\n",
            "<LTFigure(Im89) 167.254,589.031,208.051,630.132 matrix=[0.10,0.00,0.00,0.10, (167.25,589.03)]>\n",
            "<LTFigure(Im90) 219.805,589.031,274.605,630.131 matrix=[0.11,0.00,0.00,0.11, (219.80,589.03)]>\n",
            "<LTFigure(Im91) 50.112,544.098,97.036,585.199 matrix=[0.16,0.00,0.00,0.16, (50.11,544.10)]>\n",
            "<LTFigure(Im92) 110.518,544.098,152.623,585.192 matrix=[0.08,0.00,0.00,0.08, (110.52,544.10)]>\n",
            "<LTFigure(Im93) 166.107,544.098,204.741,585.198 matrix=[0.08,0.00,0.00,0.08, (166.11,544.10)]>\n",
            "<LTFigure(Im94) 218.223,544.098,272.878,585.199 matrix=[0.11,0.00,0.00,0.11, (218.22,544.10)]>\n",
            "<LTTextBoxHorizontal(0) 68.451,672.307,100.499,680.278 '(a) Image\\n'>\n",
            "<LTTextBoxHorizontal(1) 176.141,672.307,229.947,680.278 '(b) Image+Head\\n'>\n",
            "<LTTextBoxHorizontal(2) 130.672,613.652,205.798,621.623 '(c) Image+Head+Body\\n'>\n",
            "<LTTextBoxHorizontal(3) 50.112,545.758,286.364,598.622 'Figure 4. Spatial histogram layouts. The three different spatial\\nlayouts used for computing the image descriptors. The image de-\\nscriptor in each case is formed by concatenating the histograms\\ncomputed on the individual spatial components of the layout. The\\nspatial bins are denoted by yellow-black lines.\\n'>\n",
            "<LTTextBoxHorizontal(4) 50.112,381.378,286.365,523.077 'in correspondence of the pet head (as for the image+head\\nlayout) as well as other spatial bins computed on the fore-\\nground object region and its complement, as described next\\nand in Fig. 4c. The foreground region is obtained either\\nfrom the automatic segmentation of the pet body or from\\nthe ground-truth segmentation to obtain a best-case base-\\nline. The foreground region is subdivided into ﬁve spatial\\nbins, similar to the image layout. An additional bin obtained\\nfrom the foreground region with the head region removed\\nand no further spatial subdivisions is also used. Concate-\\nnating the histograms for all the spatial bins in this layout\\nresults in a 48,000 dimensional feature vector.\\n'>\n",
            "<LTTextBoxHorizontal(5) 50.112,360.071,182.879,371.029 '3.3. Automatic segmentation\\n'>\n",
            "<LTTextBoxHorizontal(6) 50.112,78.848,286.365,351.825 'The foreground (pet) and background regions needed for\\ncomputing the appearance descriptors are obtained auto-\\nmatically using the grab-cut segmentation technique [36].\\nInitialization of grab-cut segmentations was done using\\ncues from the over-segmentation of an image (i.e, super-\\npixels) similar to the method of [15].\\nIn this method, a\\nSVM classiﬁer is used to assign superpixels a conﬁdence\\nscore. This conﬁdence score is then used to assign super-\\npixels to a foreground or background region to initialize\\nthe grab-cut iteration. We used Berkeley’s ultrametric color\\nmap (UCM) [13] for obtaining the superpixels. Each super-\\npixel was described by a feature vector comprising the color\\nhistogram and Sift-BoW histogram computed on it. Super-\\npixels were assigned a score using a linear-SVM [21] which\\nwas trained on the features computed on the training data.\\nAfter this initialization, grab-cut was used as in [34]. The\\nimproved initialization achieves segmentation accuracy of\\n65% this improving over our previous method [34] by 4%\\nand is about 20% better than simply choosing all pixels as\\nforeground (i.e, assuming the pet foreground entirely occu-\\npies the image). (Tab. 2). Example segmentations produced\\nby our method on the Oxford-IIIT Pet data are shown in\\nFig. 5.\\n'>\n",
            "<LTTextBoxHorizontal(7) 322.166,672.020,389.533,719.113 'Method\\nAll foreground\\nParkhi et al. [34]\\nThis paper\\n'>\n",
            "<LTTextBoxHorizontal(8) 401.488,672.020,529.318,719.113 'Mean Segmentation Accuracy\\n45%\\n61%\\n65%\\n'>\n",
            "<LTTextBoxHorizontal(9) 308.862,627.819,545.113,658.766 'Table 2. Performance of segmentation schemes. Segmentation\\naccuracy computed as intersection over union of segmentation\\nwith ground truth.\\n'>\n",
            "<LTTextBoxHorizontal(10) 314.840,566.235,410.561,613.328 'Dataset\\nOxford-IIIT Pet Dataset\\nUCSD-Caltech Birds\\nOxford-Flowers102\\n'>\n",
            "<LTTextBoxHorizontal(11) 422.515,566.235,549.249,613.328 'Mean Classiﬁcation Accuracy\\n38.45%\\n6.91%\\n53.71%\\n'>\n",
            "<LTTextBoxHorizontal(12) 308.862,523.611,545.112,554.557 'Table 3. Fine grained classiﬁcation baseline. Mean classiﬁcation\\naccuracies obtained on three different datasets using the VLFeat-\\nBoW classiﬁcation code.\\n'>\n",
            "<LTTextBoxHorizontal(13) 308.862,492.409,385.901,504.365 '4. Experiments\\n'>\n",
            "<LTTextBoxHorizontal(14) 308.862,330.040,545.115,483.465 'The models are evaluated ﬁrst on the task of discrim-\\ninating the family of the pet (Sect. 4.1), then on the one\\nof discriminating their breed given the family (Sect. 4.2),\\nand ﬁnally discriminating both the family and the breed\\n(Sect. 4.3). For the third task, both hierarchical classiﬁca-\\ntion (i.e, determining ﬁrst the family and then the breed)\\nand ﬂat classiﬁcation (i.e, determining the family and the\\nbreed simultaneously) are evaluated. Training uses the\\nOxford-IIIT Pet train and validation data and testing uses\\nthe Oxford-IIIT Pet test data. All these results are summa-\\nrized in Tab. 4 and further results for pet family discrimina-\\ntion on the ASIRRA data are reported in Sect. 4.1. Failure\\ncases are reported in Fig. 7.\\n'>\n",
            "<LTTextBoxHorizontal(15) 308.862,182.179,545.115,311.763 'Baseline.\\nIn order to compare the difﬁculty of the Oxford-\\nIIIT Pet dataset\\nto other Fine Grained Visual Catego-\\nrization datasets, and also to provide a baseline for our\\nbreed classiﬁcation task, we have run the publicly available\\nVLFeat [40] BoW classiﬁcation code over three datasets:\\nOxford Flowers 102 [33], UCSD-Caltech Birds [14], and\\nOxford-IIIT Pet dataset (note that this code is a faster suc-\\ncessor to the VGG-MKL package [41] used on the UCSD-\\nCaltech Birds dataset in [14]). The code employs a spatial\\npyramid [30], but does not use segmentation or salient parts.\\nThe results are given in Table 3.\\n'>\n",
            "<LTTextBoxHorizontal(16) 308.862,161.142,446.539,172.100 '4.1. Pet family discrimination\\n'>\n",
            "<LTTextBoxHorizontal(17) 308.862,119.113,545.115,152.986 'This section evaluates the different models on the task\\nof discriminating the family of a pet (cat Vs dog classiﬁca-\\ntion).\\n'>\n",
            "<LTTextBoxHorizontal(18) 308.862,78.848,545.115,100.835 'Shape only. The maximum response of the cat face detec-\\ntor (Sect. 3.1) on an image is used as an image-level score\\n'>\n",
            "<LTFigure(Im95) 53.404,685.984,105.094,719.996 matrix=[0.10,0.00,0.00,0.10, (53.40,685.98)]>\n",
            "<LTFigure(Im96) 113.253,685.984,115.549,719.998 matrix=[0.14,0.00,0.00,0.14, (113.25,685.98)]>\n",
            "<LTFigure(Im95) 123.021,685.984,174.711,719.996 matrix=[0.10,0.00,0.00,0.10, (123.02,685.98)]>\n",
            "<LTFigure(Im97) 177.201,685.984,228.891,719.996 matrix=[0.10,0.00,0.00,0.10, (177.20,685.98)]>\n",
            "<LTFigure(Im98) 231.381,685.984,283.071,719.996 matrix=[0.10,0.00,0.00,0.10, (231.38,685.98)]>\n",
            "<LTFigure(Im95) 58.288,627.329,109.978,661.341 matrix=[0.10,0.00,0.00,0.10, (58.29,627.33)]>\n",
            "<LTFigure(Im97) 112.468,627.329,164.158,661.341 matrix=[0.10,0.00,0.00,0.10, (112.47,627.33)]>\n",
            "<LTFigure(Im99) 166.648,627.329,218.338,661.341 matrix=[0.10,0.00,0.00,0.10, (166.65,627.33)]>\n",
            "<LTFigure(Im100) 226.497,627.329,278.187,661.341 matrix=[0.10,0.00,0.00,0.10, (226.50,627.33)]>\n",
            "<LTLine 316.188,719.801,535.295,719.801>\n",
            "<LTLine 316.188,707.646,316.188,719.601>\n",
            "<LTLine 395.510,707.646,395.510,719.601>\n",
            "<LTLine 535.295,707.646,535.295,719.601>\n",
            "<LTLine 316.188,707.447,535.295,707.447>\n",
            "<LTLine 316.188,695.293,316.188,707.248>\n",
            "<LTLine 395.510,695.293,395.510,707.248>\n",
            "<LTLine 535.295,695.293,535.295,707.248>\n",
            "<LTLine 316.188,695.093,535.295,695.093>\n",
            "<LTLine 316.188,682.939,316.188,694.894>\n",
            "<LTLine 395.510,682.939,395.510,694.894>\n",
            "<LTLine 535.295,682.939,535.295,694.894>\n",
            "<LTLine 316.188,682.740,535.295,682.740>\n",
            "<LTLine 316.188,670.585,316.188,682.540>\n",
            "<LTLine 395.510,670.585,395.510,682.540>\n",
            "<LTLine 535.295,670.585,535.295,682.540>\n",
            "<LTLine 316.188,670.386,535.295,670.386>\n",
            "<LTLine 308.862,614.016,555.226,614.016>\n",
            "<LTLine 308.862,601.861,308.862,613.816>\n",
            "<LTLine 416.537,601.861,416.537,613.816>\n",
            "<LTLine 555.226,601.861,555.226,613.816>\n",
            "<LTLine 308.862,601.662,555.226,601.662>\n",
            "<LTLine 308.862,589.508,308.862,601.463>\n",
            "<LTLine 416.537,589.508,416.537,601.463>\n",
            "<LTLine 555.226,589.508,555.226,601.463>\n",
            "<LTLine 308.862,589.308,555.226,589.308>\n",
            "<LTLine 308.862,577.154,308.862,589.109>\n",
            "<LTLine 416.537,577.154,416.537,589.109>\n",
            "<LTLine 555.226,577.154,555.226,589.109>\n",
            "<LTLine 308.862,576.955,555.226,576.955>\n",
            "<LTLine 308.862,564.800,308.862,576.755>\n",
            "<LTLine 416.537,564.800,416.537,576.755>\n",
            "<LTLine 555.226,564.800,555.226,576.755>\n",
            "<LTLine 308.862,564.601,555.226,564.601>\n",
            "<LTTextBoxHorizontal(0) 82.049,709.081,84.540,719.044 '.\\n'>\n",
            "<LTTextBoxHorizontal(1) 99.732,709.151,125.754,719.113 'Shape\\n'>\n",
            "<LTTextBoxHorizontal(2) 197.632,709.151,248.850,719.113 'Appearance\\n'>\n",
            "<LTTextBoxHorizontal(3) 357.834,709.151,476.817,719.113 'Classiﬁcation Accuracy (%)\\n'>\n",
            "<LTTextBoxHorizontal(4) 156.935,696.727,201.488,706.690 'layout type\\n'>\n",
            "<LTTextBoxHorizontal(5) 232.669,696.727,306.283,706.690 'using ground truth\\n'>\n",
            "<LTTextBoxHorizontal(6) 446.326,696.727,496.139,706.690 'both (S. 4.3)\\n'>\n",
            "<LTTextBoxHorizontal(7) 80.803,576.379,85.784,681.983 '1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'>\n",
            "<LTTextBoxHorizontal(8) 108.592,578.531,116.894,684.135 '(cid:88)\\n–\\n–\\n–\\n–\\n(cid:88)\\n(cid:88)\\n(cid:88)\\n(cid:88)\\n'>\n",
            "<LTTextBoxHorizontal(9) 137.710,576.379,220.713,681.983 '–\\nImage\\nImage+Head\\nImage+Head+Body\\nImage+Head+Body\\nImage\\nImage+Head\\nImage+Head+Body\\nImage+Head+Body\\n'>\n",
            "<LTTextBoxHorizontal(10) 265.324,578.531,273.626,681.983 '–\\n–\\n–\\n–\\n(cid:88)\\n–\\n–\\n–\\n(cid:88)\\n'>\n",
            "<LTTextBoxHorizontal(11) 320.230,576.379,349.839,706.690 'family\\n(S. 4.1)\\n94.21\\n82.56\\n85.06\\n87.78\\n88.68\\n94.88\\n95.07\\n94.89\\n95.37\\n'>\n",
            "<LTTextBoxHorizontal(12) 361.794,576.379,418.581,706.690 'breed (S. 4.2)\\ndog\\ncat\\nNA\\nNA\\n40.59\\n52.01\\n52.10\\n60.37\\n54.31\\n64.27\\n57.29\\n66.12\\n42.94\\n50.27\\n54.56\\n59.11\\n55.68\\n63.48\\n59.18\\n66.07\\n'>\n",
            "<LTTextBoxHorizontal(13) 430.536,576.379,477.559,694.337 'hierarchical\\nNA\\nNA\\nNA\\nNA\\nNA\\n42.29\\n52.78\\n55.26\\n57.77\\n'>\n",
            "<LTTextBoxHorizontal(14) 489.510,576.379,511.930,694.337 'ﬂat\\nNA\\n39.64\\n51.23\\n54.05\\n56.60\\n43.30\\n54.03\\n56.68\\n59.21\\n'>\n",
            "<LTTextBoxHorizontal(15) 50.112,532.178,545.113,563.125 'Table 4. Comparison between different models. The table compares different models on the three tasks of discriminating the family, the\\nbreed given the family, and the breed and family of the pets in the Oxford-IIIT Pet dataset (Sect. 2). Different combinations of the shape\\nfeatures (deformable part model of the pet faces) and of the various appearance features are tested (Sect. 3.2, Fig. 4).\\n'>\n",
            "<LTTextBoxHorizontal(16) 50.112,452.313,286.365,510.097 'for the cat class. The same is done to obtain a score for\\nthe dog class. Then a linear SVM is learned to discriminate\\nbetween cats and dogs based on these two scores. The clas-\\nsiﬁcation accuracy of this model on the Oxford-IIIT Pet test\\ndata is 94.21%.\\n'>\n",
            "<LTTextBoxHorizontal(17) 50.112,260.743,286.366,426.192 'Appearance only. Spatial histograms of visual words are\\nused in a non-linear SVM to discriminate between cats and\\ndogs, as detailed in Sect. 3.2. The accuracy depends on\\nthe type of spatial histograms considered, which in turn\\ndepends on the layout of the spatial bins. On the Oxford-\\nIIIT Pet test data, the image layout obtains an accuracy of\\n82.56%; adding head information using image+head layout\\nyields an accuracy of 85.06%. Using image+head+body\\nlayout improves accuracy by a further 2.7% to 87.78%. An\\nimprovement of 1% was observed when the ground-truth\\nsegmentations were used in place of the segmentations es-\\ntimated by grab-cut (Sect. 3.2). This progression indicates\\nthat the more accurate the localization of the pet body, the\\nbetter is the classiﬁcation accuracy.\\n'>\n",
            "<LTTextBoxHorizontal(18) 50.112,140.904,286.365,234.622 'Shape and appearance. The appearance and shape infor-\\nmation are combined by summing the exp-χ2 kernel for the\\nappearance part (Sect. 3.2) with a linear kernel on the cat\\nscores and a linear kernel on the dog scores. The combina-\\ntion boosts the performance by an additional 7% over that\\nof using appearance alone, yielding approximately 95.37%\\naccuracy (Table 4, rows 5 and 9), with all the variants of the\\nappearance model performing similarly.\\n'>\n",
            "<LTTextBoxHorizontal(19) 332.588,479.527,418.997,511.271 'Method\\nGolle et al. [25]\\nThis paper (Shape only)\\n'>\n",
            "<LTTextBoxHorizontal(20) 430.952,479.527,516.904,511.271 'Mean Class. Accuracy\\n82.7%\\n92.9%\\n'>\n",
            "<LTTextBoxHorizontal(21) 308.862,399.699,545.109,463.523 'Table 5. Performance on ASIRRA Data. Table shows perfor-\\nmance achieved on task of pet family classiﬁcation posed by the\\nASIRRA challenge. Best results obtained by Golle [25] were ob-\\ntained using 10000 images from the data. 8000 for training and\\n2000 for testing. Our test results are shown on 24990 images in\\nthe ASIRRA dataset.\\n'>\n",
            "<LTTextBoxHorizontal(22) 308.862,297.883,545.115,379.577 '92.9%, which corresponds to a 42% probability of breaking\\nthe test in a single try. For comparison, the best accuracy re-\\nported in the literature on the ASIRRA data is 82.7% [25],\\nwhich corresponds to just a 9.2% chance of breaking the\\ntest. Due to lack of sufﬁcient training data to train appear-\\nance models for ASIRRA data, we did not evaluate these\\nmodels on ASIRRA dataset.\\n'>\n",
            "<LTTextBoxHorizontal(23) 308.862,277.891,426.780,288.849 '4.2. Breed discrimination\\n'>\n",
            "<LTTextBoxHorizontal(24) 308.862,152.500,545.115,270.059 'This section evaluates the models on the task of discrimi-\\nnating the different breeds of cats and dogs given their fam-\\nily. This is done by learning a multi-class SVM by using\\nthe 1-Vs-rest decomposition [37] (this means learning 12\\nbinary classiﬁers for cats and 25 for dogs). The relative per-\\nformance of the different models is similar to that observed\\nfor pet family classiﬁcation in Sect. 4.1. The best breed\\nclassiﬁcation accuracies for cats and dogs are 63.48% and\\n55.68% respectively, which improve to 66.07% and 59.18%\\nwhen the ground truth segmentations are used.\\n'>\n",
            "<LTTextBoxHorizontal(25) 308.862,132.508,481.311,143.466 '4.3. Family and breed discrimination\\n'>\n",
            "<LTTextBoxHorizontal(26) 50.112,78.848,286.365,124.745 'The ASIRRA data. The ASIRRA data does not specify a\\ntraining set, so we used models trained on the Oxford-IIIT\\nPet data and the ASIRRA data was used only for testing.\\nThe accuracy of the shape model on the ASIRRA data is\\n'>\n",
            "<LTTextBoxHorizontal(27) 308.862,78.848,545.115,124.676 'This section investigates classifying both the family and\\nthe breed. Two approaches are explored: hierarchical clas-\\nsiﬁcation, in which the family is decided ﬁrst as in Sect. 4.1,\\nand then the breed is decided as in Sect. 4.2, and ﬂat classi-\\n'>\n",
            "<LTLine 74.826,719.801,517.908,719.801>\n",
            "<LTLine 74.826,707.646,74.826,719.601>\n",
            "<LTLine 91.762,707.646,91.762,719.601>\n",
            "<LTLine 93.755,707.646,93.755,719.601>\n",
            "<LTLine 131.732,707.646,131.732,719.601>\n",
            "<LTLine 312.260,707.646,312.260,719.601>\n",
            "<LTLine 314.252,707.646,314.252,719.601>\n",
            "<LTLine 517.908,707.646,517.908,719.601>\n",
            "<LTLine 74.826,707.447,517.908,707.447>\n",
            "<LTLine 74.826,695.293,74.826,707.248>\n",
            "<LTLine 91.762,695.293,91.762,707.248>\n",
            "<LTLine 93.755,695.293,93.755,707.248>\n",
            "<LTLine 131.732,695.293,131.732,707.248>\n",
            "<LTLine 226.691,695.293,226.691,707.248>\n",
            "<LTLine 312.260,695.293,312.260,707.248>\n",
            "<LTLine 314.252,695.293,314.252,707.248>\n",
            "<LTLine 355.816,695.293,355.816,707.248>\n",
            "<LTLine 424.558,695.293,424.558,707.248>\n",
            "<LTLine 517.908,695.293,517.908,707.248>\n",
            "<LTLine 74.826,695.093,517.908,695.093>\n",
            "<LTLine 74.826,682.939,74.826,694.894>\n",
            "<LTLine 91.762,682.939,91.762,694.894>\n",
            "<LTLine 93.755,682.939,93.755,694.894>\n",
            "<LTLine 131.732,682.939,131.732,694.894>\n",
            "<LTLine 226.691,682.939,226.691,694.894>\n",
            "<LTLine 312.260,682.939,312.260,694.894>\n",
            "<LTLine 314.252,682.939,314.252,694.894>\n",
            "<LTLine 355.816,682.939,355.816,694.894>\n",
            "<LTLine 424.558,682.939,424.558,694.894>\n",
            "<LTLine 517.908,682.939,517.908,694.894>\n",
            "<LTLine 74.826,682.740,517.908,682.740>\n",
            "<LTLine 74.826,670.585,74.826,682.540>\n",
            "<LTLine 91.762,670.585,91.762,682.540>\n",
            "<LTLine 93.755,670.585,93.755,682.540>\n",
            "<LTLine 131.732,670.585,131.732,682.540>\n",
            "<LTLine 226.691,670.585,226.691,682.540>\n",
            "<LTLine 312.260,670.585,312.260,682.540>\n",
            "<LTLine 314.252,670.585,314.252,682.540>\n",
            "<LTLine 355.816,670.585,355.816,682.540>\n",
            "<LTLine 424.558,670.585,424.558,682.540>\n",
            "<LTLine 517.908,670.585,517.908,682.540>\n",
            "<LTLine 74.826,658.630,74.826,670.585>\n",
            "<LTLine 91.762,658.630,91.762,670.585>\n",
            "<LTLine 93.755,658.630,93.755,670.585>\n",
            "<LTLine 131.732,658.630,131.732,670.585>\n",
            "<LTLine 226.691,658.630,226.691,670.585>\n",
            "<LTLine 312.260,658.630,312.260,670.585>\n",
            "<LTLine 314.252,658.630,314.252,670.585>\n",
            "<LTLine 355.816,658.630,355.816,670.585>\n",
            "<LTLine 424.558,658.630,424.558,670.585>\n",
            "<LTLine 517.908,658.630,517.908,670.585>\n",
            "<LTLine 74.826,646.675,74.826,658.630>\n",
            "<LTLine 91.762,646.675,91.762,658.630>\n",
            "<LTLine 93.755,646.675,93.755,658.630>\n",
            "<LTLine 131.732,646.675,131.732,658.630>\n",
            "<LTLine 226.691,646.675,226.691,658.630>\n",
            "<LTLine 312.260,646.675,312.260,658.630>\n",
            "<LTLine 314.252,646.675,314.252,658.630>\n",
            "<LTLine 355.816,646.675,355.816,658.630>\n",
            "<LTLine 424.558,646.675,424.558,658.630>\n",
            "<LTLine 517.908,646.675,517.908,658.630>\n",
            "<LTLine 74.826,634.720,74.826,646.675>\n",
            "<LTLine 91.762,634.720,91.762,646.675>\n",
            "<LTLine 93.755,634.720,93.755,646.675>\n",
            "<LTLine 131.732,634.720,131.732,646.675>\n",
            "<LTLine 226.691,634.720,226.691,646.675>\n",
            "<LTLine 312.260,634.720,312.260,646.675>\n",
            "<LTLine 314.252,634.720,314.252,646.675>\n",
            "<LTLine 355.816,634.720,355.816,646.675>\n",
            "<LTLine 424.558,634.720,424.558,646.675>\n",
            "<LTLine 517.908,634.720,517.908,646.675>\n",
            "<LTLine 74.826,622.765,74.826,634.720>\n",
            "<LTLine 91.762,622.765,91.762,634.720>\n",
            "<LTLine 93.755,622.765,93.755,634.720>\n",
            "<LTLine 131.732,622.765,131.732,634.720>\n",
            "<LTLine 226.691,622.765,226.691,634.720>\n",
            "<LTLine 312.260,622.765,312.260,634.720>\n",
            "<LTLine 314.252,622.765,314.252,634.720>\n",
            "<LTLine 355.816,622.765,355.816,634.720>\n",
            "<LTLine 424.558,622.765,424.558,634.720>\n",
            "<LTLine 517.908,622.765,517.908,634.720>\n",
            "<LTLine 74.826,610.809,74.826,622.764>\n",
            "<LTLine 91.762,610.809,91.762,622.764>\n",
            "<LTLine 93.755,610.809,93.755,622.764>\n",
            "<LTLine 131.732,610.809,131.732,622.764>\n",
            "<LTLine 226.691,610.809,226.691,622.764>\n",
            "<LTLine 312.260,610.809,312.260,622.764>\n",
            "<LTLine 314.252,610.809,314.252,622.764>\n",
            "<LTLine 355.816,610.809,355.816,622.764>\n",
            "<LTLine 424.558,610.809,424.558,622.764>\n",
            "<LTLine 517.908,610.809,517.908,622.764>\n",
            "<LTLine 74.826,598.854,74.826,610.809>\n",
            "<LTLine 91.762,598.854,91.762,610.809>\n",
            "<LTLine 93.755,598.854,93.755,610.809>\n",
            "<LTLine 131.732,598.854,131.732,610.809>\n",
            "<LTLine 226.691,598.854,226.691,610.809>\n",
            "<LTLine 312.260,598.854,312.260,610.809>\n",
            "<LTLine 314.252,598.854,314.252,610.809>\n",
            "<LTLine 355.816,598.854,355.816,610.809>\n",
            "<LTLine 424.558,598.854,424.558,610.809>\n",
            "<LTLine 517.908,598.854,517.908,610.809>\n",
            "<LTLine 74.826,586.899,74.826,598.854>\n",
            "<LTLine 91.762,586.899,91.762,598.854>\n",
            "<LTLine 93.755,586.899,93.755,598.854>\n",
            "<LTLine 131.732,586.899,131.732,598.854>\n",
            "<LTLine 226.691,586.899,226.691,598.854>\n",
            "<LTLine 312.260,586.899,312.260,598.854>\n",
            "<LTLine 314.252,586.899,314.252,598.854>\n",
            "<LTLine 355.816,586.899,355.816,598.854>\n",
            "<LTLine 424.558,586.899,424.558,598.854>\n",
            "<LTLine 517.908,586.899,517.908,598.854>\n",
            "<LTLine 74.826,574.944,74.826,586.899>\n",
            "<LTLine 91.762,574.944,91.762,586.899>\n",
            "<LTLine 93.755,574.944,93.755,586.899>\n",
            "<LTLine 131.732,574.944,131.732,586.899>\n",
            "<LTLine 226.691,574.944,226.691,586.899>\n",
            "<LTLine 312.260,574.944,312.260,586.899>\n",
            "<LTLine 314.252,574.944,314.252,586.899>\n",
            "<LTLine 355.816,574.944,355.816,586.899>\n",
            "<LTLine 424.558,574.944,424.558,586.899>\n",
            "<LTLine 517.908,574.944,517.908,586.899>\n",
            "<LTLine 74.826,574.745,517.908,574.745>\n",
            "<LTLine 326.611,512.049,525.122,512.049>\n",
            "<LTLine 326.611,500.891,326.611,511.850>\n",
            "<LTLine 424.974,500.891,424.974,511.850>\n",
            "<LTLine 525.122,500.891,525.122,511.850>\n",
            "<LTLine 326.611,500.692,525.122,500.692>\n",
            "<LTLine 326.611,489.534,326.611,500.493>\n",
            "<LTLine 424.974,489.534,424.974,500.493>\n",
            "<LTLine 525.122,489.534,525.122,500.493>\n",
            "<LTLine 326.611,489.335,525.122,489.335>\n",
            "<LTLine 326.611,478.176,326.611,489.135>\n",
            "<LTLine 424.974,478.176,424.974,489.135>\n",
            "<LTLine 525.122,478.176,525.122,489.135>\n",
            "<LTLine 326.611,477.977,525.122,477.977>\n",
            "<LTTextBoxHorizontal(0) 308.862,439.552,545.115,536.252 'Figure 6. Confusion matrix for breed discrimination. The ver-\\ntical axis reports the ground truth labels, and the horizontal axis to\\nthe predicted ones (the upper-left block are the cats). The matrix is\\nnormalized by row and the values along the diagonal are reported\\non the right. The matrix corresponds to the breed classiﬁer using\\nshape features, appearance features with the image, head, body,\\nbody-head layouts with automatic segmentations, and a 37-class\\nSVM. This is the best result for breed classiﬁcation, and corre-\\nsponds to the last entry of row number 8 in Tab. 4.\\n'>\n",
            "<LTTextBoxHorizontal(1) 50.112,294.077,286.367,325.023 'Figure 5. Example segmentation results on Oxford-IIIT Pet\\ndataset. The segmentation of the pet from the background was\\nobtained automatically as described in Sect. 3.3.\\n'>\n",
            "<LTTextBoxHorizontal(2) 331.094,293.429,334.190,300.402 'e\\n'>\n",
            "<LTTextBoxHorizontal(3) 414.990,293.429,417.312,300.402 'f\\n'>\n",
            "<LTTextBoxHorizontal(4) 489.692,293.429,493.179,300.402 'g\\n'>\n",
            "<LTTextBoxHorizontal(5) 349.547,339.934,352.643,346.907 'a\\n'>\n",
            "<LTTextBoxHorizontal(6) 388.953,339.934,392.440,346.907 'b\\n'>\n",
            "<LTTextBoxHorizontal(7) 467.341,339.934,470.437,346.907 'c\\n'>\n",
            "<LTTextBoxHorizontal(8) 538.636,339.934,542.123,346.907 'd\\n'>\n",
            "<LTTextBoxHorizontal(9) 538.636,293.429,542.123,300.402 'h\\n'>\n",
            "<LTTextBoxHorizontal(10) 50.112,78.848,286.365,244.457 'ﬁcation, in which a 37-class SVM is learned directly, using\\nthe same method discussed in Sect. 4.2. The relative per-\\nformance of the different models is similar to that observed\\nin Sect. 4.1 and 4.2. Flat classiﬁcation is better than hier-\\narchical, but the latter requires less work at test time, due\\nto the fact that fewer SVM classiﬁers need to be evaluated.\\nFor example, using the appearance model with the image,\\nhead, image-head layouts for 37 class classiﬁcation yields\\nan accuracy of 51.23%, adding the shape information hi-\\nerarchically improves this accuracy to 52.78%, and using\\nshape and appearance together in a ﬂat classiﬁcation ap-\\nproach achieves an accuracy 54.03%. The confusion matrix\\nfor the best result for breed classiﬁcation, corresponding to\\nthe last entry of the eight row of Table 4 is shown in Fig. 4.\\n'>\n",
            "<LTTextBoxHorizontal(11) 308.862,213.914,545.115,266.923 'Figure 7. Failure cases for the model using appearance only (im-\\nage layout) in Sect. 4.2. First row: Cat images that were incor-\\nrectly classiﬁed as dogs and vice versa. Second row: Bengal cats\\n(b–d) classiﬁed as Egyptian Mau (a). Third row: English Setter\\n(f–h) classiﬁed as English Cocker Spaniel (e).\\n'>\n",
            "<LTTextBoxHorizontal(12) 308.862,181.252,371.292,193.208 '5. Summary\\n'>\n",
            "<LTTextBoxHorizontal(13) 308.862,78.848,545.115,172.497 'This paper has introduced the Oxford-IIIT Pet dataset\\nfor the ﬁne-grained categorisation problem of identifying\\nthe family and breed of pets (cats and dogs). Three differ-\\nent tasks and corresponding baseline algorithms have been\\nproposed and investigated obtaining very encouraging clas-\\nsiﬁcation results on the dataset. Furthermore, the baseline\\nmodels were shown to achieve state-of-the-art performance\\non the ASIRRA challenge data, breaking the test with 42%\\n'>\n",
            "<LTFigure(Im101) 50.112,671.811,114.363,719.999 matrix=[0.21,0.00,0.00,0.21, (50.11,671.81)]>\n",
            "<LTFigure(Im102) 119.362,671.811,183.614,720.000 matrix=[0.32,0.00,0.00,0.32, (119.36,671.81)]>\n",
            "<LTFigure(Im103) 188.613,671.811,249.270,720.000 matrix=[0.42,0.00,0.00,0.42, (188.61,671.81)]>\n",
            "<LTFigure(Im104) 254.269,671.811,286.360,719.996 matrix=[0.10,0.00,0.00,0.10, (254.27,671.81)]>\n",
            "<LTFigure(Im105) 50.112,622.626,114.363,670.814 matrix=[0.21,0.00,0.00,0.21, (50.11,622.63)]>\n",
            "<LTFigure(Im106) 119.365,622.626,183.615,670.813 matrix=[0.13,0.00,0.00,0.13, (119.36,622.63)]>\n",
            "<LTFigure(Im107) 188.613,622.626,249.269,670.814 matrix=[0.17,0.00,0.00,0.17, (188.61,622.63)]>\n",
            "<LTFigure(Im108) 254.269,622.626,286.360,670.811 matrix=[0.10,0.00,0.00,0.10, (254.27,622.63)]>\n",
            "<LTFigure(Im109) 50.112,573.441,122.688,621.631 matrix=[0.60,0.00,0.00,0.60, (50.11,573.44)]>\n",
            "<LTFigure(Im110) 131.156,573.441,203.506,621.626 matrix=[0.14,0.00,0.00,0.14, (131.16,573.44)]>\n",
            "<LTFigure(Im111) 211.974,573.441,244.068,621.630 matrix=[0.32,0.00,0.00,0.32, (211.97,573.44)]>\n",
            "<LTFigure(Im112) 252.536,573.441,286.364,621.629 matrix=[0.13,0.00,0.00,0.13, (252.54,573.44)]>\n",
            "<LTFigure(Im113) 50.112,524.255,122.682,572.441 matrix=[0.15,0.00,0.00,0.15, (50.11,524.25)]>\n",
            "<LTFigure(Im114) 131.153,524.255,203.503,572.440 matrix=[0.14,0.00,0.00,0.14, (131.15,524.25)]>\n",
            "<LTFigure(Im115) 211.972,524.255,244.063,572.440 matrix=[0.10,0.00,0.00,0.10, (211.97,524.25)]>\n",
            "<LTFigure(Im116) 252.535,524.255,286.361,572.440 matrix=[0.10,0.00,0.00,0.10, (252.53,524.25)]>\n",
            "<LTFigure(Im117) 50.112,480.739,106.797,523.253 matrix=[0.11,0.00,0.00,0.11, (50.11,480.74)]>\n",
            "<LTFigure(Im118) 111.004,480.739,167.689,523.253 matrix=[0.11,0.00,0.00,0.11, (111.00,480.74)]>\n",
            "<LTFigure(Im119) 171.896,480.739,235.740,523.259 matrix=[0.53,0.00,0.00,0.53, (171.90,480.74)]>\n",
            "<LTFigure(Im120) 239.945,480.739,286.360,523.255 matrix=[0.09,0.00,0.00,0.09, (239.94,480.74)]>\n",
            "<LTFigure(Im121) 50.112,437.223,106.797,479.737 matrix=[0.11,0.00,0.00,0.11, (50.11,437.22)]>\n",
            "<LTFigure(Im122) 111.004,437.223,167.689,479.737 matrix=[0.11,0.00,0.00,0.11, (111.00,437.22)]>\n",
            "<LTFigure(Im123) 171.896,437.223,235.741,479.744 matrix=[0.13,0.00,0.00,0.13, (171.90,437.22)]>\n",
            "<LTFigure(Im124) 239.945,437.223,286.360,479.739 matrix=[0.09,0.00,0.00,0.09, (239.94,437.22)]>\n",
            "<LTFigure(Im125) 50.112,393.708,106.797,436.222 matrix=[0.11,0.00,0.00,0.11, (50.11,393.71)]>\n",
            "<LTFigure(Im126) 112.431,393.708,169.116,436.222 matrix=[0.11,0.00,0.00,0.11, (112.43,393.71)]>\n",
            "<LTFigure(Im127) 174.750,393.708,238.210,436.226 matrix=[0.13,0.00,0.00,0.13, (174.75,393.71)]>\n",
            "<LTFigure(Im128) 243.844,393.708,286.363,436.227 matrix=[0.10,0.00,0.00,0.10, (243.84,393.71)]>\n",
            "<LTFigure(Im129) 50.112,350.192,106.797,392.706 matrix=[0.11,0.00,0.00,0.11, (50.11,350.19)]>\n",
            "<LTFigure(Im130) 112.431,350.192,169.116,392.706 matrix=[0.11,0.00,0.00,0.11, (112.43,350.19)]>\n",
            "<LTFigure(Im131) 174.750,350.192,238.210,392.710 matrix=[0.13,0.00,0.00,0.13, (174.75,350.19)]>\n",
            "<LTFigure(Im132) 243.844,350.192,286.364,392.712 matrix=[0.09,0.00,0.00,0.09, (243.84,350.19)]>\n",
            "<LTFigure(Im133) 300.281,537.442,564.941,735.937 matrix=[0.43,0.00,0.00,0.43, (300.28,537.44)]>\n",
            "<LTFigure(Im134) 308.862,384.802,365.554,427.321 matrix=[0.28,0.00,0.00,0.28, (308.86,384.80)]>\n",
            "<LTFigure(Im135) 368.332,384.802,425.020,427.318 matrix=[0.12,0.00,0.00,0.12, (368.33,384.80)]>\n",
            "<LTFigure(Im136) 427.799,384.802,483.159,427.318 matrix=[0.11,0.00,0.00,0.11, (427.80,384.80)]>\n",
            "<LTFigure(Im137) 485.935,384.802,542.620,427.316 matrix=[0.11,0.00,0.00,0.11, (485.94,384.80)]>\n",
            "<LTFigure(Im21) 308.862,338.451,355.631,380.968 matrix=[0.25,0.00,0.00,0.25, (308.86,338.45)]>\n",
            "<LTRect 346.558,335.463,355.632,347.602>\n",
            "<LTFigure(Im138) 363.541,338.451,395.428,380.967 matrix=[0.14,0.00,0.00,0.14, (363.54,338.45)]>\n",
            "<LTRect 385.964,335.463,395.429,349.185>\n",
            "<LTFigure(Im139) 403.338,338.451,473.427,380.972 matrix=[0.23,0.00,0.00,0.23, (403.34,338.45)]>\n",
            "<LTRect 464.352,335.463,473.426,347.602>\n",
            "<LTFigure(Im140) 481.335,338.451,545.112,380.969 matrix=[0.23,0.00,0.00,0.23, (481.33,338.45)]>\n",
            "<LTRect 535.648,335.463,545.113,349.185>\n",
            "<LTFigure(Im53) 308.862,291.947,337.180,334.467 matrix=[0.09,0.00,0.00,0.09, (308.86,291.95)]>\n",
            "<LTRect 328.105,288.958,337.179,301.097>\n",
            "<LTFigure(Im141) 356.360,291.947,420.300,334.467 matrix=[0.32,0.00,0.00,0.32, (356.36,291.95)]>\n",
            "<LTRect 412.001,288.958,420.301,302.680>\n",
            "<LTFigure(Im142) 439.482,291.947,496.167,334.461 matrix=[0.11,0.00,0.00,0.11, (439.48,291.95)]>\n",
            "<LTRect 486.703,288.958,496.168,301.097>\n",
            "<LTFigure(Im143) 515.349,291.947,545.113,334.467 matrix=[0.09,0.00,0.00,0.09, (515.35,291.95)]>\n",
            "<LTRect 535.647,288.958,545.112,302.680>\n",
            "<LTTextBoxHorizontal(0) 50.112,695.930,286.365,717.848 'probability, a remarkable achievement considering that this\\ndataset was designed to be challenging for machines.\\n'>\n",
            "<LTTextBoxHorizontal(1) 50.112,639.195,286.365,673.138 'Acknowledgements. We are grateful for ﬁnancial sup-\\nport from the UKIERI, EU Project AXES ICT-269980 and\\nERC grant VisRec no. 228180.\\n'>\n",
            "<LTTextBoxHorizontal(2) 50.112,611.502,105.656,623.458 'References\\n'>\n",
            "<LTTextBoxHorizontal(3) 54.595,573.210,258.692,603.716 '[1] American kennel club. http://www.akc.org/.\\n[2] The cat fanciers association inc.\\norg/Client/home.aspx.\\n'>\n",
            "<LTTextBoxHorizontal(4) 205.665,584.788,286.363,593.754 'http://www.cfa.\\n'>\n",
            "<LTTextBoxHorizontal(5) 54.595,513.434,286.362,572.832 '[3] Cats in sinks. http://catsinsinks.com/.\\n[4] Catster. http://www.catster.com/.\\n[5] Dogster. http://www.dogster.com/.\\n[6] Flickr! http://www.flickr.com/.\\n[7] Google images. http://images.google.com/.\\n[8] The international cat association. http://www.tica.\\n'>\n",
            "<LTTextBoxHorizontal(6) 70.031,502.475,93.792,512.060 'org/.\\n'>\n",
            "<LTTextBoxHorizontal(7) 50.112,461.628,267.919,502.098 '[9] My cat space. http://www.mycatspace.com/.\\n[10] My dog space. http://www.mydogspace.com/.\\n[11] Petﬁnder.\\nhtml.\\n'>\n",
            "<LTTextBoxHorizontal(8) 119.588,473.206,286.363,482.172 'http://www.petfinder.com/index.\\n'>\n",
            "<LTTextBoxHorizontal(9) 50.112,419.785,286.362,461.251 '[12] World canine organisation. http://www.fci.be/.\\n[13] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. From con-\\ntours to regions: An empirical evaluation. In Proc. CVPR,\\n2009.\\n'>\n",
            "<LTTextBoxHorizontal(10) 50.112,387.905,286.363,418.790 '[14] S. Branson, C. Wah, F. Schroff, B. Babenko, P. Welinder,\\nP. Perona, and S. Belongie. Visual recognition with humans\\nin the loop. In Proc. ECCV, 2010.\\n'>\n",
            "<LTTextBoxHorizontal(11) 50.112,366.983,286.362,386.909 '[15] Y. Chai, V. Lempitsky, and A. Zisserman. Bicos: A bi-level\\nIn Proc.\\n'>\n",
            "<LTTextBoxHorizontal(12) 70.031,356.024,250.211,375.950 'co-segmentation method for image classiﬁcation.\\nICCV, 2011.\\n'>\n",
            "<LTTextBoxHorizontal(13) 50.112,313.185,286.363,355.029 '[16] G. Csurka, C. R. Dance, L. Dan, J. Willamowski, and\\nC. Bray. Visual categorization with bags of keypoints.\\nIn\\nProc. ECCV Workshop on Stat. Learn. in Comp. Vision,\\n2004.\\n'>\n",
            "<LTTextBoxHorizontal(14) 50.112,303.222,286.359,312.189 '[17] N. Dalal and B. Triggs. Histograms of oriented gradients for\\n'>\n",
            "<LTTextBoxHorizontal(15) 70.031,292.263,213.313,301.436 'human detection. In Proc. CVPR, 2005.\\n'>\n",
            "<LTTextBoxHorizontal(16) 50.112,260.383,286.363,291.268 '[18] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\\nImageNet: A Large-Scale Hierarchical Image Database. In\\nProc. CVPR, 2009.\\n'>\n",
            "<LTTextBoxHorizontal(17) 50.112,217.544,286.363,259.387 '[19] J. Elson, J. Douceur, J. Howell, and J. J. Saul. Asirra: A\\nCAPTCHA that exploits interest-aligned manual image cat-\\negorization. In Conf. on Computer and Communications Se-\\ncurity (CCS), 2007.\\n'>\n",
            "<LTTextBoxHorizontal(18) 50.112,142.824,288.928,216.548 '[20] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\\nand A. Zisserman. The PASCAL Visual Object Classes\\nChallenge 2011 (VOC2011) Results.\\nhttp://www.pascal-\\nnetwork.org/challenges/VOC/voc2011/workshop/index.html.\\n[21] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J.\\nLin. LIBLINEAR: A library for large linear classiﬁcation.\\nJournal of Machine Learning Research, 9, 2008.\\n'>\n",
            "<LTTextBoxHorizontal(19) 50.112,110.943,286.362,141.828 '[22] L. Fei-Fei, R. Fergus, and P. Perona. A Bayesian approach to\\nunsupervised one-shot learning of object categories. In Proc.\\nICCV, 2003.\\n'>\n",
            "<LTTextBoxHorizontal(20) 50.112,79.063,286.363,109.948 '[23] P. F. Felzenszwalb, R. B. Grishick, D. McAllester, and D. Ra-\\nmanan. Object detection with discriminatively trained part\\nbased models. PAMI, 2009.\\n'>\n",
            "<LTTextBoxHorizontal(21) 308.862,665.261,545.116,717.067 '[24] F. Fleuret and D. Geman. Stationary features and cat detec-\\ntion. Journal of Machine Learning Research, 9, 2008.\\n[25] P. Golle. Machine learning attacks against the asirra captcha.\\nIn 15th ACM Conference on Computer and Communications\\nSecurity (CCS), 2008.\\n'>\n",
            "<LTTextBoxHorizontal(22) 308.862,633.381,545.113,664.265 '[26] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object cat-\\negory dataset. Technical report, California Institute of Tech-\\nnology, 2007.\\n'>\n",
            "<LTTextBoxHorizontal(23) 308.862,601.500,545.113,632.385 '[27] A. Khosla, N. Jayadevaprakash, B. Yao, and F. F. Li. Novel\\ndataset for ﬁne-grained image categorization. In First Work-\\nshop on Fine-Grained Visual Categorization, CVPR, 2011.\\n'>\n",
            "<LTTextBoxHorizontal(24) 308.862,569.620,545.113,600.504 '[28] C. Lampert, H. Nickisch, and S. Harmeling. Learning to de-\\ntect unseen object classes by between-class attribute transfer.\\nIn Proc. CVPR, 2009.\\n'>\n",
            "<LTTextBoxHorizontal(25) 308.862,559.657,545.109,568.624 '[29] I. Laptev. Improvements of object detection using boosted\\n'>\n",
            "<LTTextBoxHorizontal(26) 328.781,548.698,453.889,557.871 'histograms. In Proc. BMVC, 2006.\\n'>\n",
            "<LTTextBoxHorizontal(27) 308.862,516.818,545.113,547.702 '[30] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bag of\\nfeatures: Spatial pyramid matching for recognizing natural\\nscene categories. In Proc. CVPR, 2006.\\n'>\n",
            "<LTTextBoxHorizontal(28) 308.862,506.855,545.109,515.822 '[31] D. G. Lowe. Object recognition from local scale-invariant\\n'>\n",
            "<LTTextBoxHorizontal(29) 328.781,495.896,438.933,505.069 'features. In Proc. ICCV, 1999.\\n'>\n",
            "<LTTextBoxHorizontal(30) 308.862,485.933,545.109,494.900 '[32] M.-E. Nilsback and A. Zisserman. A visual vocabulary for\\n'>\n",
            "<LTTextBoxHorizontal(31) 328.781,474.975,484.293,484.148 'ﬂower classiﬁcation. In Proc. CVPR, 2006.\\n'>\n",
            "<LTTextBoxHorizontal(32) 308.862,443.094,545.112,473.979 '[33] M.-E. Nilsback and A. Zisserman. Automated ﬂower clas-\\nsiﬁcation over a large number of classes. In Proc. ICVGIP,\\n2008.\\n'>\n",
            "<LTTextBoxHorizontal(33) 308.862,433.131,545.109,442.098 '[34] O. Parkhi, A. Vedaldi, C. V. Jawahar, and A. Zisserman. The\\n'>\n",
            "<LTTextBoxHorizontal(34) 328.781,422.173,500.209,431.346 'truth about cats and dogs. In Proc. ICCV, 2011.\\n'>\n",
            "<LTTextBoxHorizontal(35) 308.862,358.412,545.113,421.177 '[35] O. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar. The\\nOxford-IIIT PET Dataset. http://www.robots.ox.\\nac.uk/˜vgg/data/pets/index.html, 2012.\\n[36] C. Rother, V. Kolmogorov, and A. Blake. “grabcut” — in-\\nteractive foreground extraction using iterated graph cuts. In\\nACM Trans. on Graphics, 2004.\\n'>\n",
            "<LTTextBoxHorizontal(36) 308.862,348.449,545.115,357.622 '[37] B. Sch¨olkopf and A. J. Smola. Learning with Kernels. MIT\\n'>\n",
            "<LTTextBoxHorizontal(37) 328.781,337.490,372.367,346.457 'Press, 2002.\\n'>\n",
            "<LTTextBoxHorizontal(38) 308.862,306.606,545.109,336.494 '[38] J. Sivic and A. Zisserman. Video Google: A text retrieval\\napproach to object matching in videos. In Proc. ICCV, 2003.\\n[39] M. Varma and D. Ray. Learning the discriminative power-\\n'>\n",
            "<LTTextBoxHorizontal(39) 328.781,295.647,480.698,304.820 'invariance trade-off. In Proc. ICCV, 2007.\\n'>\n",
            "<LTTextBoxHorizontal(40) 308.862,285.684,545.112,295.269 '[40] A. Vedaldi and B. Fulkerson. VLFeat library. http://\\n'>\n",
            "<LTTextBoxHorizontal(41) 328.781,274.725,434.137,284.310 'www.vlfeat.org/, 2008.\\n'>\n",
            "<LTTextBoxHorizontal(42) 308.862,243.841,545.109,273.730 '[41] A. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman. Mul-\\ntiple kernels for object detection. In Proc. ICCV, 2009.\\n[42] A. Vedaldi and A. Zisserman. Structured output regression\\n'>\n",
            "<LTTextBoxHorizontal(43) 328.781,232.882,534.567,242.055 'for detection with partial occulsion. In Proc. NIPS, 2009.\\n'>\n",
            "<LTTextBoxHorizontal(44) 308.862,201.002,545.113,231.887 '[43] P. Welinder, S. Branson, T. Mita, C. Wah, and F. Schroff.\\nCaltech-ucsd birds 200. Technical report, Caltech-UCSD,\\n2010.\\n'>\n",
            "<LTTextBoxHorizontal(45) 308.862,169.121,545.113,200.006 '[44] J. Zhang, M. Marszalek, S. Lazebnik, and C. Schmid. Local\\nfeatures and kernels for classiﬁcation of texture and object\\ncategories: A comprehensive study. IJCV, 2007.\\n'>\n",
            "<LTTextBoxHorizontal(46) 308.862,148.200,545.112,168.126 '[45] W. Zhang, J. Sun, and X. Tang. Cat head detection - how\\nIn Proc.\\n'>\n",
            "<LTTextBoxHorizontal(47) 328.781,137.241,507.795,157.167 'to effectively exploit shape and texture features.\\nECCV, 2008.\\n'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztmSFPqiPCsH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063ee2cd-6db1-4d24-82ea-60accbb365d2"
      },
      "source": [
        "from pdfminer.high_level import extract_pages\n",
        "from pdfminer.layout import LTTextContainer\n",
        "\n",
        "for page_layout in extract_pages('/content/example02.pdf'):\n",
        "  for element in page_layout:\n",
        "    if isinstance(element, LTTextContainer):\n",
        "      print(element.get_text())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "1\n",
            "0\n",
            "2\n",
            " \n",
            "n\n",
            "a\n",
            "J\n",
            " \n",
            "8\n",
            " \n",
            " \n",
            "]\n",
            "\n",
            "V\n",
            "C\n",
            ".\n",
            "s\n",
            "c\n",
            "[\n",
            " \n",
            " \n",
            "3\n",
            "v\n",
            "6\n",
            "0\n",
            "0\n",
            "5\n",
            "0\n",
            ".\n",
            "0\n",
            "1\n",
            "7\n",
            "1\n",
            ":\n",
            "v\n",
            "i\n",
            "X\n",
            "r\n",
            "a\n",
            "\n",
            "SKIN LESION ANALYSIS TOWARD MELANOMA DETECTION: A CHALLENGE AT THE\n",
            "2017 INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING (ISBI), HOSTED BY\n",
            "THE INTERNATIONAL SKIN IMAGING COLLABORATION (ISIC)\n",
            "\n",
            "Noel C. F. Codella1†, David Gutman2†, M. Emre Celebi3, Brian Helba4,\n",
            "Michael A. Marchetti5, Stephen W. Dusza5, Aadi Kalloo5, Konstantinos Liopyris5,\n",
            "Nabin Mishra6, Harald Kittler7, Allan Halpern5‡\n",
            "\n",
            "1 IBM T. J. Watson Research Center, Yorktown Heights, NY, USA\n",
            "2 Emory University, Atlanta, GA, USA\n",
            "3 University of Central Arkansas, Conway, AR, USA\n",
            "4 Kitware, Clifton Park, NY, USA\n",
            "5 Memorial Sloan-Kettering Cancer Center, New York, NY, USA\n",
            "6 Missouri University of Science and Technology, Rolla, MO USA\n",
            "7 Medical University of Vienna, Vienna, Austria\n",
            "\n",
            "ABSTRACT\n",
            "\n",
            "This article describes the design, implementation, and results\n",
            "of the latest installment of the dermoscopic image analy-\n",
            "sis benchmark challenge. The goal is to support research\n",
            "and development of algorithms for automated diagnosis of\n",
            "melanoma, the most lethal skin cancer. The challenge was\n",
            "divided into 3 tasks: lesion segmentation, feature detection,\n",
            "and disease classiﬁcation. Participation involved 593 registra-\n",
            "tions, 81 pre-submissions, 46 ﬁnalized submissions (includ-\n",
            "ing a 4-page manuscript), and approximately 50 attendees,\n",
            "making this the largest standardized and comparative study\n",
            "in this ﬁeld to date. While the ofﬁcial challenge duration and\n",
            "ranking of participants has concluded, the dataset snapshots\n",
            "remain available for further research and development.\n",
            "\n",
            "Index Terms— Dermatology, dermoscopy, melanoma,\n",
            "\n",
            "skin cancer, challenge, deep learning, dataset\n",
            "\n",
            "1. INTRODUCTION\n",
            "\n",
            "The most prevalent form of cancer in the United States is\n",
            "skin cancer, with 5 million cases occurring annually [1, 2,\n",
            "3]. Melanoma, the most dangerous type, leads to over 9,000\n",
            "deaths a year [2, 3]. Even though most melanomas are ﬁrst\n",
            "discovered by patients [4], the diagnostic accuracy of unaided\n",
            "expert visual inspection is only about 60% [5].\n",
            "\n",
            "Dermoscopy is a recent technique of visual inspection that\n",
            "both magniﬁes the skin and eliminates surface reﬂection. Re-\n",
            "\n",
            "† The ﬁrst two authors contributed equally to this work.\n",
            "‡ Corresponding author.\n",
            "Accepted for publication at ISBI 2018. c(cid:13)2018 IEEE\n",
            "\n",
            "search has shown that with proper training, diagnostic accu-\n",
            "racy with dermoscopy is 75%-84% [5, 6, 7]. In an attempt to\n",
            "improve the scalability of dermoscopic expertise, procedural\n",
            "algorithms, such as “3-point checklist,” “ABCD rule,” “Men-\n",
            "zies method,” and “7-point checklist,” were developed [6, 8].\n",
            "However, many clinicians forgo these methods in favor of re-\n",
            "lying on personal experience, as well as the “ugly duckling”\n",
            "sign (outliers on patient) [9].\n",
            "\n",
            "Recent reports have called attention to a growing shortage\n",
            "of dermatologists per capita [10]. This has increased inter-\n",
            "est in techniques for automated assessment of dermoscopic\n",
            "images [11, 12, 13, 14]. However, most studies have used\n",
            "isolated silos of data for analysis that are not available to the\n",
            "broader research community. While an earlier effort to create\n",
            "a public archive of images was made [14, 15], the dataset was\n",
            "too small (200 images) to fully represent scope of the task.\n",
            "\n",
            "The International Skin Imaging Collaboration (ISIC) has\n",
            "begun to aggregate a large-scale publicly accessible dataset of\n",
            "dermoscopy images. Currently, the dataset houses more than\n",
            "20,000 images from leading clinical centers internationally,\n",
            "acquired from a variety of devices used at each center. The\n",
            "ISIC dataset was the foundation for the ﬁrst public benchmark\n",
            "challenge on dermoscopic image analysis in 2016 [16, 17].\n",
            "The goal of the challenge was to provide a ﬁxed dataset snap-\n",
            "shot to support development of automated melanoma diagno-\n",
            "sis algorithms across 3 tasks of lesion analysis: segmentation,\n",
            "dermoscopic feature detection, and classiﬁcation.\n",
            "\n",
            "In 2017, ISIC hosted the second instance of this challenge,\n",
            "featuring an expanded dataset. In the following sections, the\n",
            "datasets, tasks, metrics, participation, and the results of this\n",
            "challenge are described.\n",
            "\n",
            "Images from “Part 1: Lesion Segmentation.” Top:\n",
            "\n",
            "Fig. 1.\n",
            "Original images. Bottom: Segmentation masks.\n",
            "\n",
            "Fig. 2. Images from “Part 2: Dermoscopic Feature Classi-\n",
            "ﬁcation”. Ground truth labels highlighted in purple. Left:\n",
            "Streaks. Right: Pigment Network.\n",
            "\n",
            "2. DATASET DESCRIPTIONS & TASKS\n",
            "\n",
            "The 2017 challenge consisted of 3 tasks:\n",
            "lesion segmenta-\n",
            "tion, dermoscopic feature detection, and disease classiﬁca-\n",
            "tion. For each, data consisted of images and corresponding\n",
            "ground truth annotations, split into training (n=2000), valida-\n",
            "tion (n=150), and holdout test (n=600) datasets. Predictions\n",
            "could be submitted on validation and test datasets. The valida-\n",
            "tion submissions provided instantaneous feedback in the form\n",
            "of performance evaluations, as well as ranking in comparison\n",
            "to other participants. Test submissions only provided feed-\n",
            "back after the submission deadline. The training, validation,\n",
            "and test datasets continue to be available for download from\n",
            "the following address: http://challenge2017.isic-archive.com/\n",
            "Part 1: Lesion Segmentation Task: Participants were\n",
            "asked to submit automated predictions of lesion segmenta-\n",
            "tions from dermoscopic images in the form of binary masks.\n",
            "Lesion segmentation training data included the original im-\n",
            "age, paired with the expert manual tracing of the lesion\n",
            "boundaries also in the form of a binary mask, where pixel\n",
            "values of 255 were considered inside the area of the lesion,\n",
            "and pixel values of 0 were outside (Fig. 1).\n",
            "\n",
            "Part 2: Dermoscopic Feature Classiﬁcation Task:\n",
            "Participants were asked to automatically detect\n",
            "the fol-\n",
            "lowing four clinically deﬁned dermoscopic features: “net-\n",
            "work,” “negative network,” “streaks,” and “milia-like cysts,”\n",
            "[18, 19]. Pattern detection involved both localization and\n",
            "classiﬁcation (Fig. 2). To reduce the variability and dimen-\n",
            "sionality of spatial feature annotations, the lesion images\n",
            "were subdivided into superpixels using the SLIC algorithm\n",
            "[20]. Lesion dermoscopic feature data included the original\n",
            "lesion image and a corresponding set of superpixel masks,\n",
            "\n",
            "Fig. 3. Example images from “Part 3: Disease Classiﬁcation.”\n",
            "Ground truth labels written above.\n",
            "\n",
            "paired with superpixel-wise expert annotations for the pres-\n",
            "ence or absence of the dermoscopic features. Validation and\n",
            "test sets included images and superpixels without annotation.\n",
            "Part 3: Disease Classiﬁcation Task: Participants were\n",
            "asked to classify images as belonging to one of 3 categories\n",
            "(Fig.\n",
            "3), including “melanoma” (374 training, 30 valida-\n",
            "tion, 117 test), “seborrheic keratosis” (254, 42, and 90), and\n",
            "“benign nevi” (1372, 78, 393), with classiﬁcation scores nor-\n",
            "malized between 0.0 to 1.0 for each category (and 0.5 as bi-\n",
            "nary decision threshold). Lesion classiﬁcation data included\n",
            "the original image paired with the gold standard diagnosis, as\n",
            "well as approximate age (5 year intervals) and gender when\n",
            "available.\n",
            "\n",
            "3. EVALUATION METRICS\n",
            "\n",
            "Details of evaluation metrics have been previously described\n",
            "[16, 17]. For classiﬁcation decisions, any conﬁdence above\n",
            "0.5 was considered positive for a category. For segmentation\n",
            "tasks, pixel values above 128 were considered positive, and\n",
            "pixel values below were considered negative.\n",
            "\n",
            "For evaluation of classiﬁcation decisions, the area under\n",
            "curve (AUC) measurement from the receiver operating char-\n",
            "acteristic (ROC) curve was computed [16].\n",
            "\n",
            "Additionally, for classiﬁcation of melanoma, speciﬁcity\n",
            "was measured on the operating curve where sensitivity was\n",
            "equal to 82%, 89%, and 95%, corresponding to dermatolo-\n",
            "gist classiﬁcation and management performance levels, and\n",
            "theoretically desired sensitivity levels, respectively [17].\n",
            "\n",
            "Segmentation submissions were compared using the Jac-\n",
            "card Index, Dice coefﬁcient, and pixel-wise accuracy [16].\n",
            "Participant ranking used Jaccard.\n",
            "\n",
            "4. RESULTS\n",
            "\n",
            "The 2017 challenge saw 593 registrations, 81 pre-submissions,\n",
            "and 46 ﬁnalized submissions (including a 4 page arXiv paper\n",
            "with each). The associated workshop at ISBI 2017 saw ap-\n",
            "proximately 50 attendees. To date, this has been the largest\n",
            "standardized and comparative study in this ﬁeld, accounting\n",
            "for the size of the dataset, the number of algorithms evalu-\n",
            "In the following, the\n",
            "ated, and the number of participants.\n",
            "results for each challenge part are investigated.\n",
            "\n",
            "Part 1: Lesion Segmentation Task: 21 sets of prediction\n",
            "scores on the ﬁnal test set were submitted for the segmenta-\n",
            "\n",
            "Method\n",
            "/ Rank\n",
            "\n",
            "AVG\n",
            "\n",
            "[22] / 1\n",
            "[23] / 2\n",
            "[23] / 3\n",
            "\n",
            "0.895\n",
            "0.833\n",
            "0.832\n",
            "\n",
            "Net-\n",
            "work\n",
            "\n",
            "0.945\n",
            "0.835\n",
            "0.828\n",
            "\n",
            "Neg.\n",
            "Net-\n",
            "work\n",
            "0.869\n",
            "0.762\n",
            "0.762\n",
            "\n",
            "Streaks Milia-\n",
            "\n",
            "Like\n",
            "Cyst\n",
            "0.807\n",
            "0.838\n",
            "0.837\n",
            "\n",
            "0.960\n",
            "0.896\n",
            "0.900\n",
            "\n",
            "Table 1. Part 2: Dermoscopic Feature Classiﬁcation AUC\n",
            "Measurements. AVG = Average across all categories.\n",
            "\n",
            "Fig. 4. Part 1 example segmentations from top ranked partic-\n",
            "ipant submission. Top Row: Original images. Middle Row:\n",
            "Ground truth segmentations. Bottom Row: Participant predic-\n",
            "tions. ISIC identiﬁers and Jaccard Index values are listed at\n",
            "each column head.\n",
            "\n",
            "Fig. 5. Histogram of Jaccard Index values for individual im-\n",
            "ages from top segmentation task participant submission.\n",
            "\n",
            "tion task, and 39 were submitted to the validation set. The\n",
            "top ranked participant achieved an average Jaccard Index of\n",
            "0.765, accuracy of 93.4%, and Dice coefﬁcient of 0.849, us-\n",
            "ing a variation of a fully convolutional network ensemble (a\n",
            "deep learning approach)\n",
            "[21]. Example segmentations are\n",
            "shown in Fig. 4, and a histogram of individual image Jaccard\n",
            "Index measurements is shown in Fig.\n",
            "5. Subjectively as-\n",
            "sessing the quality of the segmentations, one can observe that\n",
            "segmentations of Jaccard Index 0.8 or above tend to appear\n",
            "visually “correct.” This observation is consistent with prior\n",
            "reports that measured an inter-observer agreement of 0.786 on\n",
            "a subset of 100 images from the ISIC 2016 Challenge [13].\n",
            "When Jaccard falls to 0.7 or below, the “correctness” of the\n",
            "segmentation can be debated. 156 out of 600 images (26%)\n",
            "fell at or below a Jaccard of 0.7. 91 images (15.2%) fell at or\n",
            "below Jaccard of 0.6. This suggests a failure rate of 15% to\n",
            "26%, which is higher than the pixel-wise failure rate of 6.6%.\n",
            "Part 2: Dermoscopic Feature Classiﬁcation Task: For\n",
            "the second year in a row, dermoscopic feature classiﬁcation\n",
            "has received far less participation than other tasks. Only 3\n",
            "submissions [22, 23] on the test set were received from 2 par-\n",
            "ties. Whether this is due to the technical framing of the task\n",
            "(how well it maps to existing frameworks), or the perceived\n",
            "importance of the task, is a matter of current investigation.\n",
            "\n",
            "Regardless, performance levels of those submissions that\n",
            "\n",
            "Fig. 6. ROC curves for top 3 submissions to “Part 3: Disease\n",
            "Classiﬁcation”, as well as linear SVM fusion.\n",
            "\n",
            "were received demonstrated that localization of dermoscopic\n",
            "features is a tractable task for computer vision approaches.\n",
            "Top performance levels are shown in Table 1. AUC was\n",
            "above 0.75 ubiquitously, with an average close to 0.9.\n",
            "\n",
            "Part 3: Disease Classiﬁcation Task: The disease clas-\n",
            "siﬁcation task received 23 ﬁnal test set submissions, and 39\n",
            "validation set submissions. Performance characteristics of the\n",
            "average (AVG) classiﬁcation winner [24], seborrheic kerato-\n",
            "sis (SK) classiﬁcation winner [25], and melanoma (M) clas-\n",
            "siﬁcation winner [26], respectively, are shown in Table 2, as\n",
            "well as 3 fusion strategies [17]: score averaging (AVGSC),\n",
            "linear SVM (L-SVM), and non-linear SVM (NL-SVM) us-\n",
            "ing a histogram intersection kernel. Fusion strategies utilize\n",
            "all submissions on the ﬁnal test set, and are carried out via\n",
            "3-fold cross-validation. SVM input feature vectors included\n",
            "all disease category predictions. Both SVM methods used\n",
            "probabilistic SVM score normalization, producing an output\n",
            "conﬁdence between 0.0 and 1.0 (with 0.5 as binary thresh-\n",
            "old), correlating with the probability of disease on a balanced\n",
            "dataset [13]. ROC curves for the 3 submissions and the best\n",
            "fusion strategy (Linear SVM) are shown in Fig. 6.\n",
            "\n",
            "The 5 major trends observed involve the following: 1)\n",
            "All top submissions implemented various ensembles of deep\n",
            "learning networks. All used additional data sources to train,\n",
            "either from ISIC [24, 26], in-house annotations [25], or ex-\n",
            "ternal sources [26]. 2) Classiﬁcation of seborrheic kerato-\n",
            "sis appears to be an easier task in this dataset, compared to\n",
            "\n",
            "Method\n",
            "\n",
            "[24] Top AVG\n",
            "[25] Top SK\n",
            "[26] Top M\n",
            "AVGSC\n",
            "L-SVM\n",
            "NL-SVM\n",
            "\n",
            "AVG-\n",
            "AUC\n",
            "0.911\n",
            "0.910\n",
            "0.908\n",
            "0.913\n",
            "0.926\n",
            "0.904\n",
            "\n",
            "M-\n",
            "AUC\n",
            "0.868\n",
            "0.856\n",
            "0.874\n",
            "0.872\n",
            "0.892\n",
            "0.853\n",
            "\n",
            "SK-\n",
            "AUC\n",
            "0.953\n",
            "0.965\n",
            "0.943\n",
            "0.954\n",
            "0.960\n",
            "0.955\n",
            "\n",
            "M-\n",
            "SP82\n",
            "0.729\n",
            "0.727\n",
            "0.747\n",
            "0.778\n",
            "0.834\n",
            "0.801\n",
            "\n",
            "M-\n",
            "SP89\n",
            "0.588\n",
            "0.555\n",
            "0.590\n",
            "0.605\n",
            "0.692\n",
            "0.449\n",
            "\n",
            "M-\n",
            "SP95\n",
            "0.366\n",
            "0.404\n",
            "0.395\n",
            "0.435\n",
            "0.571\n",
            "0.168\n",
            "\n",
            "M-\n",
            "SENS\n",
            "0.735\n",
            "0.103\n",
            "0.547\n",
            "0.214\n",
            "0.718\n",
            "0.675\n",
            "\n",
            "M-\n",
            "SPEC\n",
            "0.851\n",
            "0.998\n",
            "0.950\n",
            "0.988\n",
            "0.901\n",
            "0.909\n",
            "\n",
            "SK-\n",
            "SENS\n",
            "0.978\n",
            "0.178\n",
            "0.356\n",
            "0.600\n",
            "0.878\n",
            "0.889\n",
            "\n",
            "SK-\n",
            "SPEC\n",
            "0.773\n",
            "0.998\n",
            "0.990\n",
            "0.975\n",
            "0.931\n",
            "0.928\n",
            "\n",
            "Table 2. Part 3: Disease classiﬁcation evaluation metrics for top 3 participants, followed by average score, linear SVM, and\n",
            "non-linear SVM 3-fold cross-validation fusion methods. Key: M = Melanoma. SK = Seborrheic Keratosis. AVG = Average\n",
            "between two classes. AUC = Area Under Curve. SP82/89/95 = Speciﬁcity mearured at 82/89/95% sensitivity. L-SVM = Linear\n",
            "SVM. NL-SVM = Non-Linear SVM. AVGSC = Average Score.\n",
            "\n",
            "melanoma classiﬁcation. This may reﬂect aspects of the dis-\n",
            "ease, or bias in the dataset. The best performance came from\n",
            "the team that added additional weakly labelled pattern an-\n",
            "notations to their training data [25]. 3) The top average\n",
            "performer was not the best in any single classiﬁcation cate-\n",
            "gory. 4) The most complex fusion approach (NL-SVM) led\n",
            "to a decrease in performance, whereas simpler methods led\n",
            "to overall improvements in performance, consistent with pre-\n",
            "vious ﬁndings\n",
            "[17]. This challenge is the second bench-\n",
            "mark to demonstrate that a collaborative among all partici-\n",
            "pants outperforms any single method alone. 5) Not all thresh-\n",
            "olds balanced sensitivity and speciﬁcity. Probabilistic score\n",
            "normalization in fusions is effective at balancing sensitivity\n",
            "and speciﬁcity [13, 17].\n",
            "\n",
            "5. DISCUSSION & CONCLUSION\n",
            "\n",
            "The International Skin Imaging Collaboration (ISIC) archive\n",
            "was used to host the second public challenge on Skin Le-\n",
            "sion Analysis Toward Melanoma Detection at the Interna-\n",
            "tional Symposium on Biomedical Imaging (ISBI) 2017. The\n",
            "challenge was divided into 3 tasks: segmentation, feature de-\n",
            "tection (4 classes), and disease classiﬁcation (3 classes). 2000\n",
            "images were available for training, 150 for validation, and\n",
            "600 for testing. The challenge involved 593 registrations, 81\n",
            "pre-submissions, and 46 ﬁnalized submissions, making it the\n",
            "largest standardized and comparative study in this ﬁeld.\n",
            "\n",
            "Analysis of segmentation results suggest that the aver-\n",
            "age Jaccard Index may not accurately reﬂect the number of\n",
            "images where automated segmentation falls outside inter-\n",
            "observer variability. Future challenges may adjust the eval-\n",
            "uation metric based on this observation. For example, a\n",
            "binary error may be more appropriate (segmentation failure\n",
            "or success), computed by either using multiple segmentations\n",
            "per image to determine a segmentation difference tolerance\n",
            "threshold, or by choosing a ﬁxed threshold as an estimator\n",
            "based on prior studies.\n",
            "\n",
            "Poor participation was noted in dermoscopic feature de-\n",
            "tection; however, submitted systems achieved reasonable per-\n",
            "\n",
            "formance. Future challenges may adjust the technical format\n",
            "of the task to more closely align with existing image detec-\n",
            "tion benchmarks to better facilitate ease of participation. For\n",
            "example, the output can be formatted as a segmentation or\n",
            "bounding-box detection task.\n",
            "\n",
            "Analysis of the classiﬁcation task demonstrates that en-\n",
            "sembles of deep learning approaches and additional data led\n",
            "to the highest performance. In addition, collaborative fusions\n",
            "of all participant systems outperformed any single system\n",
            "alone. With the exception of [25], submitted methods gener-\n",
            "ate little human interpretable evidence of disease diagnosis.\n",
            "Future work or challenges may give more focus to this need\n",
            "for proper integration into clinical workﬂows.\n",
            "\n",
            "Limitations of this study included dataset bias (not all dis-\n",
            "eases, ages, devices, or ethnicities were represented equally\n",
            "across categories), and incomplete dermoscopic feature an-\n",
            "notations. Reliance on single evaluation metrics rather than\n",
            "combinations may also be a limitation [27]. Future chal-\n",
            "lenges will attempt to address these issues in conjunction with\n",
            "the community.\n",
            "\n",
            "6. REFERENCES\n",
            "\n",
            "[1] Rogers HW, Weinstock MA, Feldman SR, Coldiron BM.:\n",
            "“Incidence estimate of nonmelanoma skin cancer (ker-\n",
            "atinocyte carcinomas) in the US population, 2012” JAMA\n",
            "Dermatol vol. 151, no. 10, pp. 1081-1086. 2015.\n",
            "\n",
            "Figures\n",
            "\n",
            "&\n",
            "Society,\n",
            "\n",
            "Facts\n",
            "Cancer\n",
            "\n",
            "[2] “Cancer\n",
            "ican\n",
            "https://www.cancer.org/research/cancer-facts-\n",
            "statistics/all-cancer-facts-ﬁgures/cancer-facts-ﬁgures-\n",
            "2017.html\n",
            "\n",
            "2017”.\n",
            "\n",
            "2017.\n",
            "\n",
            "Amer-\n",
            "Available:\n",
            "\n",
            "[3] Siegel, R.L., Miller, K.D., and Jemal, A.: “Cancer statis-\n",
            "tics, 2017,” CA: A Cancer Journal for Clinicians, vol. 67,\n",
            "no. 1, pp. 7-30. 2017.\n",
            "\n",
            "[4] Brady, M.S., Oliveria, S.A., Christos, P.J., Berwick, M.,\n",
            "Coit, D.G., Katz, J., Halpern, A.C.: “Patterns of detection\n",
            "in patients with cutaneous melanoma.” Cancer. vol. 89,\n",
            "no. 2, pp. 342-7. 2000.\n",
            "\n",
            "[5] Kittler, H., Pehamberger, H., Wolff, K., Binder, M.: “Di-\n",
            "agnostic accuracy of dermoscopy”. The Lancet Oncol-\n",
            "ogy. vol. 3, no. 3, pp. 159-165. 2002.\n",
            "\n",
            "[6] Carli, P., et al.: “Pattern analysis, not simpliﬁed algo-\n",
            "is the most reliable method for teaching der-\n",
            "rithms,\n",
            "moscopy for melanoma diagnosis to residents in derma-\n",
            "tology”. Br J Dermatol. vol. 148, no. 5, pp. 981-4. 2003.\n",
            "[7] Vestergaard, M.E., Macaskill, P., Holt, P.E., et al.: “Der-\n",
            "moscopy compared with naked eye examination for the\n",
            "diagnosis of primary melanoma: a meta-analysis of stud-\n",
            "ies performed in a clinical setting.” Br J Dermatol. vol.\n",
            "159, pp. 669-676. 2008.\n",
            "\n",
            "[8] Argenziano, G. et al.: “Dermoscopy of pigmented skin le-\n",
            "sions: Results of a consensus meeting via the Internet” J.\n",
            "American Academy of Dermatology. vol. 48, no. 5, 2003.\n",
            "[9] Gachon, J., et. al.:“First Prospective Study of the Recog-\n",
            "nition Process of Melanoma in Dermatological Practice”.\n",
            "Arch Dermatol. vol. 141, no. 4, pp. 434-438, 2005.\n",
            "[10] Kimball, A.B., Resneck, J.S. Jr.: “The US dermatology\n",
            "workforce: a specialty remains in shortage.” J Am Acad\n",
            "Dermatol. vol. 59, no. 5, pp. 741-5. 2008.\n",
            "\n",
            "[11] Mishra, N.K., Celebi, M.E.:\n",
            "\n",
            "“An Overview of\n",
            "Melanoma Detection in Dermoscopy Images Using\n",
            "Image Processing and Machine Learning” arxiv.org:\n",
            "1601.07843. Available: http://arxiv.org/abs/1601.07843\n",
            "\n",
            "[12] Ali, A.A., Deserno, T.M.: “A Systematic Review of\n",
            "Automated Melanoma Detection in Dermatoscopic Im-\n",
            "ages and its Ground Truth Data” Proc. of SPIE Vol. 8318\n",
            "83181I-1\n",
            "\n",
            "[13] Codella NCF, Nguyen B, Pankanti S, Gutman D, Helba\n",
            "B, Halpern A, Smith JR. “Deep learning ensembles for\n",
            "melanoma recognition in dermoscopy images” IBM Jour-\n",
            "nal of Research and Development, vol. 61, no. 4/5, 2017.\n",
            "Available: https://arxiv.org/pdf/1610.04662.pdf\n",
            "\n",
            "[14] Barata, C., Ruela, M., et al.: “Two Systems for the De-\n",
            "tection of Melanomas in Dermoscopy Images using Tex-\n",
            "ture and Color Features”. IEEE Systems Journal, vol. 8,\n",
            "no. 3, pp. 965-979, 2014.\n",
            "\n",
            "[15] Mendonca, T., Ferreira, P.M., Marques, J.S., Marcal,\n",
            "A.R., Rozeira, J.: “PH2 - a dermoscopic image database\n",
            "for research and benchmarking”. Conf Proc IEEE Eng\n",
            "Med Biol Soc. pp. 5437-40, 2013.\n",
            "\n",
            "[16] Gutman D, Codella N, Celebi E, Helba B, Marchetti\n",
            "M, Mishra N, Halpern A. “Skin Lesion Analysis to-\n",
            "ward Melanoma Detection: A Challenge at the Interna-\n",
            "tional Symposium on Biomedical Imaging (ISBI) 2016,\n",
            "hosted by the International Skin Imaging Collaboration\n",
            "(ISIC)”. eprint arXiv:1605.01397 [cs.CV]. 2016. Avail-\n",
            "able: https://arxiv.org/abs/1605.01397\n",
            "\n",
            "[17] Marchetti M, et al. “Results of the 2016 International\n",
            "Skin Imaging Collaboration International Symposium on\n",
            "Biomedical Imaging challenge: Comparison of the ac-\n",
            "curacy of computer algorithms to dermatologists for the\n",
            "\n",
            "diagnosis of melanoma from dermoscopic images”. Jour-\n",
            "nal of the American Academy of Dermatology, 2017. In\n",
            "Press.\n",
            "\n",
            "[18] Braun, R.P., Rabinovitz, H.S., Oliviero, M., Kopf, A.W.,\n",
            "Saurat, J.H.:“Dermoscopy of pigmented skin lesions.”. J\n",
            "Am Acad Dermatol. vol. 52, no. 1, pp. 109-21. 2005.\n",
            "[19] Rezze, G.G., Soares de S, B.C., Neves, R.I.: “Der-\n",
            "moscopy: the pattern analysis”. An Bras Dermatol., vol.\n",
            "3, pp. 261-8. 2006.\n",
            "\n",
            "[20] Achanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P.,\n",
            "and Susstrun,S.: “SLIC Superpixels”, EPFL Technical\n",
            "Report 149300, June 2010.\n",
            "\n",
            "[21] Yuan Y, Chao M, Lo YC. “Automatic skin lesion\n",
            "segmentation with fully convolutional-deconvolutional\n",
            "International Skin Imaging Collaboration\n",
            "networks”.\n",
            "(ISIC) 2017 Challenge at\n",
            "the International Sym-\n",
            "posium on Biomedical\n",
            "Imaging (ISBI). Available:\n",
            "https://arxiv.org/pdf/1703.05165.pdf\n",
            "\n",
            "[22] Kawahara J, Hamarneh G. “Fully Convolutional Net-\n",
            "works to Detect Clinical Dermoscopic Features”. Interna-\n",
            "tional Skin Imaging Collaboration (ISIC) 2017 Challenge\n",
            "at the International Symposium on Biomedical Imaging\n",
            "(ISBI). Available: https://arxiv.org/abs/1703.04559\n",
            "[23] Li Y, Shen L. “Skin Lesion Analysis Towards Melanoma\n",
            "Detection Using Deep Learning Network”. International\n",
            "Skin Imaging Collaboration (ISIC) 2017 Challenge at the\n",
            "International Symposium on Biomedical Imaging (ISBI).\n",
            "Available: https://arxiv.org/abs/1703.00577\n",
            "\n",
            "[24] Matsunaga K, Hamada A, Minagawa A, Koga H. “Im-\n",
            "age Classiﬁcation of Melanoma, Nevus and Seborrheic\n",
            "Keratosis by Deep Neural Network Ensemble”. Interna-\n",
            "tional Skin Imaging Collaboration (ISIC) 2017 Challenge\n",
            "at the International Symposium on Biomedical Imaging\n",
            "(ISBI). Available: https://arxiv.org/abs/1703.03108\n",
            "[25] Daz IG. “Incorporating the Knowledge of Dermatolo-\n",
            "gists to Convolutional Neural Networks for the Diagno-\n",
            "sis of Skin Lesions”. International Skin Imaging Col-\n",
            "laboration (ISIC) 2017 Challenge at the International\n",
            "Symposium on Biomedical Imaging (ISBI). Available:\n",
            "https://arxiv.org/abs/1703.01976\n",
            "\n",
            "[26] Menegola A, Tavares\n",
            "\n",
            "J, Fornaciali M, Li LT,\n",
            "ISIC Chal-\n",
            "Avila S, Valle E. “RECOD Titans at\n",
            "International Skin Imaging Collabora-\n",
            "lenge 2017”.\n",
            "tion (ISIC) 2017 Challenge at the International Sym-\n",
            "posium on Biomedical\n",
            "Imaging (ISBI). Available:\n",
            "https://arxiv.org/pdf/1703.04819.pdf\n",
            "\n",
            "[27] Fishbaugh, J, et al. ”Data-Driven Rank Aggregation\n",
            "with Application to Grand Challenges.” International\n",
            "Conference on Medical Image Computing and Computer-\n",
            "Assisted Intervention (MICCAI). Springer, pp. 754-762.\n",
            "2017.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoAMGkhK9ORS"
      },
      "source": [
        "### **Extração de Imagens**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFIU3bMu9Vpf"
      },
      "source": [
        "O comando a seguir extrai todas as imagens do arquivo PDF e armazena no diretório especificado em `--output-dir`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV8XJaAmT-DI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28b4e440-9d6b-4c64-ca70-accf6ab5cdf0"
      },
      "source": [
        "!pdf2txt.py example01.pdf --output-dir cats-and-dogs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cats and Dogs\n",
            "\n",
            "Omkar M Parkhi1,2 Andrea Vedaldi1 Andrew Zisserman1 C. V. Jawahar2\n",
            "1Department of Engineering Science,\n",
            "University of Oxford,\n",
            "United Kingdom\n",
            "{omkar,vedaldi,az}@robots.ox.ac.uk\n",
            "\n",
            "2Center for Visual Information Technology,\n",
            "International Institute of Information Technology,\n",
            "Hyderabad, India\n",
            "jawahar@iiit.ac.in\n",
            "\n",
            "Abstract\n",
            "\n",
            "We investigate the ﬁne grained object categorization\n",
            "problem of determining the breed of animal from an image.\n",
            "To this end we introduce a new annotated dataset of pets,\n",
            "the Oxford-IIIT-Pet dataset, covering 37 different breeds of\n",
            "cats and dogs. The visual problem is very challenging as\n",
            "these animals, particularly cats, are very deformable and\n",
            "there can be quite subtle differences between the breeds.\n",
            "\n",
            "We make a number of contributions: ﬁrst, we introduce a\n",
            "model to classify a pet breed automatically from an image.\n",
            "The model combines shape, captured by a deformable part\n",
            "model detecting the pet face, and appearance, captured by\n",
            "a bag-of-words model that describes the pet fur. Fitting the\n",
            "model involves automatically segmenting the animal in the\n",
            "image. Second, we compare two classiﬁcation approaches:\n",
            "a hierarchical one, in which a pet is ﬁrst assigned to the cat\n",
            "or dog family and then to a breed, and a ﬂat one, in which\n",
            "the breed is obtained directly. We also investigate a number\n",
            "of animal and image orientated spatial layouts.\n",
            "\n",
            "These models are very good: they beat all previously\n",
            "published results on the challenging ASIRRA test (cat vs\n",
            "dog discrimination). When applied to the task of discrimi-\n",
            "nating the 37 different breeds of pets, the models obtain an\n",
            "average accuracy of about 59%, a very encouraging result\n",
            "considering the difﬁculty of the problem.\n",
            "\n",
            "1. Introduction\n",
            "\n",
            "Research on object category recognition has largely fo-\n",
            "cused on the discrimination of well distinguished object cat-\n",
            "egories (e.g, airplane vs cat). Most popular international\n",
            "benchmarks (e.g, Caltech-101 [22], Caltech-256 [26], PAS-\n",
            "CAL VOC [20]) contain a few dozen object classes that,\n",
            "for the most part, are visually dissimilar. Even in the much\n",
            "larger ImageNet database [18], categories are deﬁned based\n",
            "on a high-level ontology and, as such, any visual similar-\n",
            "ity between them is more accidental than systematic. This\n",
            "work concentrates instead on the problem of discriminat-\n",
            "\n",
            "ing different breeds of cats and dogs, a challenging exam-\n",
            "ple of ﬁne grained object categorization in line with that of\n",
            "previous work on ﬂower [15, 32, 33, 39] and animal and\n",
            "bird species [14, 27, 28, 43] categorization. The difﬁculty\n",
            "is in the fact that breeds may differ only by a few subtle\n",
            "phenotypic details that, due to the highly deformable na-\n",
            "ture of the bodies of such animals, can be difﬁcult to mea-\n",
            "sure automatically. Indeed, authors have often focused on\n",
            "cats and dogs as examples of highly deformable objects\n",
            "for which recognition and detection is particularly challeng-\n",
            "ing [24, 29, 34, 45].\n",
            "\n",
            "Beyond the technical interest of ﬁne grained categoriza-\n",
            "tion, extracting information from images of pets has a prac-\n",
            "tical side too. People devote a lot of attention to their do-\n",
            "mestic animals, as suggested by the large number of so-\n",
            "cial networks dedicated to the sharing of images of cats\n",
            "and dogs: Pet Finder [11], Catster [4], Dogster [5], My\n",
            "Cat Space [9], My Dog Space [10], The International Cat\n",
            "Association [8] and several others [1, 2, 3, 12].\n",
            "In fact,\n",
            "the bulk of the data used in this paper has been extracted\n",
            "from annotated images that users of these social sites post\n",
            "daily (Sect. 2). It is not unusual for owners to believe (and\n",
            "post) the incorrect breed for their pet, so having a method\n",
            "of automated classiﬁcation could provide a gentle way of\n",
            "alerting them to such errors.\n",
            "\n",
            "The ﬁrst contribution of this paper is the introduction of a\n",
            "large annotated collection of images of 37 different breeds\n",
            "of cats and dogs (Sect. 2).\n",
            "It includes 12 cat breeds and\n",
            "25 dog breeds. This data constitutes the benchmark for pet\n",
            "breed classiﬁcation, and, due to its focus on ﬁne grained cat-\n",
            "egorization, is complementary to the standard object recog-\n",
            "nition benchmarks. The data, which is publicly available,\n",
            "comes with rich annotations: in addition to a breed label,\n",
            "each pet has a pixel level segmentation and a rectangle lo-\n",
            "calising its head. A simple evaluation protocol, inspired by\n",
            "the PASCAL VOC challenge, is also proposed to enable\n",
            "the comparison of future methods on a common grounds\n",
            "(Sect. 2). This dataset is also complementary to the subset\n",
            "of ImageNet used in [27] for dogs, as it contains additional\n",
            "annotations, though for fewer breeds.\n",
            "\n",
            "1\n",
            "\n",
            "\fVOC data. The dataset contains about 200 images for each\n",
            "breed (which have been split randomly into 50 for training,\n",
            "50 for validation, and 100 for testing). A detailed list of\n",
            "breeds is given in Tab. 1, and example images are given in\n",
            "Fig. 2. The dataset is available at [35].\n",
            "\n",
            "Dataset collection. The pet images were downloaded\n",
            "from Catster [4] and Dogster [5], two social web sites ded-\n",
            "icated to the collection and discussion of images of pets,\n",
            "from Flickr [6] groups, and from Google images [7]. Peo-\n",
            "ple uploading images to Catster and Dogster provide the\n",
            "breed information as well, and the Flickr groups are spe-\n",
            "ciﬁc to each breed, which simpliﬁes tagging. For each of\n",
            "the 37 breeds, about 2,000 – 2,500 images were down-\n",
            "loaded from these data sources to form a pool of candidates\n",
            "for inclusion in the dataset. From this candidate list, im-\n",
            "ages were dropped if any of the following conditions ap-\n",
            "plied, as judged by the annotators: (i) the image was gray\n",
            "scale, (ii) another image portraying the same animal existed\n",
            "(which happens frequently in Flickr), (iii) the illumination\n",
            "was poor, (iv) the pet was not centered in the image, or (v)\n",
            "the pet was wearing clothes. The most common problem\n",
            "in all the data sources, however, was found to be errors in\n",
            "the breed labels. Thus labels were reviewed by the human\n",
            "annotators and ﬁxed whenever possible. When ﬁxing was\n",
            "not possible, for instance because the pet was a cross breed,\n",
            "the image was dropped. Overall, up to 200 images for each\n",
            "of the 37 breeds were obtained.\n",
            "\n",
            "Annotations. Each image is annotated with a breed label,\n",
            "a pixel level segmentation marking the body, and a tight\n",
            "bounding box about the head. The segmentation is a trimap\n",
            "with regions corresponding to: foreground (the pet body),\n",
            "background, and ambiguous (the pet body boundary and\n",
            "any accessory such as collars). Fig. 1 shows examples of\n",
            "these annotations.\n",
            "\n",
            "Evaluation protocol. Three tasks are deﬁned: pet family\n",
            "classiﬁcation (Cat vs Dog, a two class problem), breed clas-\n",
            "siﬁcation given the family (a 12 class problem for cats and\n",
            "a 25 class problem for dogs), and breed and family classi-\n",
            "ﬁcation (a 37 class problem). In all cases, the performance\n",
            "is measured as the average per-class classiﬁcation accuracy.\n",
            "This is the proportion of correctly classiﬁed images for each\n",
            "of the classes and can be computed as the average of the\n",
            "diagonal of the (row normalized) confusion matrix. This\n",
            "means that, for example, a random classiﬁer has average ac-\n",
            "curacy of 1/2 = 50% for the family classiﬁcation task, and\n",
            "of 1/37 ≈ 3% for the breed and family classiﬁcation task.\n",
            "Algorithms are trained on the training and validation sub-\n",
            "sets and tested on the test subset. The split between training\n",
            "and validation is provided only for convenience, but can be\n",
            "disregarded.\n",
            "\n",
            "Figure 1. Annotations in the Oxford-IIIT Pet data. From left\n",
            "to right: pet image, head bounding box, and trimap segmentation\n",
            "(blue: background region; red: ambiguous region; yellow: fore-\n",
            "ground region).\n",
            "\n",
            "The second contribution of the paper is a model for pet\n",
            "breed discrimination (Sect. 3). The model captures both\n",
            "shape (by a deformable part model [23, 42] of the pet face)\n",
            "and texture (by a bag-of-visual-words model [16, 30, 38, 44]\n",
            "of the pet fur). Unfortunately, current deformable part mod-\n",
            "els are not sufﬁciently advanced to represent satisfactorily\n",
            "the highly deformable bodies of cats and dogs; nevertheless,\n",
            "they can be used to reliably extract stable and distinctive\n",
            "components of the body, such as the pet face. The method\n",
            "used in [34] followed from this observation: a cat’s face\n",
            "was detected as the ﬁrst stage in detecting the entire animal.\n",
            "Here we go further in using the detected head shape as a part\n",
            "of the feature descriptor. Two natural ways of combining\n",
            "the shape and appearance features are then considered and\n",
            "compared: a ﬂat approach, in which both features are used\n",
            "to regress the pet’s family and the breed simultaneously, and\n",
            "a hierarchical one, in which the family is determined ﬁrst\n",
            "based on the shape features alone, and then appearance is\n",
            "used to predict the breed conditioned on the family. Infer-\n",
            "ring the model in an image involves segmenting the animal\n",
            "from the background. To this end, we improved on our pre-\n",
            "vious method on of segmentation in [34] basing it on the\n",
            "extraction of superpixels.\n",
            "\n",
            "The model is validated experimentally on the task of dis-\n",
            "criminating the 37 pet breeds (Sect. 4), obtaining very en-\n",
            "couraging results, especially considering the toughness of\n",
            "the problem. Furthermore, we also use the model to break\n",
            "the ASIRRA test that uses the ability of discriminating be-\n",
            "tween cats and dogs to tell humans from machines.\n",
            "\n",
            "2. Datasets and evaluation measures\n",
            "\n",
            "2.1. The Oxford-IIIT Pet dataset\n",
            "\n",
            "The Oxford-IIIT Pet dataset is a collection of 7,349 im-\n",
            "ages of cats and dogs of 37 different breeds, of which 25\n",
            "are dogs and 12 are cats. Images are divided into training,\n",
            "validation, and test sets, in a similar manner to the PASCAL\n",
            "\n",
            "\fBreed\n",
            "\n",
            "Abyssinian\n",
            "Bengal\n",
            "Birman\n",
            "Bombay\n",
            "British Shorthair\n",
            "Egyptian Mau\n",
            "Maine Coon\n",
            "Persian\n",
            "Ragdoll\n",
            "Russian Blue\n",
            "Siamese\n",
            "Sphynx\n",
            "American Bulldog\n",
            "American Pit Bull Terrier\n",
            "Basset Hound\n",
            "Beagle\n",
            "Boxer\n",
            "Chihuahua\n",
            "English Cocker Spaniel\n",
            "\n",
            "Breed\n",
            "\n",
            "Japanese Chin\n",
            "\n",
            "Training Validation Test Total\n",
            "50\n",
            "50\n",
            "50\n",
            "47\n",
            "50\n",
            "46\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "49\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "46\n",
            "\n",
            "98\n",
            "100\n",
            "100\n",
            "88\n",
            "100\n",
            "97\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "99\n",
            "100\n",
            "100\n",
            "\n",
            "198 English Setter\n",
            "200 German Shorthaired\n",
            "200 Great Pyrenees\n",
            "184 Havanese\n",
            "200\n",
            "190 Keeshond\n",
            "200 Leonberger\n",
            "200 Miniature Pinscher\n",
            "200 Newfoundland\n",
            "200\n",
            "199\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "199 Wheaten Terrier\n",
            "200 Yorkshire Terrier\n",
            "196\n",
            "\n",
            "Pomeranian\n",
            "Pug\n",
            "Saint Bernard\n",
            "Samoyed\n",
            "Scottish Terrier\n",
            "Shiba Inu\n",
            "Staffordshire Bull Terrier\n",
            "\n",
            "50\n",
            "50\n",
            "50\n",
            "49\n",
            "50\n",
            "47\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "\n",
            "Total\n",
            "\n",
            "Training Validation Test Total\n",
            "200\n",
            "100\n",
            "200\n",
            "100\n",
            "200\n",
            "100\n",
            "200\n",
            "100\n",
            "200\n",
            "100\n",
            "199\n",
            "99\n",
            "200\n",
            "100\n",
            "200\n",
            "100\n",
            "196\n",
            "100\n",
            "200\n",
            "100\n",
            "200\n",
            "100\n",
            "200\n",
            "100\n",
            "200\n",
            "100\n",
            "199\n",
            "99\n",
            "200\n",
            "100\n",
            "189\n",
            "89\n",
            "200\n",
            "100\n",
            "100\n",
            "200\n",
            "3669 7349\n",
            "\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "1846\n",
            "\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "46\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "1834\n",
            "\n",
            "Table 1. Oxford-IIIT Pet data composition. The 12 cat breeds followed by the 25 dog breeds.\n",
            "\n",
            "Abyssinian\n",
            "\n",
            "Bengal\n",
            "\n",
            "Bombay\n",
            "\n",
            "Birman\n",
            "\n",
            "British Shorthair\n",
            "\n",
            "Maine Coon\n",
            "\n",
            "Persian\n",
            "\n",
            "Egyptian\n",
            "\n",
            "Ragdoll\n",
            "\n",
            "Russian Blue\n",
            "\n",
            "Siamese\n",
            "\n",
            "Sphynx\n",
            "\n",
            "Eng. Setter\n",
            "\n",
            "Boxer\n",
            "\n",
            "Keeshond\n",
            "\n",
            "Havanese\n",
            "\n",
            "Basset Hound\n",
            "\n",
            "Mini Pinscher\n",
            "\n",
            "Chihuahua\n",
            "\n",
            "Great Pyrenees\n",
            "\n",
            "German Shorthaired\n",
            "\n",
            "Beagle\n",
            "\n",
            "Staff. Bull Terrier\n",
            "\n",
            "Eng. Cocker\n",
            "\n",
            "New Found Land\n",
            "\n",
            "Pomeranian\n",
            "\n",
            "Leonberger\n",
            "\n",
            "Am. Pit Bull Terrier\n",
            "\n",
            "Wheaten Terrier\n",
            "\n",
            "Japanese Chin\n",
            "\n",
            "Samoyed\n",
            "\n",
            "Scottish Terrier\n",
            "\n",
            "Shiba Inu\n",
            "\n",
            "Pug\n",
            "\n",
            "Saint Bernard\n",
            "\n",
            "Am. Bull Dog\n",
            "\n",
            "Figure 2. Example images from the Oxford-IIIT Pet data. Two images per breed are shown side by side to illustrate the data variability.\n",
            "\n",
            "2.2. The ASIRRA dataset\n",
            "\n",
            "Microsoft Research (MSR) proposed the problem of dis-\n",
            "criminating cats from dogs as a test to tell humans from ma-\n",
            "\n",
            "chines, and created the ASIRRA test ([19], Fig. 3) on this ba-\n",
            "sis. The assumption is that, out of a batch of twelve images\n",
            "of pets, any machine would predict incorrectly the family\n",
            "\n",
            "\fseen by examining the performance of this detector on the\n",
            "cats and dogs in the recent PASCAL VOC 2011 challenge\n",
            "data [20]. The deformable parts detector [23] obtains an\n",
            "Average Precision (AP) of only 31.7% and 22.1% on cats\n",
            "and dogs respectively [20]; by comparison, an easier cat-\n",
            "egory such as bicycle has AP of 54% [20]. However, in\n",
            "the PASCAL VOC challenge the task is to detect the whole\n",
            "body of the animal. As in the method of [34], we use the\n",
            "deformable part model to detect certain stable and distinc-\n",
            "tive components of the body. In particular, the head annota-\n",
            "tions included in the Oxford-IIIT Pet data are used to learn\n",
            "a deformable part model of the cat faces, and one of the\n",
            "dog faces ([24, 29, 45] also focus on modelling the faces of\n",
            "pets). Sect. 4.1 shows that these shape models are in fact\n",
            "very good.\n",
            "\n",
            "To represent texture, we use a bag-of-words [16] model.\n",
            "Visual words [38] are computed densely on the image by ex-\n",
            "tracting SIFT descriptors [31] with a stride of 6 pixels and\n",
            "at 4 scales, deﬁned by setting the width of the SIFT spatial\n",
            "bins to 4, 6, 8, and 10 pixels respectively. The SIFT features\n",
            "have constant orientation (i.e, they are not adapted to the lo-\n",
            "cal image appearance). The SIFT descriptors are then quan-\n",
            "tized based on a vocabulary of 4,000 visual words. The vo-\n",
            "cabulary is learned by using k-means on features randomly\n",
            "sampled from the training data. In order to obtain a descrip-\n",
            "tor for the image, the quantized SIFT features are pooled\n",
            "into a spatial histogram [30], which has dimension equal to\n",
            "4,000 times the number of spatial bins. Histograms are then\n",
            "l1 normalized and used in a support vector machine (SVM)\n",
            "based on the exponential-χ2 kernel [44] for classiﬁcation.\n",
            "\n",
            "Different variants of the spatial histograms can be ob-\n",
            "tained by placing the spatial bins in correspondence of par-\n",
            "ticular geometric features of the pet. These layouts are de-\n",
            "scribed next and in Fig. 4:\n",
            "\n",
            "Image layout. This layout consists of ﬁve spatial bins or-\n",
            "ganized as a 1 × 1 and a 2 × 2 grids (Fig. 4a) covering the\n",
            "entire image area, as in [30]. This results in a 20,000 di-\n",
            "mensional feature vector.\n",
            "\n",
            "Image+head layout. This layout adds to the image layout\n",
            "just described a spatial bin in correspondence of the head\n",
            "bounding box (as detected by the deformable part model\n",
            "of the pet face) as well as one for the complement of this\n",
            "box. These two regions do not contain further spatial subdi-\n",
            "visions (Fig. 4b). Concatenating the histograms for all the\n",
            "spatial bins in this layout results in a 28,000 dimensional\n",
            "feature vector.\n",
            "\n",
            "Figure 3. Example images from the MSR ASIRRA dataset.\n",
            "\n",
            "3.2. Appearance model\n",
            "\n",
            "of at least one of them, while humans would make no mis-\n",
            "takes. The ASIRRA test is currently used to protect a num-\n",
            "ber of web sites from the unwanted access by Internet bots.\n",
            "However, the reliability of this test depends on the clas-\n",
            "siﬁcation accuracy α of the classiﬁer implemented by the\n",
            "bot. For instance, if the classiﬁer has accuracy α = 95%,\n",
            "then the bot fools the ASIRRA test roughly half of the times\n",
            "(α12 ≈ 54%).\n",
            "\n",
            "The complete MSR ASIRRA system is based on a\n",
            "database of several millions images of pets, equally divided\n",
            "between cats and dogs. Our classiﬁers are tested on the\n",
            "24,990 images that have been made available to the public\n",
            "for research and evaluation purposes.\n",
            "\n",
            "3. A model for breed discrimination\n",
            "\n",
            "The breed of a pet affects its size, shape, fur type and\n",
            "color. Since it is not possible to measure the pet size from\n",
            "an image without an absolute reference, our model focuses\n",
            "on capturing the pet shape (Sect. 3.1) and the appearance of\n",
            "its fur (Sect. 3.2). The model also involves automatically\n",
            "segmenting the pet from the image background (Sect. 3.3).\n",
            "\n",
            "3.1. Shape model\n",
            "\n",
            "To represent shape, we use the deformable part model\n",
            "of [23]. In this model, an object is given by a root part con-\n",
            "nected with springs to eight smaller parts at a ﬁner scale.\n",
            "The appearance of each part is represented by a HOG ﬁl-\n",
            "ter [17], capturing the local distribution of the image edges;\n",
            "inference (detection) uses dynamic programming to ﬁnd the\n",
            "best trade-off between matching well each part to the image\n",
            "and not deforming the springs too much.\n",
            "\n",
            "While powerful, this model is insufﬁcient to represent\n",
            "the ﬂexibility and variability of a pet body. This can be\n",
            "\n",
            "Image+head+body layout. This layout combines the\n",
            "spatial tiles in the image layout with an additional spatial bin\n",
            "\n",
            "\f(a) Image\n",
            "\n",
            "(b) Image+Head\n",
            "\n",
            "(c) Image+Head+Body\n",
            "\n",
            "Figure 4. Spatial histogram layouts. The three different spatial\n",
            "layouts used for computing the image descriptors. The image de-\n",
            "scriptor in each case is formed by concatenating the histograms\n",
            "computed on the individual spatial components of the layout. The\n",
            "spatial bins are denoted by yellow-black lines.\n",
            "\n",
            "in correspondence of the pet head (as for the image+head\n",
            "layout) as well as other spatial bins computed on the fore-\n",
            "ground object region and its complement, as described next\n",
            "and in Fig. 4c. The foreground region is obtained either\n",
            "from the automatic segmentation of the pet body or from\n",
            "the ground-truth segmentation to obtain a best-case base-\n",
            "line. The foreground region is subdivided into ﬁve spatial\n",
            "bins, similar to the image layout. An additional bin obtained\n",
            "from the foreground region with the head region removed\n",
            "and no further spatial subdivisions is also used. Concate-\n",
            "nating the histograms for all the spatial bins in this layout\n",
            "results in a 48,000 dimensional feature vector.\n",
            "\n",
            "3.3. Automatic segmentation\n",
            "\n",
            "The foreground (pet) and background regions needed for\n",
            "computing the appearance descriptors are obtained auto-\n",
            "matically using the grab-cut segmentation technique [36].\n",
            "Initialization of grab-cut segmentations was done using\n",
            "cues from the over-segmentation of an image (i.e, super-\n",
            "pixels) similar to the method of [15].\n",
            "In this method, a\n",
            "SVM classiﬁer is used to assign superpixels a conﬁdence\n",
            "score. This conﬁdence score is then used to assign super-\n",
            "pixels to a foreground or background region to initialize\n",
            "the grab-cut iteration. We used Berkeley’s ultrametric color\n",
            "map (UCM) [13] for obtaining the superpixels. Each super-\n",
            "pixel was described by a feature vector comprising the color\n",
            "histogram and Sift-BoW histogram computed on it. Super-\n",
            "pixels were assigned a score using a linear-SVM [21] which\n",
            "was trained on the features computed on the training data.\n",
            "After this initialization, grab-cut was used as in [34]. The\n",
            "improved initialization achieves segmentation accuracy of\n",
            "65% this improving over our previous method [34] by 4%\n",
            "and is about 20% better than simply choosing all pixels as\n",
            "foreground (i.e, assuming the pet foreground entirely occu-\n",
            "pies the image). (Tab. 2). Example segmentations produced\n",
            "by our method on the Oxford-IIIT Pet data are shown in\n",
            "Fig. 5.\n",
            "\n",
            "Method\n",
            "All foreground\n",
            "Parkhi et al. [34]\n",
            "This paper\n",
            "\n",
            "Mean Segmentation Accuracy\n",
            "45%\n",
            "61%\n",
            "65%\n",
            "\n",
            "Table 2. Performance of segmentation schemes. Segmentation\n",
            "accuracy computed as intersection over union of segmentation\n",
            "with ground truth.\n",
            "\n",
            "Dataset\n",
            "Oxford-IIIT Pet Dataset\n",
            "UCSD-Caltech Birds\n",
            "Oxford-Flowers102\n",
            "\n",
            "Mean Classiﬁcation Accuracy\n",
            "38.45%\n",
            "6.91%\n",
            "53.71%\n",
            "\n",
            "Table 3. Fine grained classiﬁcation baseline. Mean classiﬁcation\n",
            "accuracies obtained on three different datasets using the VLFeat-\n",
            "BoW classiﬁcation code.\n",
            "\n",
            "4. Experiments\n",
            "\n",
            "The models are evaluated ﬁrst on the task of discrim-\n",
            "inating the family of the pet (Sect. 4.1), then on the one\n",
            "of discriminating their breed given the family (Sect. 4.2),\n",
            "and ﬁnally discriminating both the family and the breed\n",
            "(Sect. 4.3). For the third task, both hierarchical classiﬁca-\n",
            "tion (i.e, determining ﬁrst the family and then the breed)\n",
            "and ﬂat classiﬁcation (i.e, determining the family and the\n",
            "breed simultaneously) are evaluated. Training uses the\n",
            "Oxford-IIIT Pet train and validation data and testing uses\n",
            "the Oxford-IIIT Pet test data. All these results are summa-\n",
            "rized in Tab. 4 and further results for pet family discrimina-\n",
            "tion on the ASIRRA data are reported in Sect. 4.1. Failure\n",
            "cases are reported in Fig. 7.\n",
            "\n",
            "Baseline.\n",
            "In order to compare the difﬁculty of the Oxford-\n",
            "IIIT Pet dataset\n",
            "to other Fine Grained Visual Catego-\n",
            "rization datasets, and also to provide a baseline for our\n",
            "breed classiﬁcation task, we have run the publicly available\n",
            "VLFeat [40] BoW classiﬁcation code over three datasets:\n",
            "Oxford Flowers 102 [33], UCSD-Caltech Birds [14], and\n",
            "Oxford-IIIT Pet dataset (note that this code is a faster suc-\n",
            "cessor to the VGG-MKL package [41] used on the UCSD-\n",
            "Caltech Birds dataset in [14]). The code employs a spatial\n",
            "pyramid [30], but does not use segmentation or salient parts.\n",
            "The results are given in Table 3.\n",
            "\n",
            "4.1. Pet family discrimination\n",
            "\n",
            "This section evaluates the different models on the task\n",
            "of discriminating the family of a pet (cat Vs dog classiﬁca-\n",
            "tion).\n",
            "\n",
            "Shape only. The maximum response of the cat face detec-\n",
            "tor (Sect. 3.1) on an image is used as an image-level score\n",
            "\n",
            "\f.\n",
            "\n",
            "Shape\n",
            "\n",
            "Appearance\n",
            "\n",
            "Classiﬁcation Accuracy (%)\n",
            "\n",
            "layout type\n",
            "\n",
            "using ground truth\n",
            "\n",
            "both (S. 4.3)\n",
            "\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "\n",
            "(cid:88)\n",
            "–\n",
            "–\n",
            "–\n",
            "–\n",
            "(cid:88)\n",
            "(cid:88)\n",
            "(cid:88)\n",
            "(cid:88)\n",
            "\n",
            "–\n",
            "Image\n",
            "Image+Head\n",
            "Image+Head+Body\n",
            "Image+Head+Body\n",
            "Image\n",
            "Image+Head\n",
            "Image+Head+Body\n",
            "Image+Head+Body\n",
            "\n",
            "–\n",
            "–\n",
            "–\n",
            "–\n",
            "(cid:88)\n",
            "–\n",
            "–\n",
            "–\n",
            "(cid:88)\n",
            "\n",
            "family\n",
            "(S. 4.1)\n",
            "94.21\n",
            "82.56\n",
            "85.06\n",
            "87.78\n",
            "88.68\n",
            "94.88\n",
            "95.07\n",
            "94.89\n",
            "95.37\n",
            "\n",
            "breed (S. 4.2)\n",
            "dog\n",
            "cat\n",
            "NA\n",
            "NA\n",
            "40.59\n",
            "52.01\n",
            "52.10\n",
            "60.37\n",
            "54.31\n",
            "64.27\n",
            "57.29\n",
            "66.12\n",
            "42.94\n",
            "50.27\n",
            "54.56\n",
            "59.11\n",
            "55.68\n",
            "63.48\n",
            "59.18\n",
            "66.07\n",
            "\n",
            "hierarchical\n",
            "NA\n",
            "NA\n",
            "NA\n",
            "NA\n",
            "NA\n",
            "42.29\n",
            "52.78\n",
            "55.26\n",
            "57.77\n",
            "\n",
            "ﬂat\n",
            "NA\n",
            "39.64\n",
            "51.23\n",
            "54.05\n",
            "56.60\n",
            "43.30\n",
            "54.03\n",
            "56.68\n",
            "59.21\n",
            "\n",
            "Table 4. Comparison between different models. The table compares different models on the three tasks of discriminating the family, the\n",
            "breed given the family, and the breed and family of the pets in the Oxford-IIIT Pet dataset (Sect. 2). Different combinations of the shape\n",
            "features (deformable part model of the pet faces) and of the various appearance features are tested (Sect. 3.2, Fig. 4).\n",
            "\n",
            "for the cat class. The same is done to obtain a score for\n",
            "the dog class. Then a linear SVM is learned to discriminate\n",
            "between cats and dogs based on these two scores. The clas-\n",
            "siﬁcation accuracy of this model on the Oxford-IIIT Pet test\n",
            "data is 94.21%.\n",
            "\n",
            "Appearance only. Spatial histograms of visual words are\n",
            "used in a non-linear SVM to discriminate between cats and\n",
            "dogs, as detailed in Sect. 3.2. The accuracy depends on\n",
            "the type of spatial histograms considered, which in turn\n",
            "depends on the layout of the spatial bins. On the Oxford-\n",
            "IIIT Pet test data, the image layout obtains an accuracy of\n",
            "82.56%; adding head information using image+head layout\n",
            "yields an accuracy of 85.06%. Using image+head+body\n",
            "layout improves accuracy by a further 2.7% to 87.78%. An\n",
            "improvement of 1% was observed when the ground-truth\n",
            "segmentations were used in place of the segmentations es-\n",
            "timated by grab-cut (Sect. 3.2). This progression indicates\n",
            "that the more accurate the localization of the pet body, the\n",
            "better is the classiﬁcation accuracy.\n",
            "\n",
            "Shape and appearance. The appearance and shape infor-\n",
            "mation are combined by summing the exp-χ2 kernel for the\n",
            "appearance part (Sect. 3.2) with a linear kernel on the cat\n",
            "scores and a linear kernel on the dog scores. The combina-\n",
            "tion boosts the performance by an additional 7% over that\n",
            "of using appearance alone, yielding approximately 95.37%\n",
            "accuracy (Table 4, rows 5 and 9), with all the variants of the\n",
            "appearance model performing similarly.\n",
            "\n",
            "Method\n",
            "Golle et al. [25]\n",
            "This paper (Shape only)\n",
            "\n",
            "Mean Class. Accuracy\n",
            "82.7%\n",
            "92.9%\n",
            "\n",
            "Table 5. Performance on ASIRRA Data. Table shows perfor-\n",
            "mance achieved on task of pet family classiﬁcation posed by the\n",
            "ASIRRA challenge. Best results obtained by Golle [25] were ob-\n",
            "tained using 10000 images from the data. 8000 for training and\n",
            "2000 for testing. Our test results are shown on 24990 images in\n",
            "the ASIRRA dataset.\n",
            "\n",
            "92.9%, which corresponds to a 42% probability of breaking\n",
            "the test in a single try. For comparison, the best accuracy re-\n",
            "ported in the literature on the ASIRRA data is 82.7% [25],\n",
            "which corresponds to just a 9.2% chance of breaking the\n",
            "test. Due to lack of sufﬁcient training data to train appear-\n",
            "ance models for ASIRRA data, we did not evaluate these\n",
            "models on ASIRRA dataset.\n",
            "\n",
            "4.2. Breed discrimination\n",
            "\n",
            "This section evaluates the models on the task of discrimi-\n",
            "nating the different breeds of cats and dogs given their fam-\n",
            "ily. This is done by learning a multi-class SVM by using\n",
            "the 1-Vs-rest decomposition [37] (this means learning 12\n",
            "binary classiﬁers for cats and 25 for dogs). The relative per-\n",
            "formance of the different models is similar to that observed\n",
            "for pet family classiﬁcation in Sect. 4.1. The best breed\n",
            "classiﬁcation accuracies for cats and dogs are 63.48% and\n",
            "55.68% respectively, which improve to 66.07% and 59.18%\n",
            "when the ground truth segmentations are used.\n",
            "\n",
            "4.3. Family and breed discrimination\n",
            "\n",
            "The ASIRRA data. The ASIRRA data does not specify a\n",
            "training set, so we used models trained on the Oxford-IIIT\n",
            "Pet data and the ASIRRA data was used only for testing.\n",
            "The accuracy of the shape model on the ASIRRA data is\n",
            "\n",
            "This section investigates classifying both the family and\n",
            "the breed. Two approaches are explored: hierarchical clas-\n",
            "siﬁcation, in which the family is decided ﬁrst as in Sect. 4.1,\n",
            "and then the breed is decided as in Sect. 4.2, and ﬂat classi-\n",
            "\n",
            "\fFigure 6. Confusion matrix for breed discrimination. The ver-\n",
            "tical axis reports the ground truth labels, and the horizontal axis to\n",
            "the predicted ones (the upper-left block are the cats). The matrix is\n",
            "normalized by row and the values along the diagonal are reported\n",
            "on the right. The matrix corresponds to the breed classiﬁer using\n",
            "shape features, appearance features with the image, head, body,\n",
            "body-head layouts with automatic segmentations, and a 37-class\n",
            "SVM. This is the best result for breed classiﬁcation, and corre-\n",
            "sponds to the last entry of row number 8 in Tab. 4.\n",
            "\n",
            "Figure 5. Example segmentation results on Oxford-IIIT Pet\n",
            "dataset. The segmentation of the pet from the background was\n",
            "obtained automatically as described in Sect. 3.3.\n",
            "\n",
            "e\n",
            "\n",
            "f\n",
            "\n",
            "g\n",
            "\n",
            "a\n",
            "\n",
            "b\n",
            "\n",
            "c\n",
            "\n",
            "d\n",
            "\n",
            "h\n",
            "\n",
            "ﬁcation, in which a 37-class SVM is learned directly, using\n",
            "the same method discussed in Sect. 4.2. The relative per-\n",
            "formance of the different models is similar to that observed\n",
            "in Sect. 4.1 and 4.2. Flat classiﬁcation is better than hier-\n",
            "archical, but the latter requires less work at test time, due\n",
            "to the fact that fewer SVM classiﬁers need to be evaluated.\n",
            "For example, using the appearance model with the image,\n",
            "head, image-head layouts for 37 class classiﬁcation yields\n",
            "an accuracy of 51.23%, adding the shape information hi-\n",
            "erarchically improves this accuracy to 52.78%, and using\n",
            "shape and appearance together in a ﬂat classiﬁcation ap-\n",
            "proach achieves an accuracy 54.03%. The confusion matrix\n",
            "for the best result for breed classiﬁcation, corresponding to\n",
            "the last entry of the eight row of Table 4 is shown in Fig. 4.\n",
            "\n",
            "Figure 7. Failure cases for the model using appearance only (im-\n",
            "age layout) in Sect. 4.2. First row: Cat images that were incor-\n",
            "rectly classiﬁed as dogs and vice versa. Second row: Bengal cats\n",
            "(b–d) classiﬁed as Egyptian Mau (a). Third row: English Setter\n",
            "(f–h) classiﬁed as English Cocker Spaniel (e).\n",
            "\n",
            "5. Summary\n",
            "\n",
            "This paper has introduced the Oxford-IIIT Pet dataset\n",
            "for the ﬁne-grained categorisation problem of identifying\n",
            "the family and breed of pets (cats and dogs). Three differ-\n",
            "ent tasks and corresponding baseline algorithms have been\n",
            "proposed and investigated obtaining very encouraging clas-\n",
            "siﬁcation results on the dataset. Furthermore, the baseline\n",
            "models were shown to achieve state-of-the-art performance\n",
            "on the ASIRRA challenge data, breaking the test with 42%\n",
            "\n",
            "35.7%39.0%77.0%81.8%69.0%71.1%60.0%64.0%51.0%46.0%70.0%82.0%52.0%4.0%62.0%33.0%38.4%20.0%29.0%43.0%80.0%70.0%51.0%82.0%75.8%53.0%39.0%82.0%28.0%85.0%59.0%91.0%66.7%57.0%37.1%53.0%50.0%12345678910111213141516171819202122232425262728293031323334353637Abyssinian  1Bengal  2Birman  3Bombay  4British Shorthair  5Egyptian Mau  6Maine Coon  7Persian  8Ragdoll  9Russian Blue 10Siamese 11Sphynx 12Am. Bulldog 13Am. Pit Bull Terrier 14Basset Hound 15Beagle 16Boxer 17Chihuahua 18Eng. Cocker Spaniel 19Eng. Setter 20German Shorthaired 21Great Pyrenees 22Havanese 23Japanese Chin 24Keeshond 25Leonberger 26Miniature Pinscher 27Newfoundland 28Pomeranian 29Pug 30Saint Bernard 31Samoyed 32Scottish Terrier 33Shiba Inu 34Staff. Bull Terrier 35Wheaten Terrier 36Yorkshire Terrier 37\fprobability, a remarkable achievement considering that this\n",
            "dataset was designed to be challenging for machines.\n",
            "\n",
            "Acknowledgements. We are grateful for ﬁnancial sup-\n",
            "port from the UKIERI, EU Project AXES ICT-269980 and\n",
            "ERC grant VisRec no. 228180.\n",
            "\n",
            "References\n",
            "\n",
            "[1] American kennel club. http://www.akc.org/.\n",
            "[2] The cat fanciers association inc.\n",
            "org/Client/home.aspx.\n",
            "\n",
            "http://www.cfa.\n",
            "\n",
            "[3] Cats in sinks. http://catsinsinks.com/.\n",
            "[4] Catster. http://www.catster.com/.\n",
            "[5] Dogster. http://www.dogster.com/.\n",
            "[6] Flickr! http://www.flickr.com/.\n",
            "[7] Google images. http://images.google.com/.\n",
            "[8] The international cat association. http://www.tica.\n",
            "\n",
            "org/.\n",
            "\n",
            "[9] My cat space. http://www.mycatspace.com/.\n",
            "[10] My dog space. http://www.mydogspace.com/.\n",
            "[11] Petﬁnder.\n",
            "html.\n",
            "\n",
            "http://www.petfinder.com/index.\n",
            "\n",
            "[12] World canine organisation. http://www.fci.be/.\n",
            "[13] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. From con-\n",
            "tours to regions: An empirical evaluation. In Proc. CVPR,\n",
            "2009.\n",
            "\n",
            "[14] S. Branson, C. Wah, F. Schroff, B. Babenko, P. Welinder,\n",
            "P. Perona, and S. Belongie. Visual recognition with humans\n",
            "in the loop. In Proc. ECCV, 2010.\n",
            "\n",
            "[15] Y. Chai, V. Lempitsky, and A. Zisserman. Bicos: A bi-level\n",
            "In Proc.\n",
            "\n",
            "co-segmentation method for image classiﬁcation.\n",
            "ICCV, 2011.\n",
            "\n",
            "[16] G. Csurka, C. R. Dance, L. Dan, J. Willamowski, and\n",
            "C. Bray. Visual categorization with bags of keypoints.\n",
            "In\n",
            "Proc. ECCV Workshop on Stat. Learn. in Comp. Vision,\n",
            "2004.\n",
            "\n",
            "[17] N. Dalal and B. Triggs. Histograms of oriented gradients for\n",
            "\n",
            "human detection. In Proc. CVPR, 2005.\n",
            "\n",
            "[18] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\n",
            "ImageNet: A Large-Scale Hierarchical Image Database. In\n",
            "Proc. CVPR, 2009.\n",
            "\n",
            "[19] J. Elson, J. Douceur, J. Howell, and J. J. Saul. Asirra: A\n",
            "CAPTCHA that exploits interest-aligned manual image cat-\n",
            "egorization. In Conf. on Computer and Communications Se-\n",
            "curity (CCS), 2007.\n",
            "\n",
            "[20] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\n",
            "and A. Zisserman. The PASCAL Visual Object Classes\n",
            "Challenge 2011 (VOC2011) Results.\n",
            "http://www.pascal-\n",
            "network.org/challenges/VOC/voc2011/workshop/index.html.\n",
            "[21] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J.\n",
            "Lin. LIBLINEAR: A library for large linear classiﬁcation.\n",
            "Journal of Machine Learning Research, 9, 2008.\n",
            "\n",
            "[22] L. Fei-Fei, R. Fergus, and P. Perona. A Bayesian approach to\n",
            "unsupervised one-shot learning of object categories. In Proc.\n",
            "ICCV, 2003.\n",
            "\n",
            "[23] P. F. Felzenszwalb, R. B. Grishick, D. McAllester, and D. Ra-\n",
            "manan. Object detection with discriminatively trained part\n",
            "based models. PAMI, 2009.\n",
            "\n",
            "[24] F. Fleuret and D. Geman. Stationary features and cat detec-\n",
            "tion. Journal of Machine Learning Research, 9, 2008.\n",
            "[25] P. Golle. Machine learning attacks against the asirra captcha.\n",
            "In 15th ACM Conference on Computer and Communications\n",
            "Security (CCS), 2008.\n",
            "\n",
            "[26] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object cat-\n",
            "egory dataset. Technical report, California Institute of Tech-\n",
            "nology, 2007.\n",
            "\n",
            "[27] A. Khosla, N. Jayadevaprakash, B. Yao, and F. F. Li. Novel\n",
            "dataset for ﬁne-grained image categorization. In First Work-\n",
            "shop on Fine-Grained Visual Categorization, CVPR, 2011.\n",
            "\n",
            "[28] C. Lampert, H. Nickisch, and S. Harmeling. Learning to de-\n",
            "tect unseen object classes by between-class attribute transfer.\n",
            "In Proc. CVPR, 2009.\n",
            "\n",
            "[29] I. Laptev. Improvements of object detection using boosted\n",
            "\n",
            "histograms. In Proc. BMVC, 2006.\n",
            "\n",
            "[30] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bag of\n",
            "features: Spatial pyramid matching for recognizing natural\n",
            "scene categories. In Proc. CVPR, 2006.\n",
            "\n",
            "[31] D. G. Lowe. Object recognition from local scale-invariant\n",
            "\n",
            "features. In Proc. ICCV, 1999.\n",
            "\n",
            "[32] M.-E. Nilsback and A. Zisserman. A visual vocabulary for\n",
            "\n",
            "ﬂower classiﬁcation. In Proc. CVPR, 2006.\n",
            "\n",
            "[33] M.-E. Nilsback and A. Zisserman. Automated ﬂower clas-\n",
            "siﬁcation over a large number of classes. In Proc. ICVGIP,\n",
            "2008.\n",
            "\n",
            "[34] O. Parkhi, A. Vedaldi, C. V. Jawahar, and A. Zisserman. The\n",
            "\n",
            "truth about cats and dogs. In Proc. ICCV, 2011.\n",
            "\n",
            "[35] O. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar. The\n",
            "Oxford-IIIT PET Dataset. http://www.robots.ox.\n",
            "ac.uk/˜vgg/data/pets/index.html, 2012.\n",
            "[36] C. Rother, V. Kolmogorov, and A. Blake. “grabcut” — in-\n",
            "teractive foreground extraction using iterated graph cuts. In\n",
            "ACM Trans. on Graphics, 2004.\n",
            "\n",
            "[37] B. Sch¨olkopf and A. J. Smola. Learning with Kernels. MIT\n",
            "\n",
            "Press, 2002.\n",
            "\n",
            "[38] J. Sivic and A. Zisserman. Video Google: A text retrieval\n",
            "approach to object matching in videos. In Proc. ICCV, 2003.\n",
            "[39] M. Varma and D. Ray. Learning the discriminative power-\n",
            "\n",
            "invariance trade-off. In Proc. ICCV, 2007.\n",
            "\n",
            "[40] A. Vedaldi and B. Fulkerson. VLFeat library. http://\n",
            "\n",
            "www.vlfeat.org/, 2008.\n",
            "\n",
            "[41] A. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman. Mul-\n",
            "tiple kernels for object detection. In Proc. ICCV, 2009.\n",
            "[42] A. Vedaldi and A. Zisserman. Structured output regression\n",
            "\n",
            "for detection with partial occulsion. In Proc. NIPS, 2009.\n",
            "\n",
            "[43] P. Welinder, S. Branson, T. Mita, C. Wah, and F. Schroff.\n",
            "Caltech-ucsd birds 200. Technical report, Caltech-UCSD,\n",
            "2010.\n",
            "\n",
            "[44] J. Zhang, M. Marszalek, S. Lazebnik, and C. Schmid. Local\n",
            "features and kernels for classiﬁcation of texture and object\n",
            "categories: A comprehensive study. IJCV, 2007.\n",
            "\n",
            "[45] W. Zhang, J. Sun, and X. Tang. Cat head detection - how\n",
            "In Proc.\n",
            "\n",
            "to effectively exploit shape and texture features.\n",
            "ECCV, 2008.\n",
            "\n",
            "\f"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcd90OuY9kP3"
      },
      "source": [
        "O código a seguir extrai as imagems (alguns tipos) do arquivo PDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGI8M3x3aqbp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cad31e7-5d98-45f2-a003-7344c7d74109"
      },
      "source": [
        "# We can use PyPDF2 along with Pillow (Python Imaging Library) \n",
        "# to extract images from the PDF pages and save them as image files\n",
        "\n",
        "!pip3 install Pillow\n",
        "!pip3 install PyPDF2\n",
        "\n",
        "import PyPDF2\n",
        "from PIL import Image\n",
        "\n",
        "with open('/content/example02.pdf', 'rb') as pdf_file:\n",
        "    pdf_reader = PyPDF2.PdfFileReader(pdf_file)\n",
        "\n",
        "    # extracting images from the 1st page\n",
        "    #page0 = pdf_reader.getPage(0)\n",
        "    page0 = pdf_reader.getPage(2)\n",
        "\n",
        "    if '/XObject' in page0['/Resources']:\n",
        "        xObject = page0['/Resources']['/XObject'].getObject()\n",
        "\n",
        "        for obj in xObject:\n",
        "            if xObject[obj]['/Subtype'] == '/Image':\n",
        "                size = (xObject[obj]['/Width'], xObject[obj]['/Height'])\n",
        "                data = xObject[obj].getData()\n",
        "                if xObject[obj]['/ColorSpace'] == '/DeviceRGB':\n",
        "                    mode = \"RGB\"\n",
        "                else:\n",
        "                    mode = \"P\"\n",
        "\n",
        "                if '/Filter' in xObject[obj]:\n",
        "                    if xObject[obj]['/Filter'] == '/FlateDecode':\n",
        "                        img = Image.frombytes(mode, size, data)\n",
        "                        img.save(obj[1:] + \".png\")\n",
        "                        print(obj[1:] + \".png\")\n",
        "                    elif xObject[obj]['/Filter'] == '/DCTDecode':\n",
        "                        img = open(obj[1:] + \".jpg\", \"wb\")\n",
        "                        img.write(data)\n",
        "                        img.close()\n",
        "                        print(obj[1:] + \".jpg\")\n",
        "                    elif xObject[obj]['/Filter'] == '/JPXDecode':\n",
        "                        img = open(obj[1:] + \".jp2\", \"wb\")\n",
        "                        img.write(data)\n",
        "                        img.close()\n",
        "                        print(obj[1:] + \".jp2\")\n",
        "                    elif xObject[obj]['/Filter'] == '/CCITTFaxDecode':\n",
        "                        img = open(obj[1:] + \".tiff\", \"wb\")\n",
        "                        img.write(data)\n",
        "                        img.close()\n",
        "                        print(obj[1:] + \".tiff\")\n",
        "                else:\n",
        "                    img = Image.frombytes(mode, size, data)\n",
        "                    img.save(obj[1:] + \".png\")\n",
        "                    print(obj[1:] + \".png\")\n",
        "    else:\n",
        "        print(\"No image found.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Collecting PyPDF2\n",
            "  Downloading PyPDF2-1.26.0.tar.gz (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 3.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyPDF2\n",
            "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-py3-none-any.whl size=61101 sha256=bcf29938a7ed9b22b2141a5163a48df6ec51c52d865e81db97b5476d1943099b\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1a/24/648467ade3a77ed20f35cfd2badd32134e96dd25ca811e64b3\n",
            "Successfully built PyPDF2\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-1.26.0\n",
            "Im4.png\n",
            "Im5.png\n",
            "Im6.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQrPo-uo90u_"
      },
      "source": [
        "### **Informação do arquivo PDF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_U8OB3EgKAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d43504-265d-4456-ae4e-43cdc03bc3de"
      },
      "source": [
        "import PyPDF2\n",
        "\n",
        "with open('/content/example02.pdf', 'rb') as pdf_file:\n",
        "    pdf_reader = PyPDF2.PdfFileReader(pdf_file)\n",
        "    print(f'Number of Pages in PDF File is {pdf_reader.getNumPages()}')\n",
        "    print(f'PDF Metadata is {pdf_reader.documentInfo}')\n",
        "    print(f'PDF File Author is {pdf_reader.documentInfo[\"/Author\"]}')\n",
        "    print(f'PDF File Creator is {pdf_reader.documentInfo[\"/Creator\"]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Pages in PDF File is 5\n",
            "PDF Metadata is {'/Author': '', '/CreationDate': 'D:20180109020539Z', '/Creator': 'LaTeX with hyperref package', '/Keywords': '', '/ModDate': 'D:20180109020539Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', '/Producer': 'pdfTeX-1.40.17', '/Subject': '', '/Title': '', '/Trapped': '/False'}\n",
            "PDF File Author is \n",
            "PDF File Creator is LaTeX with hyperref package\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVxh2xdU95yf"
      },
      "source": [
        "### **Listagem dos arquivos no Google Colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cViQvEb0c9Zc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12a69308-d1e2-4744-a470-c27a72c53c8d"
      },
      "source": [
        "import os\n",
        "print( os.getcwd() )\n",
        "print( os.listdir() )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "['.config', 'cats-and-dogs', 'Im5.png', 'example02.pdf', 'example01.pdf', 'Im4.png', 'Im6.png', 'sample_data']\n"
          ]
        }
      ]
    }
  ]
}