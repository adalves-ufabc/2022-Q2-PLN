{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022-Q2 PLN Notebook 04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1LobzYMhrwRnnbBncCAov3ZSrZlKICnej",
      "authorship_tag": "ABX9TyNAIyw8aMOBGI4xwf4MaQ3v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalves-ufabc/2022.Q2-PLN/blob/main/2022_Q2_PLN_Notebook_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2022.Q2]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmK05FgcOzL2"
      },
      "source": [
        "## **Tokens e Vocabulário**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVnXhhy1_1KC"
      },
      "source": [
        "### **Exemplo 01**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFmK0BEjxH0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed22e032-b526-4e12-f875-4d6d01d694dd"
      },
      "source": [
        "# O trecho de codigo a seguir extrai palavras (tokens) de uma string, e \n",
        "# calcula o numero de palavras e o tamanho do vocabulario\n",
        "\n",
        "import re\n",
        "\n",
        "regex = r\"[-'a-zA-ZÀ-ÖØ-öø-ÿ]+\"\n",
        "#regex = r\"[-'a-zA-ZÀ-ÖØ-öø-ÿ0-9]+\"\n",
        "\n",
        "# https://pt.wikipedia.org/wiki/Capivara\n",
        "\n",
        "texto = (\"\"\"A capivara (nome científico: Hydrochoerus hydrochaeris) é uma espécie\n",
        "         de mamífero roedor da família Caviidae e subfamília Hydrochoerinae. \n",
        "         Alguns autores consideram que deva ser classificada em uma família própria. \n",
        "         Está incluída no mesmo grupo de roedores ao qual se classificam as pacas, \n",
        "         cutias, os preás e o porquinho-da-índia. Ocorre por toda a América do Sul \n",
        "         ao leste dos Andes em habitats associados a rios, lagos e pântanos, \n",
        "         do nível do mar até 1 300 m de altitude. Extremamente adaptável, \n",
        "         pode ocorrer em ambientes altamente alterados pelo ser humano. (d'água)\"\"\")\n",
        "\n",
        "matches = re.finditer(regex, texto)\n",
        "\n",
        "vocabulario = set([])\n",
        "\n",
        "for (i, match) in enumerate(matches):\n",
        "   #print (f\"Palavra {i+1} foi identificada nas posições {match.start()}-{match.end()}: {match.group()}\")\n",
        "   vocabulario.add(match.group())\n",
        "    \n",
        "print (f\"N={i+1}, V={len(vocabulario)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N=87, V=74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn7YcdHUCHVt"
      },
      "source": [
        "### **Exemplo 02**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzhGg3zvya15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee003522-42db-4f34-80d7-e56f69bd1d99"
      },
      "source": [
        "# O trecho de codigo a seguir conta o numero de palavras de um livro\n",
        "# de Machado de Assis\n",
        "\n",
        "regex = r\"[-'a-zA-ZÀ-ÖØ-öø-ÿ]+\"  \n",
        "#regex = r\"[-'a-zA-ZÀ-ÖØ-öø-ÿ0-9]+\" \n",
        "\n",
        "with open(\"/content/A-Semana-Machado-de-Assis.txt\",'r') as document:\n",
        "\n",
        "    content  = document.read()\n",
        "\n",
        "    words = re.findall(regex, content)\n",
        "    #for w in words:\n",
        "    #    print (w)\n",
        "    print (f\"Total de palavras (tokens): {len(words)}\")   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de palavras (tokens): 266724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWpAtXofFg77"
      },
      "source": [
        "### **Exemplo 03**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZpYbwuu0Bn0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a183c1-4dca-4940-c4fc-bc6b6f29225f"
      },
      "source": [
        "# O trecho de codigo a seguir calcula a frequencia das palavras \n",
        "# e imprime as 20 mais frequentes\n",
        "\n",
        "regex = r\"[-'a-zA-ZÀ-ÖØ-öø-ÿ]+\" \n",
        "#regex = r\"[-'a-zA-ZÀ-ÖØ-öø-ÿ0-9]+\" \n",
        "\n",
        "with open(\"/content/A-Semana-Machado-de-Assis.txt\",'r') as document:\n",
        "\n",
        "    content  = document.read()\n",
        "   \n",
        "    # identificacao de palavras\n",
        "    words       = re.findall(regex, content)\n",
        "    frequencies = dict([])\n",
        "\n",
        "    # quantidade de vezes no documento\n",
        "    for w in words:\n",
        "        w = w.lower()\n",
        "        if w not in frequencies:\n",
        "            frequencies[w] = 0\n",
        "        frequencies[w] += 1\n",
        "    print (f\"Tokens: {len(words)}, Vocabulario: {len(frequencies)}\")\n",
        "\n",
        "    # imprimir as 20 palavras mais frequentes\n",
        "    fs = sorted(frequencies, key=frequencies.get, reverse=True)\n",
        "    for i in range(0,20):\n",
        "        print (f\"--> {frequencies[fs[i]]} {fs[i]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: 266724, Vocabulario: 25197\n",
            "--> 11053 que\n",
            "--> 9875 a\n",
            "--> 9767 de\n",
            "--> 8236 o\n",
            "--> 7525 e\n",
            "--> 5519 não\n",
            "--> 4018 é\n",
            "--> 3476 do\n",
            "--> 3465 os\n",
            "--> 3352 um\n",
            "--> 3203 da\n",
            "--> 2862 se\n",
            "--> 2421 as\n",
            "--> 2237 em\n",
            "--> 2054 uma\n",
            "--> 2024 com\n",
            "--> 2019 mas\n",
            "--> 1811 para\n",
            "--> 1756 por\n",
            "--> 1456 ao\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHvy_nvjE3NQ"
      },
      "source": [
        "### **Exemplo 04**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjC4beqy4M7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd8de95-395d-4b10-d44c-fc121b22e710"
      },
      "source": [
        "# O trecho de codigo a seguir remove as stopwords e imprime as 20 mais \n",
        "# frequentes (desconsiderando as stopwords)\n",
        "\n",
        "regex = r\"[-'a-zA-ZÀ-ÖØ-öø-ÿ]+\" \n",
        "#regex = r\"[-'a-zA-ZÀ-ÖØ-öø-ÿ0-9]+\" \n",
        "\n",
        "stopwords = set([])\n",
        "\n",
        "# leitura das stopwords\n",
        "with open(\"/content/stopwords-pt.txt\",'r') as stopwordsPTfile:\n",
        "    for s in stopwordsPTfile.readlines():\n",
        "        stopwords.add(s.strip().lower())\n",
        "    \n",
        "with open(\"/content/A-Semana-Machado-de-Assis.txt\",'r') as document:\n",
        "\n",
        "    # leitura do documento\n",
        "    content  = document.read()\n",
        "\n",
        "    # identificacao de palavras\n",
        "    words       = re.findall(regex, content)\n",
        "    frequencies = dict([])\n",
        "\n",
        "    # quantidade de vezes no documento\n",
        "    for w in words:\n",
        "        w = w.lower()\n",
        "        if w not in stopwords:\n",
        "            if w not in frequencies:\n",
        "                frequencies[w] = 0\n",
        "            frequencies[w] += 1\n",
        "    print (f\"Tokens: {len(words)}, Vocabulario: {len(frequencies)}\")\n",
        "\n",
        "    # imprimir as 20 palavras mais frequentes\n",
        "    fs = sorted(frequencies, key=frequencies.get, reverse=True)\n",
        "    for i in range(0,20):\n",
        "        print (f\"--> {frequencies[fs[i]]} {fs[i]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: 266724, Vocabulario: 25005\n",
            "--> 703 ser\n",
            "--> 629 ainda\n",
            "--> 564 tudo\n",
            "--> 526 pode\n",
            "--> 475 outro\n",
            "--> 471 homem\n",
            "--> 455 todos\n",
            "--> 454 assim\n",
            "--> 437 outra\n",
            "--> 420 outros\n",
            "--> 409 grande\n",
            "--> 385 dois\n",
            "--> 384 coisa\n",
            "--> 375 tempo\n",
            "--> 367 dia\n",
            "--> 361 bem\n",
            "--> 360 menos\n",
            "--> 353 vez\n",
            "--> 339 porque\n",
            "--> 334 onde\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ2ObpSlYXx0"
      },
      "source": [
        "### **Extra**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cND7-A83YaiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cea11038-6c5e-4387-b46f-0e0f5390ed80"
      },
      "source": [
        "import re\n",
        "\n",
        "regex = r\"[-'a-zA-ZÀ-ÖØ-öø-ÿ0-9]+\" \n",
        "\n",
        "with open(\"/content/teste-palavras.txt\",'r') as document:\n",
        "\n",
        "    content  = document.read()\n",
        "   \n",
        "    # identificacao de palavras\n",
        "    words       = re.findall(regex, content)\n",
        "    frequencies = dict([])\n",
        "\n",
        "    # quantidade de vezes no documento\n",
        "    for w in words:\n",
        "        w = w.lower()\n",
        "        if w not in frequencies:\n",
        "            frequencies[w] = 0\n",
        "        frequencies[w] += 1\n",
        "    print (f\"Tokens: {len(words)}, Vocabulario: {len(frequencies)}\")\n",
        "\n",
        "    # imprimir as palavras de acordo com a frequencia\n",
        "    fs = sorted(frequencies, key=frequencies.get, reverse=True)\n",
        "    for i in range(len(fs)):\n",
        "       print (f\"--> {frequencies[fs[i]]} {fs[i]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: 33, Vocabulario: 27\n",
            "--> 4 c\n",
            "--> 2 45\n",
            "--> 2 a\n",
            "--> 2 ufabc\n",
            "--> 1 total\n",
            "--> 1 de\n",
            "--> 1 r\n",
            "--> 1 10\n",
            "--> 1 para\n",
            "--> 1 valores\n",
            "--> 1 superiores\n",
            "--> 1 455\n",
            "--> 1 67\n",
            "--> 1 www\n",
            "--> 1 edu\n",
            "--> 1 br\n",
            "--> 1 livre-docente\n",
            "--> 1 homem-máquina\n",
            "--> 1 d'água\n",
            "--> 1 u\n",
            "--> 1 f\n",
            "--> 1 b\n",
            "--> 1 m\n",
            "--> 1 ph\n",
            "--> 1 d\n",
            "--> 1 sant'anna\n",
            "--> 1 l'ensemble\n"
          ]
        }
      ]
    }
  ]
}